--- 
title: "Analyze Survey Data for Free"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: ajdamico/asdfree
description: ""
---

```{r echo = FALSE , eval = TRUE , results = "hide" }
file.copy( "CNAME" , "docs/CNAME" , overwrite = TRUE )
file.copy( "404.html" , "docs/404.html" , overwrite = TRUE )
```


# Step by Step Instructions to Explore Public Microdata from an Easy to Type Website {-}

Edited by Anthony Joseph Damico <<ajdamico@gmail.com>>.  Contact me directly if you have funding available to add chapters to this book, or for consultancy work in survey analysis or syntax translation across SAS, SPSS, Stata, SUDAAN, and R.  Please ask questions about the content of this book at [stackoverflow.com](https://stackoverflow.com/questions/ask?tags=r+survey) with the `R` and `survey` tags.  Public data is the original crowdsourcing.

<!--chapter:end:index.Rmd-->

# Prerequisites {-}

This book assumes a basic understanding of [the R language](http://www.r-project.org/).  If you like my style of pedagogy, you could try watching [my introductory video lectures](http://www.twotorials.com/).  Otherwise, review the R learning options at [flowingdata.com](http://flowingdata.com/2012/06/04/resources-for-getting-started-with-r/).

The R `lodown` package depends on most of the packages used in this text, so these three lines should install many of the R packages presented here, including the `survey` and `MonetDBLite` R packages.  Windows users may need to install the external software [Rtools](http://cran.r-project.org/bin/windows/Rtools/) prior to installing `lodown` from [github](http://github.com/).

```{r eval = FALSE}
install.packages( "devtools" , repos = "http://cran.rstudio.com/" )
library(devtools)
install_github( "ajdamico/lodown" , dependencies = TRUE )
```

The survey microdata presented in this book require the [R survey package](http://r-survey.r-forge.r-project.org/survey/) by [Dr. Thomas Lumley](https://www.stat.auckland.ac.nz/people/tlum005) at the University of Auckland.  Dr. Lumley wrote [a textbook](http://r-survey.r-forge.r-project.org/svybook/) to showcase that software.

The larger datasets presented in this book rely on [MonetDBLite](https://cran.r-project.org/web/packages/MonetDBLite/README.html) by [Dr. Hannes Muehleisen](http://hannes.muehleisen.org/) and [the MonetDB Development Team](https://www.monetdb.org/AboutUs/Contributors), which typically completes SQL queries blazingly fast due to its *columnar* storage architecture and bulk query processing model.

The R `convey` package estimates measures of inequality, poverty and wellbeing.  Guilherme Jacob, Dr. Djalma Pessoa, and I have written [this book](https://guilhermejacob.github.io/context/) about inequality measurement with complex survey microdata to accompany the software.

```{r eval = FALSE}
install.packages( "convey" , repos = "http://cran.rstudio.com/" )
```

The R `srvyr` package by [Greg Freedman Ellis](https://github.com/gergness/) allows [dplyr](https://cran.r-project.org/web/packages/dplyr/README.html)-like syntax for many `survey` package commands.  For detailed usage examples, review his [vignettes](https://cran.r-project.org/web/packages/srvyr/index.html).

```{r eval = FALSE}
install.packages( "srvyr" , repos = "http://cran.rstudio.com/" )
```

<!--chapter:end:01-prerequisites.Rmd-->

# The Census of 82 Countries Made Laptop-Friendly {-}


<!--chapter:end:02-ipumsi.Rmd-->

# Maps and the Art of Survey-Weighted Maintenance {-}


<!--chapter:end:03-swmap.Rmd-->

# Statistically Significant Trends with Multiple Years of Complex Survey Data  {-}

[![Build Status](https://travis-ci.org/asdfree/trendy.svg?branch=master)](https://travis-ci.org/asdfree/trendy) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/trendy?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/trendy)


*Contributed by Thomas Yokota <<thomasyokota@gmail.com>>*


Palermo Professor [Vito Muggeo](mailto:vito.muggeo@unipa.it) wrote the joinpoint analysis section of the code below to demonstrate that [the `segmented` package](https://cran.r-project.org/web/packages/segmented/index.html) eliminates the need for [external (registration-only, windows-only, workflow-disrupting) software](http://surveillance.cancer.gov/joinpoint/download.html).  [`survey` package](http://r-survey.r-forge.r-project.org/survey/) creator and professor [Dr. Thomas Lumley](mailto:t.lumley@auckland.ac.nz) wrote [the `svypredmeans` function](https://cran.r-project.org/web/packages/survey/NEWS) to replicate SUDAAN's `PREDMARG` command and match the CDC to the decimal.  [Dr. Richard Lowry, M.D.](mailto:rxl1@cdc.gov) at the Centers for Disease Control & Prevention wrote [the original linear trend analysis](http://www.cdc.gov/healthyyouth/yrbs/pdf/yrbs_conducting_trend_analyses.pdf) and then answered our infinite questions.  Thanks to everyone.





The purpose of this analysis is to make statistically valid statements such as, *"there was a significant linear decrease in the prevalence of high school aged americans who have ever smoked a cigarette across the period 1999-2011"* with complex sample survey data.

This step-by-step walkthrough exactly reproduces the statistics presented in the [Center for Disease Control & Prevention's (CDC) linear trend analysis](http://www.cdc.gov/healthyyouth/yrbs/pdf/yrbs_conducting_trend_analyses.pdf), using free and open source methods rather than proprietary or restricted software.

The example below displays only linearized designs (created with [the `svydesign` function](http://r-survey.r-forge.r-project.org/survey/html/svydesign.html)).  For more detail about how to reproduce this analysis with a replicate-weighted design (created with [the `svrepdesign` function](http://r-survey.r-forge.r-project.org/survey/html/svrepdesign.html)), see the methods note below section #4.


## Data Importation {-}
Prior to running this analysis script, the Youth Risk Behavioral Surveillance System (YRBSS) 1991-2011 single-year files must all be loaded as R data files (.rda) on your local machine.  Running the download automation script will create the appropriate files.  If you need assistance with the data-loading step, first review [the main YRBSS blog post](http://www.asdfree.com/search/label/youth%20risk%20behavior%20surveillance%20system%20%28yrbss%29).

```{r eval = FALSE }
options( survey.lonely.psu = "adjust" )

library(survey)

library(lodown)

# retrieve a listing of all available extracts for the european social survey
yrbss_cat <- get_catalog( "yrbss" , output_dir = file.path( path.expand( "~" ) , "YRBSS" ) )

# limit the catalog to only years 2005-2015
yrbss_cat <- subset( yrbss_cat , year %in% seq( 2005 , 2015 , 2 ) )

# download the yrbss microdata
lodown( "yrbss" , yrbss_cat )
```


## Load Required Packages, Options, External Functions {-}

```{r eval = FALSE }
# install.packages( c( "segmented" , "ggplot2" , "ggthemes" , "texreg" ) )

# Muggeo V. (2008) Segmented: an R package to fit regression models with broken-line relationships. R News, 8, 1: 20-25.
library(segmented)

library(ggplot2)
library(ggthemes)
library(texreg)
```



## Harmonize and Stack Multiple Years of Survey Data {-}

This step is clearly dataset-specific.  In order for your trend analysis to work, you'll need to figure out how to align the variables from multiple years of data into a trendable, stacked `data.frame` object.

```{r eval = FALSE }

# initiate an empty `y` object
y <- NULL

# loop through each year of YRBSS microdata
for ( year in seq( 2005 , 2015 , 2 ) ){

	# load the current year
	x <- readRDS( 
		file.path( path.expand( "~" ) , "YRBSS" , 
			paste0( year , " main.rds" ) ) )
	
	# tack on a `year` column
	x$year <- year
	
	if( year == 2005 ) x$raceeth <- NA
	
	# stack that year of data alongside the others,
	# ignoring mis-matching columns
	y <- rbind( x[ c( "q2" , "q3" , "q4" , "qn10" , "year" , "psu" , "stratum" , "weight" , "raceeth" ) ] , y )
	
	# clear the single-year of microdata from RAM
	rm( x )
	
}


# convert every column to numeric type
y[ , ] <- sapply( y[ , ] , as.numeric )

# construct year-specific recodes so that
# "ever smoked a cigarette" // grade // sex // race-ethnicity align across years
y <-
	transform(
		
		y ,
		
		rode_with_drunk_driver = qn10 ,
				
		raceeth = 
			
			ifelse( year == 2005 ,
				ifelse( q4 %in% 6 , 1 ,
				ifelse( q4 %in% 3 , 2 ,
				ifelse( q4 %in% c( 4 , 7 ) , 3 ,
				ifelse( q4 %in% c( 1 , 2 , 5 , 8 ) , 4 , NA ) ) ) ) ,
				
				ifelse( raceeth %in% 5 , 1 ,
				ifelse( raceeth %in% 3 , 2 ,
				ifelse( raceeth %in% c( 6 , 7 ) , 3 ,
				ifelse( raceeth %in% c( 1 , 2 , 4 , 8 ) , 4 , NA ) ) ) ) ) ,
				
				
		grade = ifelse( q3 == 5 , NA , as.numeric( q3 ) ) ,
		
		sex = ifelse( q2 %in% 1:2 , q2 , NA )
		
	)
	

# again remove unnecessary variables, keeping only the complex sample survey design columns
# plus independent/dependent variables to be used in the regression analyses
y <- y[ c( "year" , "psu" , "stratum" , "weight" , "rode_with_drunk_driver" , "raceeth" , "sex" , "grade" ) ]

# set female to the reference group
y$sex <- relevel( factor( y$sex ) , ref = "2" )

# set ever smoked=yes // white // 9th graders as the reference groups
for ( i in c( 'rode_with_drunk_driver' , 'raceeth' , 'grade' ) ) y[ , i ] <- relevel( factor( y[ , i ] ) , ref = "1" )

```



## Construct a Multi-Year Stacked Complex Survey Design Object {-}

Before constructing a multi-year stacked design object, check out `?contr.poly` - this function implements polynomials used in our trend analysis during step #6.  For more detail on this subject, see [page 216 of Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences](https://www.google.com/search?q=The+polynomials+we+have+used+as+predictors+to+this+point+are+natural+polynomials%2C+generated+from+the+linear+predictor+by+centering+and+the+powering+the+linear+predictor.&ie=utf-8&oe=utf-8) By Jacob Cohen, Patricia Cohen, Stephen G. West, Leona S. Aiken *"The polynomials we have used as predictors to this point are natural polynomials, generated from the linear predictor by centering and the powering the linear predictor."*

```{r eval = FALSE }
# extract a linear contrast vector of length eleven,
# because we have eleven distinct years of yrbss data `seq( 2005 , 2015 , 2 )`
c6l <- contr.poly( 6 )[ , 1 ]

# also extract a quadratic (squared) contrast vector
c6q <- contr.poly( 6 )[ , 2 ]

# just in case, extract a cubic contrast vector
c6c <- contr.poly( 6 )[ , 3 ]

# for each record in the data set, tack on the linear, quadratic, and cubic contrast value
# these contrast values will serve as replacement for the linear `year` variable in any regression.

# year^1 term (linear)
y$t6l <- c6l[ match( y$year , seq( 2005 , 2015 , 2 ) ) ]

# year^2 term (quadratic)
y$t6q <- c6q[ match( y$year , seq( 2005 , 2015 , 2 ) ) ]

# year^3 term (cubic)
y$t6c <- c6c[ match( y$year , seq( 2005 , 2015 , 2 ) ) ]

# construct a complex sample survey design object
# stacking multiple years and accounting for `year` in the nested strata
des <- 
	svydesign(
		id = ~psu , 
		strata = ~interaction( stratum , year ) ,
		data = y , 
		weights = ~weight , 
		nest = TRUE
	)

```


Now we've got a multi-year stack of complex survey designs with linear, quadratic, and cubic contrast values appended.  If you'd like more detail about stacking multiple years of complex survey data, review [the CDC's manual](http://www.cdc.gov/healthyyouth/yrbs/pdf/yrbs_combining_data.pdf) on the topic.  Hopefully we won't need anything beyond cubic, but let's find out.


**Methods note** about how to stack replication designs: This is only relevant if you are trying to create a `des` like the object above but just have replicate weights and do not have the clustering information (psu).  It is straightforward to construct a replication design *from* a linearized design (see [`as.svrepdesign`](http://r-survey.r-forge.r-project.org/survey/html/as.svrepdesign.html)).  However, [for privacy reasons, going in the opposite direction is much more challenging](http://www.asdfree.com/2014/09/how-to-provide-variance-calculation-on.html).  Therefore, you'll need to do some dataset-specific homework on how to best *stack* multiple years of a replicate-weighted design you construct a multiple-year-stacked survey design like the object above.

If you'd like to experiment with how the two approaches differ (theoretically, very little), these publicly-available survey data sets include both replicate weights and, separately, clustering information:

[Medical Expenditure Panel Survey](http://www.asdfree.com/search/label/medical%20expenditure%20panel%20survey%20%28meps%29)  
[National Health and Nutrition Examination Survey](http://www.asdfree.com/search/label/national%20health%20and%20nutrition%20examination%20survey%20%28nhanes%29)  
[Consumer Expenditure Survey](http://www.asdfree.com/search/label/consumer%20expenditure%20survey%20%28ce%29)  

In most cases, omitting the `year` variable from the `strata = ~interaction( stratum , year )` construction of `des` above will make your standard errors larger (conservative) -> ergo -> you can probably just `rbind( file_with_repweights_year_one , file_with_repweights_year_two , ... )` so long as the survey design has not changed in structure over the time period that you are analyzing.  Once you have the rbound replicate weights object for every year, you could just construct one huge multi-year `svrepdesign` object. Make sure you include `scale`, `rscales`, `rho`, and whatever else the `svrepdesign()` call asks for.  If you are worried you missed something, check `attributes( your_single_year_replication_design_object )`.  This solution is likely to be a decent approach in most cases.

If you need to be very conservative with your computation of trend statistical significance, you might attempt to re-construct fake clusters for yourself using a regression.  Search for "malicious" in [this confidentiality explanation document](https://github.com/ajdamico/asdfree/blob/master/Confidentiality/how%20to%20create%20de-identified%20replicate%20weights.R).  The purpose here, though, isn't to identify individual respondents in the dataset, it's to get a variable like `psu` above that gives you reasonable standard errors.  Look for the object `your.replicate.weights` in that script.  You could reconstruct a fake psu for each record in your data set with something as easy as..

```{r eval = FALSE }
# # fake_psu should be a one-record-per-person vector object
# # that can immediately be appended onto your data set.

# fake_psu <- kmeans( your.replicate.weights , 20 )
```
..where 20 is the (completely made up) number of clusters x strata.  Hopefully the methodology documents (or the people who wrote them) will at least tell you how many clusters there were in the original sample, even if the clusters themselves were not disclosed.  At the point you've made fake clusters, they will surely be worse than the real clusters (i.e. conservative standard errors) and you can construct a multiple-year survey design with:

```{r eval = FALSE }
# des <- svydesign( id = ~ your_fake_psus , strata = ~ year , data = y , weights = ~ weight , nest = TRUE )
```

This approach will probably be conservative probably.


## Review the unadjusted results {-}
Here's the change over time for smoking prevalence among youth. Unadjusted prevalence rates (Figure 1) suggest a significant change in smoking prevalence.

```{r eval = FALSE }
# immediately remove records with missing smoking status
des_ns <- subset( des , !is.na( rode_with_drunk_driver ) )

# calculate unadjusted, un-anythinged "ever smoked" rates by year
# note that this reproduces the unadjusted "ever smoked" statistics at the top of
# pdf page 6 of http://www.cdc.gov/healthyyouth/yrbs/pdf/yrbs_conducting_trend_analyses.pdf
unadjusted <- svyby( ~ rode_with_drunk_driver , ~ year , svymean , design = des_ns , vartype = c( 'ci' , 'se' ) )

# coerce that result into a `data.frame` object
my_plot <- data.frame( unadjusted )

# plot the unadjusted decline in smoking
ggplot( my_plot , aes( x = year, y = rode_with_drunk_driver1 ) ) +
  geom_point() + 
  geom_errorbar( aes( ymax = ci_u.rode_with_drunk_driver1 , ymin = ci_l.rode_with_drunk_driver1 ) , width = .2 ) +
  geom_line() +
  theme_tufte() +
  ggtitle( "Figure 1. Unadjusted smoking prevalence 1999-2011" ) +
  theme( plot.title = element_text( size = 9 , face = "bold" ) )
```



## Calculate the Number of Joinpoints Needed {-}

Using the orthogonal coefficients (linear, quadratic, cubic terms) that we previously added to our `data.frame` object before constructing the multi-year stacked survey design, let's now determine how many joinpoints will be needed for a trend analysis.

Epidemiological models typically control for possible confounding variables such as sex and race, so let's add them in with the linear, cubic, and quadratic year terms.

Calculate the "ever smoked" binomial regression, adjusted by sex, age, race-ethnicity, and a linear year contrast.

```{r eval = FALSE }
linyear <- 
	svyglm(
		I( rode_with_drunk_driver == 1 ) ~ sex + raceeth + grade + t6l , 
		design = subset( des_ns , rode_with_drunk_driver %in% 1:2 ) , 
		family = quasibinomial
	)

summary( linyear )
```

The linear year-contrast variable `t11l` is hugely significant here.  Therefore, there is probably going to be some sort of trend.  A linear trend does not need joinpoints.  Not one, just zero joinpoints.  If the linear term were the only significant term (out of linear, quadratic, cubic, etc.), then we would not need to calculate a joinpoint.  In other words, we would not need to figure out where to best break our time trend into two, three, or even four segments.

The linear trend is significant, so we should keep going.

-----

**Interpretation note** about segments of time: The linear term `t11l` was significant, so we probably have a significant linear trend somewhere to report.  Now we need to figure out when that significant linear trend started and when it ended.  It might be semantically true that there was a significant linear decrease in high school aged smoking over the entire period of our data 1991-2011; however, it's inexact, unrefined to give up after only detecting a linear trend.  The purpose of the following few steps is really to *cordon off* different time points from one another.  As you'll see later, there actually was not any detectable decrease from 1991-1999.  The entirety of the decline in smoking occurred over the period from 1999-2011.  So these next (methodologically tricky) steps serve to provide you and your audience with a more careful statement of statistical significance.  It's not technically wrong to conclude that smoking declined over the period of 1991-2011, it's just verbose.

Think of it as the difference between "humans first walked on the moon in the sixties" and "humans first walked on the moon in 1969" - both statements are correct, but the latter exhibits greater scientific precision.

-----

Calculate the "ever smoked" binomial regression, adjusted by sex, age, race-ethnicity, and both linear and quadratic year contrasts.  Notice the addition of `t11q`.

```{r eval = FALSE }
quadyear <-
	svyglm(
		I( rode_with_drunk_driver == 1 ) ~ sex + raceeth + grade + t6l + t6q , 
		design = subset( des_ns , rode_with_drunk_driver %in% 1:2 ) , 
		family = quasibinomial 
	)

summary( quadyear )
```
The linear year-contrast variable is hugely significant here but the quadratic year-contrast variable is also significant.  Therefore, we should use joinpoint software for this analysis.  A significant quadratic trend needs one joinpoint.

Since both linear and quadratic terms are significant, we should move ahead and test whether the cubic term is also significant.

Calculate the "ever smoked" binomial regression, adjusted by sex, age, race-ethnicity, and linear, quadratic, and cubic year contrasts.  Notice the addition of `t11c`.
```{r eval = FALSE }
cubyear <-
	svyglm(
		I( rode_with_drunk_driver == 1 ) ~ sex + raceeth + grade + t6l + t6q + t6c , 
		design = subset( des_ns , rode_with_drunk_driver %in% 1:2 ) , 
		family = quasibinomial 
	)
	
summary( cubyear )
```
The cubic year-contrast term is *not* significant in this model.  Therefore, we should stop testing the shape of this line.  In other words, we can stop at a quadratic trend and *do not* need a cubic trend.  That means we can stop at a single joinpoint.  Remember: a linear trend requires zero joinpoints, a quadratic trend typically requires one joinpoint, a cubic trend usually requires two, and on and on.

Note: if the cubic trend *were* significant, then we would increase the number of joinpoints to *two* instead of *one* but since the cubic term is not significant, we should stop with the previous regression.  If we keep getting significant trends, we ought to continue testing whether higher terms continue to be significant.  So `year^4` requires three joinpoints, `year^5` requires four joinpoints, and so on.  If these terms continued to be significant, we would need to return to step #4 and add additional `year^n` terms to the model.

Just for coherence's sake, let's assemble all of these results into a single table where you can see the linear, quadratic, and cubic models side-by-side.  The quadratic trend best describes the relationship between prevalence of smoking and change-over-time. The decision to test beyond linear trends, however, is a decision for the individual researcher to make. It is a decision that can be driven by theoretical issues, existing literature, or the availability of data.
<center>
```{r eval = FALSE }
htmlreg(list(linyear , quadyear , cubyear), doctype = F, html.tag = F, inline.css = T, 
    head.tag = F, body.tag = F, center = F, single.row = T, caption = "Table 1. Testing for linear trends")
```
</center>


## Calculate the Adjusted Prevalence and Predicted Marginals {-}

First, calculate the survey-year-independent predictor effects and store these results into a separate object.

```{r eval = FALSE }
marginals <- 
	svyglm(
		formula = I( rode_with_drunk_driver == 1 ) ~ sex + raceeth + grade ,
		design = des_ns , 
		family = quasibinomial
	)
```

Second, run these marginals through the `svypredmeans` function written by Dr. Thomas Lumley.  For any archaeology fans out there, this function emulates the `PREDMARG` statement in the ancient language of SUDAAN.
```{r eval = FALSE }
( means_for_joinpoint <- svypredmeans( marginals , ~factor( year ) ) )
```

Finally, clean up these results a bit in preparation for a joinpoint analysis.
```{r eval = FALSE }
# coerce the results to a data.frame object
means_for_joinpoint <- as.data.frame( means_for_joinpoint )

# extract the row names as the survey year
means_for_joinpoint$year <- as.numeric( rownames( means_for_joinpoint ) )

# must be sorted, just in case it's not already
means_for_joinpoint <- means_for_joinpoint[ order( means_for_joinpoint$year ) , ]

# rename columns so they do not conflict with variables in memory
names( means_for_joinpoint ) <- c( 'mean' , 'se' , 'yr' )
# the above line is only because the ?segmented function (used below)
# does not work if an object of the same name is also in memory.

another_plot <- means_for_joinpoint
another_plot$ci_l.mean <- another_plot$mean - (1.96 * another_plot$se)
another_plot$ci_u.mean <- another_plot$mean + (1.96 * another_plot$se)

ggplot(another_plot, aes(x = yr, y = mean)) +
  geom_point() + 
  geom_errorbar(aes(ymax = ci_u.mean, ymin = ci_l.mean), width=.2) +
  geom_line() +
  theme_tufte() +
  ggtitle("Figure 2. Adjusted smoking prevalence 1999-2011") +
  theme(plot.title = element_text(size=9, face="bold"))
```



## Identify the Breakpoint/Changepoint {-}

The original CDC analysis recommended some [external software from the National Cancer Institute](http://surveillance.cancer.gov/joinpoint/), which only runs on selected platforms. Dr. Vito Muggeo wrote this within-R solution using his [segmented](https://cran.r-project.org/web/packages/segmented/index.html) package available on CRAN.  Let's take a look at how confident we are in the value at each adjusted timepoint.  Carrying out a trend analysis requires creating new weights to fit a piecewise linear regression. Figure 3 shows the relationship between variance at each datum and weighting.  Larger circles display greater uncertainty and therefore lower weight.

```{r eval = FALSE }
ggplot( means_for_joinpoint , aes( x = yr , y = mean ) ) +
	geom_point( aes( size = se ) ) +
	theme_tufte() +
	ggtitle( "Figure 3. Standard Error at each timepoint\n(smaller dots indicate greater confidence in each adjusted value)"
) 
```


First, create that weight variable.

```{r eval = FALSE }
means_for_joinpoint$wgt <- with( means_for_joinpoint, ( mean / se ) ^ 2 ) 
```

Second, fit a piecewise linear regression.

```{r eval = FALSE }
# estimate the 'starting' linear model with the usual "lm" function using the log values and the weights.
o <- lm( log( mean ) ~ yr , weights = wgt , data = means_for_joinpoint )
``` 

Now that the regression has been structured correctly, estimate the year that our complex survey trend should be broken into two segments (the changepoint/breakpoint/joinpoint).

```{r eval = FALSE }
# the segmented() function uses a random process in its algorithm.
# setting the random seed ensures reproducibility
set.seed( 2015 )

# add a segmented variable (`yr` in this example) with 1 breakpoint
os <- segmented( o , ~yr )

# `os` is now a `segmented` object, which means it includes information on the fitted model,
# such as parameter estimates, standard errors, residuals.
summary( os )
```

See the `Estimated Break-Point(s)` in that result?  That's the critical number from this joinpoint analysis.

Note that the above number is not an integer! The [R `segmented` package](https://cran.r-project.org/web/packages/segmented/index.html) uses an iterative procedure (described in the article below) and therefore between-year solutions are returned.  The joinpoint software implements two estimating algorithms: the grid-search and the Hudson algorithm.  For more detail about these methods, see [Muggeo V. (2003) Estimating regression models with unknown break-points. Statistics in Medicine, 22: 3055-3071.](http://onlinelibrary.wiley.com/doi/10.1002/sim.1545/abstract).

```{r eval = FALSE }
# figuring out the breakpoint year was the purpose of this joinpoint analysis.
( your_breakpoint <- round( as.vector( os$psi[, "Est." ] ) ) )
# so.  that's a joinpoint.  that's where the two line segments join.  okay?

# obtain the annual percent change (APC=) estimates for each time point
slope( os , APC = TRUE )
```

The returned CIs for the annual percent change (APC) may be different from the ones returned by [NCI's Joinpoint Software](surveillance.cancer.gov/joinpoint/); for further details, check out [Muggeo V. (2010) A Comment on `Estimating average annual per cent change in trend analysis' by Clegg et al., Statistics in Medicine; 28, 3670-3682. Statistics in Medicine, 29, 1958-1960.](http://onlinelibrary.wiley.com/doi/10.1002/sim.3850/abstract)

This analysis returned similar results to the NCI's Joinpoint Regression Program by estimating a changepoint at `year=1999` - and, more precisely, that the start of that decreasing trend in smoking prevalence happened at an APC of -3.92 percent.  That is, `slope2` from the output above.


## Make statistically defensible statements about linear trends with complex survey data {-}

After identifying the change point for smoking prevalence, we can create two regression models (one for each time segment).  (If we had two joinpoints, we would need three regression models.)  The first model covers the years leading up to (and including) the changepoint (i.e., 1991 to 1999).  The second model includes the years from the changepoint forward (i.e., 1999 to 2011).  So start with 1991, 1993, 1995, 1997, 1999, the five year-points before (and including 1999).

```{r eval = FALSE }
# calculate a five-timepoint linear contrast vector
c3l <- contr.poly( 3 )[ , 1 ]

# tack the five-timepoint linear contrast vectors onto the current survey design object
des_ns <- update( des_ns , t3l = c3l[ match( year , seq( 2005 , 2009 , 2 ) ) ] )

pre_91_99 <-
	svyglm(
		I( rode_with_drunk_driver == 1 ) ~ sex + raceeth + grade + t3l ,
		design = subset( des_ns , rode_with_drunk_driver %in% 1:2 & year <= 2009 ) , 
		family = quasibinomial
	)

summary( pre_91_99 )
```

Reproduce the sentence on [pdf page 6 of the original document](http://www.cdc.gov/healthyyouth/yrbs/pdf/yrbs_conducting_trend_analyses.pdf#page=6).  **In this example, T5L_L had a p-value=0.52261 and beta=0.03704. Therefore, there was "no significant change in the prevalence of ever smoking a cigarette during 1991-1999."**


Then move on to 1999, 2001, 2003, 2005, 2007, 2009, and 2011, the seven year-points after (and including 1999).


```{r eval = FALSE }
# calculate a seven-timepoint linear contrast vector
c4l <- contr.poly( 4 )[ , 1 ]

# tack the seven-timepoint linear contrast vectors onto the current survey design object
des_ns <- update( des_ns , t4l = c4l[ match( year , seq( 2009 , 2015 , 2 ) ) ] )

post_99_11 <-
	svyglm(
		I( rode_with_drunk_driver == 1 ) ~ sex + raceeth + grade + t4l ,
		design = subset( des_ns , rode_with_drunk_driver %in% 1:2 & year >= 2009 ) , 
		family = quasibinomial
	)
	
summary( post_99_11 )
``` 


Reproduce the sentence on [pdf page 6 of the original document](http://www.cdc.gov/healthyyouth/yrbs/pdf/yrbs_conducting_trend_analyses.pdf#page=6).  **In this example, T7L_R had a p-value<0.0001 and beta=-0.99165. Therefore, there was a "significant linear decrease in the prevalence of ever smoking a cigarette during 1999-2011."**

Note also that the 1999-2011 time period saw a linear decrease, which supports the APC estimate in step #8.  Here's everything displayed as a single coherent table.


```{r eval = FALSE }
htmlreg(list(pre_91_99, post_99_11), doctype = F, html.tag = F, inline.css = T, 
    head.tag = F, body.tag = F, center = F, single.row = T, caption = "Table 2. Linear trends pre-post changepoint")
```


*This analysis may complement qualitative evaluation on prevalence changes observed from surveillance data by providing quantitative evidence, such as when a change point occurred. This analysis does not explain why or how changes in trends occur.*

<!--chapter:end:04-trendy.Rmd-->

# Structural Equation Models (SEM) with Complex Survey Data {-}
 
[![Build Status](https://travis-ci.org/asdfree/lavaanex.svg?branch=master)](https://travis-ci.org/asdfree/lavaanex) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/lavaanex?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/lavaanex)

*Contributed by Dr. Daniel Oberski <<daniel.oberski@gmail.com>>*

The R `lavaan.survey` package by [Dr. Daniel Oberski](http://daob.nl) fits structural equation models to complex survey microdata, described in his [JSS article](https://www.jstatsoft.org/article/view/v057i01).

```{r eval = FALSE}
install.packages( "lavaan.survey" , repos = "http://cran.rstudio.com/" )
```

## Load the 2008 Wave of the European Social Survey German and Spanish Microdata {-}
```{r eval = FALSE}
library(lodown)
library(lavaan.survey)	
options( survey.lonely.psu = "adjust" )

# retrieve a listing of all available extracts for the european social survey
ess_cat <- get_catalog( "ess" , output_dir = file.path( path.expand( "~" ) , "ESS" ) )

# limit the catalog to only wave #4 for germany and spain
ess_cat <- subset( ess_cat , wave == 4 & grepl( "c=DE|c=ES" , full_url ) )

# download the ess microdata
lodown( "ess" , ess_cat , your_email = "email@address.com" )
```

Immediately pull the German files into the workspace:
```{r eval = FALSE}
# load Germany's round four main data file..
ess4.de <- readRDS( file.path( path.expand( "~" ) , "ESS" , "2008/ESS4DE.rds" ) )

# load Germany's round four sample design data file (sddf)..
ess4.de.sddf <- readRDS( file.path( path.expand( "~" ) , "ESS" , "2008/ESS4_DE_SDDF.rds" ) )
```

The stratify variable is not literally equal to the actual strata but contains more information (which we don't need here).  Create a new variable that only uses the actual stratification namely, East v. West Germany, by using a regular expression / string substitution function to take the data.frame object's `stratify` variable, convert it to a character variable, search for a dash, and keep only the text before the dash.  Then, convert that resultant vector of ones and twos into a factor variable, labeled East versus West Germany.
```{r eval = FALSE}
ess4.de.sddf$stratify <- 
	factor( gsub( "(\\d+)-.+" , "\\1" , as.character( ess4.de.sddf$stratify ) ) )

levels(ess4.de.sddf$stratify) <- c("West Germany", "East Germany")
```
Check against [ESS documentation](www.europeansocialsurvey.org/docs/round4/survey/ESS4_data_documentation_report_e05_4.pdf#page=121) statement that "The number of sampling points is 109 in the West, and 59 in the East":

```{r eval = FALSE}
stopifnot(tapply(ess4.de.sddf$psu, 
                 ess4.de.sddf$stratify, 
                 function(x) length(unique(x))) == c(109, 59))
```

Merge these two files together, creating a single table:
```{r eval = FALSE}
ess4.de.m <- merge( ess4.de , ess4.de.sddf)

stopifnot( 
	nrow( ess4.de ) == nrow( ess4.de.m ) & 
	nrow( ess4.de.sddf ) == nrow( ess4.de.m ) 
)
```

Create a survey design object:
```{r eval = FALSE}
ess4.de.design <- 
	svydesign(
		ids = ~psu ,
		strata = ~stratify ,
		probs = ~prob ,
		data = ess4.de.m
	)
```

## Two-factor CFA of attitudes toward the welfare state {-}


This analysis uses the model of the below article. Please see the article for more information.

Roosma, F., Gelissen, J., & van Oorschot, W. (2013). The multidimensionality of welfare state attitudes: a European cross-national study. Social indicators research, 113(1), 235-255.

Formulate the two-factor CFA using lavaan syntax:
```{r eval = FALSE}
model.cfa <-    
	"range =~ gvjbevn + gvhlthc + gvslvol + gvslvue + gvcldcr + gvpdlwk
	 goals =~ sbprvpv  +  sbeqsoc  +  sbcwkfm"
```

Fit the model using lavaan, accounting for possible nonnormality using the MLM estimator:
```{r eval = FALSE}
fit.cfa.ml <- 
	lavaan(
		model.cfa , 
		data = ess4.de.m , 
		estimator = "MLM" , 
		int.ov.free = TRUE ,
		auto.var = TRUE , 
		auto.fix.first = TRUE , 
		auto.cov.lv.x = TRUE
	)
```
	
Show some fit measure results, note the "scaling correction" which accounts for nonnormality:
```{r eval = FALSE}
fit.cfa.ml
```


Fit the two-factor model while taking the survey design into account:
```{r eval = FALSE}
fit.cfa.surv <- 
	lavaan.survey(
		fit.cfa.ml , 
		survey.design = ess4.de.design
	)
```

Show some fit measure results, "scaling correction" now accounts for both nonnormality and survey design.
```{r eval = FALSE}
fit.cfa.surv
```

Display parameter estimates and standard errors accounting for survey design:
```{r eval = FALSE}
summary( fit.cfa.surv , standardized = TRUE )
```

## Invariance testing on Schwarz human values while accounting for the survey design. {-}

For more information on this analysis, see: Davidov, E., Schmidt, P., & Schwartz, S. H. (2008). "Bringing values back in: The adequacy of the European Social Survey to measure values in 20 countries". Public opinion quarterly, 72(3), 420-445.

Test the measurement equivalence of Schwarz human values from round 4 of the ESS, comparing Germany with Spain. 

First load the Spanish data so these can be merged:
```{r eval = FALSE}
# load Spain's round four main data file..
ess4.es <- readRDS( file.path( path.expand( "~" ) , "ESS" , "2008/ESS4ES.rds" ) )

# load Spain's round four sample design data file (sddf)..
ess4.es.sddf <- readRDS( file.path( path.expand( "~" ) , "ESS" , "2008/ESS4_ES_SDDF.rds" ) )
```

Merge these two files together, creating a single table:
```{r eval = FALSE}
ess4.es.m <- merge( ess4.es , ess4.es.sddf)

stopifnot( 
	nrow( ess4.es ) == nrow( ess4.es.m ) & 
	nrow( ess4.es.sddf ) == nrow( ess4.es.m ) 
)
```

Make sure PSU names are unique between the two countries. Paste on a "de-" to the German PSUs, and by pasting an "es-" to the front of the Spanish PSUs.

```{r eval = FALSE}
ess4.de.m$psu <- paste( "de" , ess4.de.m$psu , sep="-" )
ess4.es.m$psu <- paste( "es" , ess4.es.m$psu , sep="-" )
```

Stack the two countries into a single table, then construct a survey design:
```{r eval = FALSE}
ess4.m <- rbind( ess4.de.m , ess4.es.m )

ess4.design <- 
	svydesign(
		ids = ~psu,
		strata = ~stratify ,
		probs = ~prob ,
		data = ess4.m
	)
```

Model based on Schwarz human value theory. Note that this is the basic starting model, not the final model used by Davidov et al. They merge certain values and allow cross-loadings:

```{r eval = FALSE}
free.values.model.syntax <- " 
  Universalism =~ ipeqopt + ipudrst + impenv
  Benevolence  =~ iphlppl + iplylfr

  Tradition    =~ ipmodst + imptrad
  Conformity   =~ ipfrule + ipbhprp 
  Security     =~ impsafe + ipstrgv
"
```

Fit two-group configural invariance model:
```{r eval = FALSE}
free.values.fit <- 
	lavaan(
		free.values.model.syntax , 
		data = ess4.m , 
		auto.cov.lv.x = TRUE , 
		auto.fix.first = TRUE , 
		auto.var = TRUE ,
		int.ov.free = TRUE , 
		estimator = "MLM" ,
		group = "cntry"
	)

summary( free.values.fit , standardized = TRUE )
```

Fit a two-group metric invariance model:
```{r eval = FALSE}
free.values.fit.eq <- 
	lavaan(
		free.values.model.syntax , 
		data = ess4.m , 
		auto.cov.lv.x = TRUE , 
		auto.fix.first = TRUE , 
		auto.var = TRUE ,
		int.ov.free = TRUE , 
		estimator = "MLM" ,
		group = "cntry" , 
		group.equal = "loadings"
	)

summary( free.values.fit.eq , standardized = TRUE )
```

Metric invariance test (anova() would work here too, but not below):
```{r eval = FALSE}
lavTestLRT( free.values.fit , free.values.fit.eq , SB.classic = TRUE )
```

Compare chisquares of the survey and non-survey SEM analyses for the configural invariance model:
```{r eval = FALSE}
free.values.fit.surv <- lavaan.survey( free.values.fit , ess4.design )
free.values.fit
free.values.fit.surv
```

Compare chisquares of the survey and non-survey SEM analyses for the metric invariance model:
```{r eval = FALSE}
free.values.fit.eq.surv <- lavaan.survey( free.values.fit.eq , ess4.design )
free.values.fit.eq
free.values.fit.eq.surv
```


Perform metric invariance test accounting for the survey design:
```{r eval = FALSE}
lavTestLRT(free.values.fit.surv, free.values.fit.eq.surv, SB.classic = TRUE)
```
The two models are more dissimilar after survey design is accounted for.

## An example with a Latent Variable Regression {-}

See

Davidov, E., Meuleman, B., Billiet, J., & Schmidt, P. (2008). Values and support for immigration: A cross-country comparison. European Sociological Review, 24(5), 583-599.

The human values scale again, but this time:

1. only two value dimensions are modeled.
2. the two latent value dimensions are used to predict anti-immigration attitudes in the two countries.
3. a test is performed on the difference between countries in latent regression coefficients.

```{r eval = FALSE}
reg.syntax <- "
  SelfTranscendence =~ ipeqopt + ipudrst + impenv + iphlppl + iplylfr
  Conservation =~ ipmodst + imptrad + ipfrule + ipbhprp + impsafe + ipstrgv

  ALLOW =~ imdfetn + impcntr

  ALLOW ~ SelfTranscendence + Conservation
"

reg.vals.fit <- 
	lavaan(
		reg.syntax , 
		data = ess4.m , 
		group = "cntry" ,
		estimator = "MLM" ,
		auto.cov.lv.x = TRUE , 
		auto.fix.first = TRUE , 
		auto.var = TRUE , 
		int.ov.free = TRUE
	)

reg.vals.fit.eq <- 
	lavaan( 
		reg.syntax , 
		data = ess4.m , 
		group = "cntry" , 
		group.equal = "regressions" ,
		estimator = "MLM" ,
		auto.cov.lv.x = TRUE , 
		auto.fix.first = TRUE , 
		auto.var = TRUE , 
		int.ov.free = TRUE
	)

	
summary( reg.vals.fit.eq , standardize = TRUE )
```

Test whether the relationship between values and anti-immigration attitudes is equal in Germany and Spain:
```{r eval = FALSE}
lavTestLRT( reg.vals.fit , reg.vals.fit.eq , SB.classic = TRUE)
```


Now do the same but accounting for the sampling design:
```{r eval = FALSE}
reg.vals.fit.surv <- lavaan.survey( reg.vals.fit , ess4.design )
reg.vals.fit.eq.surv <- lavaan.survey( reg.vals.fit.eq , ess4.design )

lavTestLRT(reg.vals.fit.surv, reg.vals.fit.eq.surv, SB.classic = TRUE)
```

The two models are less dissimilar after survey design is accounted for.


<!--chapter:end:05-lavaanex.Rmd-->

# Balancing Respondent Confidentiality with Variance Estimation Precision {-}

[![Build Status](https://travis-ci.org/asdfree/confidentiality.svg?branch=master)](https://travis-ci.org/asdfree/confidentiality) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/confidentiality?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/confidentiality)


```{r eval = FALSE}

# analyze survey data for free (http://asdfree.com) with the r language
# how to create de-identified replicate weights
# in less than ten steps

# anthony joseph damico
# ajdamico@gmail.com


# the institute for digital research and education at ucla
# hosts a very readable explanation of replicate weights,
# how they protect confidentiality, and why they're generally awesome
# http://www.ats.ucla.edu/stat/stata/library/replicate_weights.htm



# remove the # in order to run this install.packages line only once
# install.packages( c( "survey" , "sdcMicro" ) )


# load the r survey package
library(survey)

# load the sdcMicro package
library(sdcMicro)


# load some sample complex sample survey data
data(api)


# look at the first six records of the `apistrat` data.frame object
# so you have a sense of what you're working with in this example
# this particular example is student performance in california schools
# http://r-survey.r-forge.r-project.org/survey/html/api.html
head( apistrat )


###########################################
# # # # # # # # # # # # # # # # # # # # # #
# part one.  create the replicate weights #
# # # # # # # # # # # # # # # # # # # # # #
###########################################


# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# step one: construct the taylor-series linearized design #
# that you use on your internal, confidential microdata   #
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #


# there are many permutations of linearized designs.
# you can find well-documented examples of tsl design setups
# by searching for the text `svydesign` here:
# https://github.com/ajdamico/asdfree/search?q=svydesign&ref=cmdform


# does your tsl design have a strata argument? #

# this design does not have a `strata=` parameter
api.tsl.without.strata <-
	svydesign(
		id = ~dnum , 
		data = apistrat , 
		weights = ~pw
	)

# this design does have a `strata=` parameter
api.tsl.with.strata <- 
	svydesign(
		id = ~dnum , 
		strata = ~stype , 
		data = apistrat , 
		weights = ~pw ,
		nest = TRUE
	)


# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# step two: convert this taylor-series linearized design  #
# to one of a few choices of replication-based designs    #
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #


# for unstratified designs,
# use a "jackknife delete one" method
api.jk1 <- 
	as.svrepdesign( 
		api.tsl.without.strata , 
		type = "JK1" , 
		fay.rho = 0.5 ,
		mse = TRUE ,
		compress = FALSE # note: compressed replicate weights require less RAM but complicate the weight-extraction in step three
	)


# for stratified designs,
# you might first attempt to use a
# balanced repeated replication method
# with a fay's adjustment
# (this is the most common setup from the united states census bureau)
try( {

api.fay <- 
	as.svrepdesign( 
		api.tsl.with.strata , 
		type = "Fay" , 
		fay.rho = 0.5 ,
		mse = TRUE ,
		compress = FALSE # note: compressed replicate weights require less RAM but complicate the weight-extraction in step three
	)

} , silent = TRUE )
	
# however, if the sampling plan contains an
# odd number of clusters within any stratum
# you will hit this error
# Error in brrweights(design$strata[, 1], design$cluster[, 1], ..., fay.rho = fay.rho,  : 
  # Can't split with odd numbers of PSUs in a stratum


# for stratified designs with an
# odd number of clusters in any stratum,
# use a "jackknife delete n" method
api.jkn <- 
	as.svrepdesign( 
		api.tsl.with.strata , 
		type = "JKn" ,
		mse = TRUE ,
		compress = FALSE # note: compressed replicate weights require less RAM but complicate the weight-extraction in step three
	)


# now you have a matrix of replicate weights
# stored in your replication-based survey-design


# the purpose of this exercise is to produce comparable
# variance estimates without compromising confidentiality

# therefore, run a few standard error calculations
# using the original linearized designs,
# compared to the newly-created jackknife design

# calculate the mean 1999 and 2000 academic performance index scores
# using the original stratified taylor-series design..
svymean( ~ api99 + api00 , api.tsl.with.strata )
# ..and the newly-created jackknife replication-based design
svymean( ~ api99 + api00 , api.jkn )
# the standard errors for these estimates are nearly identical #

# run the same commands as above, broken down by award program eligibility
svyby( ~ api99 + api00 , ~ awards , api.tsl.with.strata , svymean )
svyby( ~ api99 + api00 , ~ awards , api.jkn , svymean )

# in each case, the replication-based design
# (that we created off of the linearized design)
# produced a comparable variance estimate



# # # # # # # # # # # # # # # # # # # # # # # # #
# step three: extract the replication weights   #
# from your newly created survey design object  #
# # # # # # # # # # # # # # # # # # # # # # # # #

# note that this example shows how to extract
# the weights from the `api.jkn` survey object
# however, this method would be identical
# for the `api.jk1` and `api.fay` objects
# displayed above as well
# despite `api.fay` throwing an error,
# because of an uneven number of clusters within strata


# look at your survey design
api.jkn

# look at the contents of your replication-based survey design
names( api.jkn )
	
# look at the first six replicate weight records within your survey design
head( api.jkn$repweights )
# note that these weights are not *combined* by default
# in other words, they still need to be multiplied by the original weight

# you can confirm this by looking at the flag that indicates:
# "have replicate weights been combined?"
api.jkn$combined.weights
# no.  they have not been combined


# therefore, these replication weights are `uncombined`
# and will need to be analyzed by the user as such
your.replicate.weights <- data.frame( unclass( api.jkn$repweights ) )



# # # # # # # # # # # # # # # # # # # # # # # #
# you've created the set of replicate-weights #

#############################################
# # # # # # # # # # # # # # # # # # # # # # #
# part two. mask the strata from evil users #
# # # # # # # # # # # # # # # # # # # # # # #
#############################################

# now prevent users from reidentifying strata #
# # # # # # # # # # # # # # # # # # # # # # # #


# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# step four: understand how a malicious user might easily #
# identify clustering variables on un-obfuscated data     #
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #

# start with the replicate weights object from the script
transposed.rw <- data.frame( t( your.replicate.weights ) )

# the first record of apistrat has dnum==401 and stype=='E'

# going back to the original microdata set,
# eleven records are in this cluster x stratum
which( apistrat$dnum == 401 & apistrat$stype == 'E' )

# without massaging your replicate weights at all,
# a malicious user could easily view the correlations
# between each record's replicate weights
# in order to determine what other records
# land in the same cluster x strata

# for example, here are all replicate-weight records that
# perfectly (after rounding) correlate with 
# the first record.
which( round( cor( transposed.rw )[ 1 , ] ) == 1 )

# same numbers!

# (wikipedia has a great definition: http://en.wikipedia.org/wiki/Obfuscation)
# if you do not obfuscate your data, a malicious user could
# identify unique clusters and strata even if only given replicate weights


# # # # # # # # # # # # # # # #
# step five: BRING THE NOISE  #
# # # # # # # # # # # # # # # #

# set a random seed.  this allows you to
# go back to this code and reproduce the
# exact same results in the future.

# the following steps include a random process
# setting a seed enforces the same random process every time
sum( utf8ToInt( "anthony is cool" ) )
# my current favorite number is 1482, so let's go with that.
set.seed( 1482 )


# figure out how much noise to add.
noisy.transposed.rw <- addNoise( transposed.rw , noise = 1 )$xm


# remember, on the original replicate weight objects,
# the correlations between the first record and
# other records within the same clusters and strata
# were perfect.
which( sapply( cor( transposed.rw )[ 1 , ] , function( z ) isTRUE( all.equal( z , 1 ) ) ) )

# run the same test on noisified weights
which( sapply( cor( noisy.transposed.rw )[ 1 , ] , function( z ) isTRUE( all.equal( z , 1 ) ) ) )
# and suddenly none of the other records are perfectly correlated


# but even moderately correlations might allow evildoers
# to identify clusters and strata
# these records have a 0.1 or higher correlation coefficient
which( cor( noisy.transposed.rw )[ 1 , ] > 0.1 )
# these records have a 0.2 or higher correlation coefficient
which( cor( noisy.transposed.rw )[ 1 , ] > 0.2 )
# whoops.  we did not add enough noise.
# records in the same cluster x strata still have
# too high of a correlation coefficient,
# relative to other records


# okay.  this will make a big difference
# on the size of the standard errors that
# your users will actually see.

# make a clutch decision:
# how much noise can you tolerate?
# hmncyt <- 1
hmncyt <- 3
# hmncyt <- 10

# you need to run this script a few times
# because the size of your standard errors
# are going to change, depending on your data set.


# essentially, choose the lowest noise value
# that does not lead you to uncomfortable levels
# of correlation within cluster/strata
# in your replicate weights columns


# crank up the random noise percentage to three
noisy.transposed.rw <- addNoise( transposed.rw , noise = hmncyt )$xm
# and suddenly..

# records 29, 73 and 158 have a correlation coefficient
# that's greater than zero point two.
which( cor( noisy.transposed.rw )[ 1 , ] > 0.2 )

# and of those, only record #29 is in the same cluster x strata
intersect(
	which( cor( noisy.transposed.rw )[ 1 , ] > 0.2 ) ,
	which( round( cor( transposed.rw )[ 1 , ] ) == 1 )
)
# that's awesome, because you have two false-positives here.
# the "73" and "158" will throw off a malicious user.


# these records have a 0.1 or higher correlation coefficient
which( cor( noisy.transposed.rw )[ 1 , ] > 0.1 )
# and that looks very good.
# because less than half of those records
# are actually in the same cluster x strata
intersect(
	which( cor( noisy.transposed.rw )[ 1 , ] > 0.1 ) ,
	which( round( cor( transposed.rw )[ 1 , ] ) == 1 )
)
# and there are lots of other records with high correlations (false positives)
# that in fact are not in the same strata.  great.  perf.  magnifique!

# this object `noisy.transposed.rw` is the set of replicate weights
# that you might now feel comfortable disclosing to your users.

# bee tee dubs

# you should un-transpose the weights rightaboutnow
noisy.rw <- t( noisy.transposed.rw )


# # # # # # # # # # # # # # # # # # # # #
# step six: check with your legal dept. #
# # # # # # # # # # # # # # # # # # # # #

# brilliant malicious users might still be able to identify certain records
# if they work really really really hard and have access to other information
# included in your survey microdata set.

# for example:
# if your technical documentation says that memphis was one of your sampled clusters, and
# if your microdata does have a state identifier, and
# if the content of your survey was about barbeque consumption

# it's possible that no amount of masking, obfuscating, massaging, noising, whathaveyouing
# will prevent malicious users from determining the geography of some of the records
# included in your public use file.  so don't mindlessly follow this example.

# consider exactly what you're disclosing, consider how someone might use it improperly.


# # # # # # # # # # # # # # # # # # # # # # #
# you've protected cluster confidentiality  #

#############################################
# # # # # # # # # # # # # # # # # # # # # # #
# part three. share these replicate weights #
# # # # # # # # # # # # # # # # # # # # # # #
#############################################

# now share these weights with your users.  #
# # # # # # # # # # # # # # # # # # # # # # #


# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# step seven: remove the confidential fields from your data #
# and tack on the replicate weight columns created above.   #
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #


# go back to our original data.frame object `apistrat`
# and store that entire table into `x`
x <- apistrat
# remove the columns that were deemed too confidential to release
x$dnum <- x$stype <- NULL
# in other words, delete the cluster and strata variables
# from this data.frame object so you have a "safe-for-the-public" data set


# look at the first six records
# to confirm they have been removed
head( x )

# merge on the replicate weight data.frame
# that we just created, which contains
# obfuscated-confidential information for users
y <- cbind( x , noisy.rw )

# look at the first six records
# to confirm the weights have been tacked on
head( y )
# the replicate weights are now stored
# as `X1` through `X162`

# this data.frame object `y`
# contains all of the information that
# a user needs to correctly calculate a
# variance, standard error, confidence interval

# uncomment this line to
# export `y` to a csv file
# write.csv( y , "C:/My Directory/your microdata.csv" )
# this csv file contains your full microdata set,
# except for the cluster and strata variables
# that you had determined to be confidential information


# in other words, now you've got a public-use file (puf)
# that no longer contains identifiable geographic information


# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# step eight: determine the `svrepdesign` specification to  #
# match the survey object above, but without cluster info   #
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #


# once users have a copy of this de-identified
# microdata file, they will need the
# replication-based complex sample design


# there are many permutations of replication-based designs.
# you can find well-documented examples of replicate weighted design setups
# by searching for the text `svrepdesign` here:
# https://github.com/ajdamico/asdfree/search?q=svrepdesign&type=Code


# for the object `y` built above,
# construct the replication-based
# "jackknife delete n" method
# complex sample survey design object
z <-
	svrepdesign(
		data = y ,
		type = "JKn" ,
		repweights = "X[1-9]+" ,
		weights = ~pw ,
		scale = 1 ,
		combined.weights = FALSE ,
		mse = TRUE
	)
# is it giving you a warning about calculating the rscales by itself?
# good.  then you're doing right by me.


# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# step nine: confirm this new replication survey object   #
# matches the standard errors derived from as.svrepdesign #
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #

# this `z` object can now be analyzed
# and the statistics but not the standard errors
# will match the `api.jkn` shown above.
svymean( ~ api99 + api00 , z )
svymean( ~ api99 + api00 , api.jkn )

svyby( ~ api99 + api00 , ~ awards , z , svymean )
svyby( ~ api99 + api00 , ~ awards , api.jkn , svymean )
# see how the standard errors have a-little-more-than-doubled?
# that's because we had to BRING THE NOISE to the replicate weights

# if you re-run this script but lower the value of `hmncyt`
# your standard errors will *decrease* and get closer to
# what they actually are when you have the confidential information.

# that's the trade-off.  re-run this entire script but set `hmncyt <- 0`
# and you'll see some almost-perfect standard errors..
# but doing that would allow malicious users to identify clusters easily

# then re-run it again and set `hmncyt <- 10` and suddenly
# no way in hell can a malicious user identify clustering information
# but your standard errors (and subsequent confidence intervals) are ginormous

# obfuscating your replicate weights is going to make it harder for users
# to detect statistically significant differences when analyzing your microdata.
# no way around that.
# just do your best to minimize the amount of obfuscation.

# dooooo it.  public use microdata are an indisputable good.

```

<!--chapter:end:06-confidentiality.Rmd-->

# American Community Survey (ACS) {-}

The US Census Bureau's annual replacement for the long-form decennial census.

* One table with one row per household and a second table with one row per individual within each household.

* The civilian population of the United States.

* Released annually since 2005.

* Administered and financed by the [US Census Bureau](http://www.census.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available ACS microdata by simply specifying `"acs"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "acs" , output_dir = file.path( path.expand( "~" ) , "ACS" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the ACS catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available ACS microdata files
acs_cat <-
	get_catalog( "acs" ,
		output_dir = file.path( path.expand( "~" ) , "ACS" ) )

# 2011 single-year only
acs_cat <- subset( acs_cat , year == 2011 & time_period == '1-Year' )
# download the microdata to your local computer
lodown( "acs" , acs_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a database-backed complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
library(DBI)
library(MonetDBLite)
library(survey)

acs_design <- readRDS( file.path( path.expand( "~" ) , "ACS" , "acs2011_1yr.rds" ) )

acs_design_with_puerto_rico <- open( acs_design , driver = MonetDBLite() )

# remove puerto rico
acs_design <- subset( acs_design_with_puerto_rico , st != 72 )
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
acs_design <-
	update(
		
		acs_design ,
		
		state_name =
			factor(
				st ,
				levels = 
					c(1L, 2L, 4L, 5L, 6L, 8L, 9L, 10L, 
					11L, 12L, 13L, 15L, 16L, 17L, 18L, 
					19L, 20L, 21L, 22L, 23L, 24L, 25L, 
					26L, 27L, 28L, 29L, 30L, 31L, 32L, 
					33L, 34L, 35L, 36L, 37L, 38L, 39L, 
					40L, 41L, 42L, 44L, 45L, 46L, 47L, 
					48L, 49L, 50L, 51L, 53L, 54L, 55L, 
					56L) ,
				labels =
					c("Alabama", "Alaska", "Arizona", "Arkansas", "California", 
					"Colorado", "Connecticut", "Delaware", "District of Columbia", 
					"Florida", "Georgia", "Hawaii", "Idaho", "Illinois", "Indiana", 
					"Iowa", "Kansas", "Kentucky", "Louisiana", "Maine", "Maryland", 
					"Massachusetts", "Michigan", "Minnesota", "Mississippi", "Missouri", 
					"Montana", "Nebraska", "Nevada", "New Hampshire", "New Jersey", 
					"New Mexico", "New York", "North Carolina", "North Dakota", "Ohio", 
					"Oklahoma", "Oregon", "Pennsylvania", "Rhode Island", "South Carolina", 
					"South Dakota", "Tennessee", "Texas", "Utah", "Vermont", "Virginia", 
					"Washington", "West Virginia", "Wisconsin", "Wyoming")
			) ,
		
		married = as.numeric( mar %in% 1 ) ,
		
		sex = factor( sex , labels = c( 'male' , 'female' ) )
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( acs_design , "sampling" ) != 0 )

svyby( ~ one , ~ state_name , acs_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , acs_design )

svyby( ~ one , ~ state_name , acs_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ povpip , acs_design , na.rm = TRUE )

svyby( ~ povpip , ~ state_name , acs_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ sex , acs_design )

svyby( ~ sex , ~ state_name , acs_design , svymean )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ povpip , acs_design , na.rm = TRUE )

svyby( ~ povpip , ~ state_name , acs_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ sex , acs_design )

svyby( ~ sex , ~ state_name , acs_design , svytotal )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ povpip , acs_design , 0.5 , na.rm = TRUE )

svyby( 
	~ povpip , 
	~ state_name , 
	acs_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ ssip , 
	denominator = ~ pincp , 
	acs_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to California:
```{r eval = FALSE , results = "hide" }
sub_acs_design <- subset( acs_design , st == 6 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ povpip , sub_acs_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ povpip , acs_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ povpip , 
		~ state_name , 
		acs_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( acs_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ povpip , acs_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ povpip , acs_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ povpip , acs_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ married , acs_design ,
	method = "likelihood" )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( povpip ~ married , acs_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ married + sex , 
	acs_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		povpip ~ married + sex , 
		acs_design 
	)

summary( glm_result )
```

## Poverty and Inequality Estimation with `convey` {-}

The R `convey` library estimates measures of income concentration, poverty, inequality, and wellbeing. [This textbook](https://guilhermejacob.github.io/context/) details the available features. As a starting point for ACS users, this code calculates the gini coefficient on complex sample survey data:

```{r eval = FALSE , results = "hide" }
library(convey)
acs_design <- convey_prep( acs_design )

svygini( ~ hincp , acs_design , na.rm = TRUE )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

svytotal( ~I( relp %in% 0:17 ) , acs_design )						# total population
svytotal( ~I( relp %in% 0:15 ) , acs_design )						# housing unit population
svytotal( ~I( relp %in% 16:17 ) , acs_design )						# gq population
svytotal( ~I( relp == 16 ) , acs_design )							# gq institutional population
svytotal( ~I( relp == 17 ) , acs_design )							# gq noninstitutional population
svyby( ~I( relp %in% 0:17 ) , ~ sex , acs_design , svytotal )		# total males & females

# all age categories at once #

svytotal( 
	~I( agep %in% 0:4 ) +
	I( agep %in% 5:9 ) +
	I( agep %in% 10:14 ) +
	I( agep %in% 15:19 ) +
	I( agep %in% 20:24 ) +
	I( agep %in% 25:34 ) +
	I( agep %in% 35:44 ) +
	I( agep %in% 45:54 ) +
	I( agep %in% 55:59 ) +
	I( agep %in% 60:64 ) +
	I( agep %in% 65:74 ) +
	I( agep %in% 75:84 ) +
	I( agep %in% 85:100 ) , 
	acs_design
)

# note: the MOE (margin of error) column can be calculated as the standard error x 1.645 #

```

## Database Shutdown {-}

```{r eval = FALSE , results = "hide" }
close( acs_design , shutdown = TRUE )
```

<!--chapter:end:07-acs.Rmd-->

# National Longitudinal Study of Adolescent to Adult Health (ADDHEALTH) {-}

[![Build Status](https://travis-ci.org/asdfree/addhealth.svg?branch=master)](https://travis-ci.org/asdfree/addhealth) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/addhealth?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/addhealth)

The National Longitudinal Study of Adolescent to Adult Health follows a cohort of teenagers from the 1990s into adulthood.

* Many tables, most with one row per sampled youth respondent.

* A complex sample survey designed to generalize to adolescents in grades 7-12 in the United States during the 1994-95 school year.

* Released at irregular intervals, with 1994-1995, 1996, 2001-2002, and 2008-2009 available and 2016-2018 forthcoming.

* Administered by the [Carolina Population Center](http://www.cpc.unc.edu/) and funded by [a consortium](http://www.cpc.unc.edu/projects/addhealth/about/funders).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available ADDHEALTH microdata by simply specifying `"addhealth"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "addhealth" , output_dir = file.path( path.expand( "~" ) , "ADDHEALTH" ) , 
	your_email = "email@address.com" , 
	your_password = "password" )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the ADDHEALTH catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available ADDHEALTH microdata files
addhealth_cat <-
	get_catalog( "addhealth" ,
		output_dir = file.path( path.expand( "~" ) , "ADDHEALTH" ) , 
		your_email = "email@address.com" , 
		your_password = "password" )

# wave i only
addhealth_cat <- subset( addhealth_cat , wave == "wave i" )
# download the microdata to your local computer
lodown( "addhealth" , addhealth_cat , 
	your_email = "email@address.com" , 
	your_password = "password" )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
options( survey.lonely.psu = "adjust" )

library(survey)

addhealth_df <- 
	readRDS( 
		file.path( path.expand( "~" ) , "ADDHEALTH" , 
		"wave i/wave i consolidated.rds" ) 
	)

addhealth_design <- 
	svydesign( 
		id = ~cluster2 , 
		data = addhealth_df , 
		weights = ~ gswgt1 , 
		nest = TRUE 
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
addhealth_design <- 
	update( 
		addhealth_design , 
		
		one = 1 ,
		
		male = as.numeric( as.numeric( bio_sex ) == 1 ) ,
		
		how_many_hours_of_computer_games = ifelse( h1da10 > 99 , NA , h1da10 ) ,
		
		how_many_hours_of_television = ifelse( h1da8 > 99 , NA , h1da8 )
		
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( addhealth_design , "sampling" ) != 0 )

svyby( ~ one , ~ h1gh25 , addhealth_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , addhealth_design )

svyby( ~ one , ~ h1gh25 , addhealth_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ how_many_hours_of_computer_games , addhealth_design , na.rm = TRUE )

svyby( ~ how_many_hours_of_computer_games , ~ h1gh25 , addhealth_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ h1gh24 , addhealth_design , na.rm = TRUE )

svyby( ~ h1gh24 , ~ h1gh25 , addhealth_design , svymean , na.rm = TRUE )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ how_many_hours_of_computer_games , addhealth_design , na.rm = TRUE )

svyby( ~ how_many_hours_of_computer_games , ~ h1gh25 , addhealth_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ h1gh24 , addhealth_design , na.rm = TRUE )

svyby( ~ h1gh24 , ~ h1gh25 , addhealth_design , svytotal , na.rm = TRUE )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ how_many_hours_of_computer_games , addhealth_design , 0.5 , na.rm = TRUE )

svyby( 
	~ how_many_hours_of_computer_games , 
	~ h1gh25 , 
	addhealth_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ how_many_hours_of_computer_games , 
	denominator = ~ how_many_hours_of_television , 
	addhealth_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to self-reported fair or poor health:
```{r eval = FALSE , results = "hide" }
sub_addhealth_design <- subset( addhealth_design , as.numeric( h1gh1 ) %in% c( 4 , 5 ) )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ how_many_hours_of_computer_games , sub_addhealth_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ how_many_hours_of_computer_games , addhealth_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ how_many_hours_of_computer_games , 
		~ h1gh25 , 
		addhealth_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( addhealth_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ how_many_hours_of_computer_games , addhealth_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ how_many_hours_of_computer_games , addhealth_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ how_many_hours_of_computer_games , addhealth_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ male , addhealth_design ,
	method = "likelihood" )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( how_many_hours_of_computer_games ~ male , addhealth_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ male + h1gh24 , 
	addhealth_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		how_many_hours_of_computer_games ~ male + h1gh24 , 
		addhealth_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for ADDHEALTH users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
addhealth_srvyr_design <- as_survey( addhealth_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
addhealth_srvyr_design %>%
	summarize( mean = survey_mean( how_many_hours_of_computer_games , na.rm = TRUE ) )

addhealth_srvyr_design %>%
	group_by( h1gh25 ) %>%
	summarize( mean = survey_mean( how_many_hours_of_computer_games , na.rm = TRUE ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:08-addhealth.Rmd-->

# Area Health Resource File (AHRF) {-}

[![Build Status](https://travis-ci.org/asdfree/ahrf.svg?branch=master)](https://travis-ci.org/asdfree/ahrf) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/ahrf?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/ahrf)

Though not a survey data set itself, useful to merge onto other microdata.

* One table with one row per county and a second table with one row per state.

* Replaced annually with the latest available county- and state-level statistics.

* Compiled by the United States [Health Services and Resources Administration (HRSA)](http://www.hrsa.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available AHRF microdata by simply specifying `"ahrf"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "ahrf" , output_dir = file.path( path.expand( "~" ) , "AHRF" ) )
```

## Analysis Examples with base R {-}

Load a data frame:

```{r eval = FALSE }
ahrf_df <- readRDS( file.path( path.expand( "~" ) , "AHRF" , "county/AHRF_2016-2017.rds" ) )
```

```{r eval = FALSE }

```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
ahrf_df <- 
	transform( 
		ahrf_df , 
		
		cbsa_indicator_code = 
			factor( 
				1 + as.numeric( f1406715 ) , 
				labels = c( "not metro" , "metro" , "micro" ) 
			) ,
			
		mhi_2014 = f1322614 ,
		
		whole_county_hpsa_2016 = as.numeric( f0978716 == 1 ) ,
		
		census_region = 
			factor( 
				as.numeric( f04439 ) , 
				labels = c( "northeast" , "midwest" , "south" , "west" ) 
			)

	)
	
```

### Unweighted Counts {-}

Count the unweighted number of records in the table, overall and by groups:
```{r eval = FALSE , results = "hide" }
nrow( ahrf_df )

table( ahrf_df[ , "cbsa_indicator_code" ] , useNA = "always" )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
mean( ahrf_df[ , "mhi_2014" ] , na.rm = TRUE )

tapply(
	ahrf_df[ , "mhi_2014" ] ,
	ahrf_df[ , "cbsa_indicator_code" ] ,
	mean ,
	na.rm = TRUE 
)
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
prop.table( table( ahrf_df[ , "census_region" ] ) )

prop.table(
	table( ahrf_df[ , c( "census_region" , "cbsa_indicator_code" ) ] ) ,
	margin = 2
)
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( ahrf_df[ , "mhi_2014" ] , na.rm = TRUE )

tapply(
	ahrf_df[ , "mhi_2014" ] ,
	ahrf_df[ , "cbsa_indicator_code" ] ,
	sum ,
	na.rm = TRUE 
)
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
quantile( ahrf_df[ , "mhi_2014" ] , 0.5 , na.rm = TRUE )

tapply(
	ahrf_df[ , "mhi_2014" ] ,
	ahrf_df[ , "cbsa_indicator_code" ] ,
	quantile ,
	0.5 ,
	na.rm = TRUE 
)
```

### Subsetting {-}

Limit your `data.frame` to California:
```{r eval = FALSE , results = "hide" }
sub_ahrf_df <- subset( ahrf_df , f12424 == "CA" )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
mean( sub_ahrf_df[ , "mhi_2014" ] , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Calculate the variance, overall and by groups:
```{r eval = FALSE , results = "hide" }
var( ahrf_df[ , "mhi_2014" ] , na.rm = TRUE )

tapply(
	ahrf_df[ , "mhi_2014" ] ,
	ahrf_df[ , "cbsa_indicator_code" ] ,
	var ,
	na.rm = TRUE 
)
```

### Regression Models and Tests of Association {-}

Perform a t-test:
```{r eval = FALSE , results = "hide" }
t.test( mhi_2014 ~ whole_county_hpsa_2016 , ahrf_df )
```

Perform a chi-squared test of association:
```{r eval = FALSE , results = "hide" }
this_table <- table( ahrf_df[ , c( "whole_county_hpsa_2016" , "census_region" ) ] )

chisq.test( this_table )
```

Perform a generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	glm( 
		mhi_2014 ~ whole_county_hpsa_2016 + census_region , 
		data = ahrf_df
	)

summary( glm_result )
```

## Analysis Examples with `dplyr` {-}

The R `dplyr` library offers an alternative grammar of data manipulation to base R and SQL syntax. [dplyr](https://github.com/tidyverse/dplyr/) offers many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, and the `tidyverse` style of non-standard evaluation. [This vignette](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html) details the available features. As a starting point for AHRF users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(dplyr)
ahrf_tbl <- tbl_df( ahrf_df )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
ahrf_tbl %>%
	summarize( mean = mean( mhi_2014 , na.rm = TRUE ) )

ahrf_tbl %>%
	group_by( cbsa_indicator_code ) %>%
	summarize( mean = mean( mhi_2014 , na.rm = TRUE ) )
```



<!--chapter:end:09-ahrf.Rmd-->

# American Housing Survey (AHS) {-}

[![Build Status](https://travis-ci.org/asdfree/ahs.svg?branch=master)](https://travis-ci.org/asdfree/ahs) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/ahs?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/ahs)

The American Housing Survey tracks housing structures across the United States.

* A collection of tables, most with one row per housing unit.

* A complex sample survey designed to generalize to both occupied and vacant housing units across the United States and also for about twenty-five metropolitan areas.

* Released more or less biennially since 1973.

* Sponsored by [the Department of Housing and Urban Development (HUD)](https://www.hud.gov/) and conducted by the [U.S. Census Bureau](https://www.census.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available AHS microdata by simply specifying `"ahs"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "ahs" , output_dir = file.path( path.expand( "~" ) , "AHS" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the AHS catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available AHS microdata files
ahs_cat <-
	get_catalog( "ahs" ,
		output_dir = file.path( path.expand( "~" ) , "AHS" ) )

# 2015 only
ahs_cat <- subset( ahs_cat , year == 2015 )
# download the microdata to your local computer
lodown( "ahs" , ahs_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
options( survey.replicates.mse = TRUE )

library(survey)

ahs_df <- 
	readRDS( 
		file.path( path.expand( "~" ) , "AHS" , 
			"2015/national_v1.3/household.rds" 
		) 
	)

ahs_design <- 
	svrepdesign(
		weights = ~weight,
		repweights = "repwgt[1-9]" ,
		type = "Fay" ,
		rho = ( 1 - 1 / sqrt( 4 ) ) ,
		data = ahs_df
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
ahs_design <- 
	update( 
		ahs_design , 

		occupant = 
			ifelse( tenure == 1 , "owner" , 
			ifelse( tenure %in% 2:3 , "renter" , 
				"not occupied" ) ) ,
				
		lotsize =
			factor( lotsize , levels = 1:7 ,
				labels = c( "Less then 1/8 acre" , 
				"1/8 up to 1/4 acre" , "1/4 up to 1/2 acre" ,
				"1/2 up to 1 acre" , "1 up to 5 acres" , 
				"5 up to 10 acres" , "10 acres or more" ) ) ,
				
				
		below_poverty = as.numeric( perpovlvl < 100 )
				
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( ahs_design , "sampling" ) != 0 )

svyby( ~ one , ~ occupant , ahs_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , ahs_design )

svyby( ~ one , ~ occupant , ahs_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ totrooms , ahs_design )

svyby( ~ totrooms , ~ occupant , ahs_design , svymean )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ lotsize , ahs_design , na.rm = TRUE )

svyby( ~ lotsize , ~ occupant , ahs_design , svymean , na.rm = TRUE )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ totrooms , ahs_design )

svyby( ~ totrooms , ~ occupant , ahs_design , svytotal )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ lotsize , ahs_design , na.rm = TRUE )

svyby( ~ lotsize , ~ occupant , ahs_design , svytotal , na.rm = TRUE )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ totrooms , ahs_design , 0.5 )

svyby( 
	~ totrooms , 
	~ occupant , 
	ahs_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE 
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ totrooms , 
	denominator = ~ rent , 
	ahs_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to homes with a garage or carport:
```{r eval = FALSE , results = "hide" }
sub_ahs_design <- subset( ahs_design , garage == 1 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ totrooms , sub_ahs_design )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ totrooms , ahs_design )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ totrooms , 
		~ occupant , 
		ahs_design , 
		svymean 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( ahs_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ totrooms , ahs_design )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ totrooms , ahs_design , deff = TRUE )

# SRS with replacement
svymean( ~ totrooms , ahs_design , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ below_poverty , ahs_design ,
	method = "likelihood" , na.rm = TRUE )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( totrooms ~ below_poverty , ahs_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ below_poverty + lotsize , 
	ahs_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		totrooms ~ below_poverty + lotsize , 
		ahs_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for AHS users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
ahs_srvyr_design <- as_survey( ahs_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
ahs_srvyr_design %>%
	summarize( mean = survey_mean( totrooms ) )

ahs_srvyr_design %>%
	group_by( occupant ) %>%
	summarize( mean = survey_mean( totrooms ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:10-ahs.Rmd-->

# American National Election Study (ANES) {-}

[![Build Status](https://travis-ci.org/asdfree/anes.svg?branch=master)](https://travis-ci.org/asdfree/anes) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/anes?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/anes)

The American National Election Study (ANES) collects information on political belief and behavior from eligible voters in the United States.

* Most tables contain one row per sampled eligible voter.

* A complex sample survey designed to generalize to eligible voters in the United States.

* Time series studies released biennially.

* Administered by [Stanford University](https://iriss.stanford.edu/) and the [University of Michigan](https://www.isr.umich.edu/cps/index.html) and funded by the [National Science Foundation](https://www.nsf.gov).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available ANES microdata by simply specifying `"anes"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "anes" , output_dir = file.path( path.expand( "~" ) , "ANES" ) , 
	your_email = "email@address.com" )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the ANES catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available ANES microdata files
anes_cat <-
	get_catalog( "anes" ,
		output_dir = file.path( path.expand( "~" ) , "ANES" ) , 
		your_email = "email@address.com" )

# 2016 only
anes_cat <- subset( anes_cat , directory == "2016 Time Series Study" )
# download the microdata to your local computer
lodown( "anes" , anes_cat , 
	your_email = "email@address.com" )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
library(survey)

anes_df <- 
	readRDS( 
		file.path( path.expand( "~" ) , "ANES" , 
			"2016 Time Series Study/anes_timeseries_2016_.rds" )
	)

anes_design <-
	svydesign( 
		~v160202 , 
		strata = ~v160201 , 
		data = anes_df , 
		weights = ~v160102 , 
		nest = TRUE 
	)

```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
anes_design <- 
	update( 
		anes_design , 
		
		one = 1 ,
		
		pope_francis_score = ifelse( v162094 %in% 0:100 , v162094 , NA ) ,

		christian_fundamentalist_score = ifelse( v162095 %in% 0:100 , v162095 , NA ) ,
		
		primary_voter = ifelse( v161021 %in% 1:2 , as.numeric( v161021 == 1 ) , NA ) ,

		think_gov_spend =
			factor( v161514 , levels = 1:4 , labels =
				c( 'foreign aid' , 'medicare' , 'national defense' , 'social security' )
			) ,
		
		undoc_kids =
			factor( v161195x , levels = 1:6 , labels =
				c( 'should sent back - favor a great deal' ,
					'should sent back - favor a moderate amount' ,
					'should sent back - favor a little' ,
					'should allow to stay - favor a little' ,
					'should allow to stay - favor a moderate amount' ,
					'should allow to stay - favor a great deal' )
			)

	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( anes_design , "sampling" ) != 0 )

svyby( ~ one , ~ undoc_kids , anes_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , anes_design )

svyby( ~ one , ~ undoc_kids , anes_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ pope_francis_score , anes_design , na.rm = TRUE )

svyby( ~ pope_francis_score , ~ undoc_kids , anes_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ think_gov_spend , anes_design , na.rm = TRUE )

svyby( ~ think_gov_spend , ~ undoc_kids , anes_design , svymean , na.rm = TRUE )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ pope_francis_score , anes_design , na.rm = TRUE )

svyby( ~ pope_francis_score , ~ undoc_kids , anes_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ think_gov_spend , anes_design , na.rm = TRUE )

svyby( ~ think_gov_spend , ~ undoc_kids , anes_design , svytotal , na.rm = TRUE )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ pope_francis_score , anes_design , 0.5 , na.rm = TRUE )

svyby( 
	~ pope_francis_score , 
	~ undoc_kids , 
	anes_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ christian_fundamentalist_score , 
	denominator = ~ pope_francis_score , 
	anes_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to party id: independent:
```{r eval = FALSE , results = "hide" }
sub_anes_design <- subset( anes_design , v161158x == 4 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ pope_francis_score , sub_anes_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ pope_francis_score , anes_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ pope_francis_score , 
		~ undoc_kids , 
		anes_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( anes_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ pope_francis_score , anes_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ pope_francis_score , anes_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ pope_francis_score , anes_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ primary_voter , anes_design ,
	method = "likelihood" , na.rm = TRUE )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( pope_francis_score ~ primary_voter , anes_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ primary_voter + think_gov_spend , 
	anes_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		pope_francis_score ~ primary_voter + think_gov_spend , 
		anes_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for ANES users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
anes_srvyr_design <- as_survey( anes_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
anes_srvyr_design %>%
	summarize( mean = survey_mean( pope_francis_score , na.rm = TRUE ) )

anes_srvyr_design %>%
	group_by( undoc_kids ) %>%
	summarize( mean = survey_mean( pope_francis_score , na.rm = TRUE ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:11-anes.Rmd-->

# American Time Use Survey (ATUS) {-}

[![Build Status](https://travis-ci.org/asdfree/atus.svg?branch=master)](https://travis-ci.org/asdfree/atus) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/atus?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/atus)

The American Time Use Survey (ATUS) collects information about how we spend our time. Sampled individuals write down everything they do for a single twenty-four hour period, in ten minute intervals. Many economists use ATUS to study uncompensated work (chores and childcare), but you can use it to learn that even in the dead of night, one-twentieth of us are awake.

* Many tables with different structures [described in the user guide](https://www.bls.gov/tus/atususersguide.pdf#page=33).

* A complex sample survey designed to generalize to the number of person-hours in the civilian non-institutional population of the United States aged older than fourteen.

* Released annually since 2003.

* Administered by the [Bureau of Labor Statistics](https://www.bls.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available ATUS microdata by simply specifying `"atus"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "atus" , output_dir = file.path( path.expand( "~" ) , "ATUS" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the ATUS catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available ATUS microdata files
atus_cat <-
	get_catalog( "atus" ,
		output_dir = file.path( path.expand( "~" ) , "ATUS" ) )

# 2015 only
atus_cat <- subset( atus_cat , directory == 2015 )
# download the microdata to your local computer
lodown( "atus" , atus_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
options( survey.replicates.mse = TRUE )
library(survey)

atusact <- readRDS( file.path( path.expand( "~" ) , "ATUS" , "2015/atusact.rds" ) )
atusact <- atusact[ c( 'tucaseid' , 'tutier1code' , 'tutier2code' , 'tuactdur24' ) ]

atusresp <- readRDS( file.path( path.expand( "~" ) , "ATUS" , "2015/atusresp.rds" ) )
atusresp <- atusresp[ c( 'tucaseid' , 'tufinlwgt' , 'tulineno' ) ]

atusrost <- readRDS( file.path( path.expand( "~" ) , "ATUS" , "2015/atusrost.rds" ) )
atusrost <- atusrost[ , c( 'tucaseid' , 'tulineno' , 'teage' , 'tesex' ) ]

atuswgts <- readRDS( file.path( path.expand( "~" ) , "ATUS" , "2015/atuswgts.rds" ) )
atuswgts <- atuswgts[ , c( 1 , grep( 'finlwgt' , names( atuswgts ) ) ) ]

# looking at the 2012 lexicon, travel-related activities
# have a tier 1 code of 18 --
# http://www.bls.gov/tus/lexiconnoex2012.pdf#page=22

# for all records where the tier 1 code is 18 (travel)
# replace that tier 1 of 18 with whatever's stored in tier 2
atusact[ atusact$tutier1code == 18 , 'tutier1code' ] <- atusact[ atusact$tutier1code == 18 , 'tutier2code' ]
# this will distribute all travel-related activities
# to the appropriate tier 1 category, which matches
# the structure of the 2012 bls table available at
# http://www.bls.gov/tus/tables/a1_2012.pdf

# sum up activity duration at the respondent-level
# *and* also the tier 1 code level
# (using tucaseid as the unique identifier)
# from the activities file
x <- aggregate( tuactdur24 ~ tucaseid + tutier1code , data = atusact , sum )

# now table `x` contains
# one record per person per major activity category

# reshape this data from "long" to "wide" format,
# creating a one-record-per-person table
y <- reshape( x , idvar = 'tucaseid' , timevar = 'tutier1code' , direction = 'wide' )

y[ is.na( y ) ] <- 0
# convert all missings to zeroes,
# since those individuals simply did not
# engage in those activities during their interview day
# (meaning they should have zero minutes of time)

# except for the first column (the unique identifier,
# replace each column by the quotient of itself and sixty
y[ , -1 ] <- y[ , -1 ] / 60
# now you've got an activity file `y`
# with one record per respondent

# merge together the data.frame objects with all needed columns
# in order to create a replicate-weighted survey design object

# merge the respondent file with the newly-created activity file
# (which, remember, is also one-record-per-respondent)
resp_y <- merge( atusresp , y )

# confirm that the result of the merge has the same number of records
# as the original bls atus respondent file. (this is a worthwhile check)
stopifnot( nrow( resp_y ) == nrow( atusresp ) )

# merge that result with the roster file
# note that the roster file has multiple records per `tucaseid`
# but only the `tulineno` columns equal to 1 will match
# records in the original respondent file, this merge works.
resp_y_rost <- merge( resp_y , atusrost )

# confirm that the result of the merge has the same number of records
stopifnot( nrow( resp_y_rost ) == nrow( atusresp ) )

# merge that result with the replicate weights file
z <- merge( resp_y_rost , atuswgts )

# confirm that the result of the merge has the same number of records
stopifnot( nrow( z ) == nrow( atusresp ) )

# remove dots from column names
names( z ) <- gsub( "\\." , "_" , names( z ) )

# add a column of ones
z$one <- 1

atus_design <- 
	svrepdesign(
		weights = ~tufinlwgt ,
		repweights = "finlwgt[1-9]" , 
		type = "Fay" , 
		rho = ( 1 - 1 / sqrt( 4 ) ) ,
		data = z
	)

```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }

atus_design <-
	update(
		atus_design ,
		any_care = as.numeric( tuactdur24_3 > 0 ) ,
		age_category = 
			factor( 
				1 + findInterval( teage , c( 18 , 35 , 65 ) ) , 
				labels = c( "under 18" , "18 - 34" , "35 - 64" , "65 or older" ) 
			)
	)
# caring for and helping household members row
# which we know is top level 03 from
# http://www.bls.gov/tus/lexiconnoex2012.pdf

```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( atus_design , "sampling" ) != 0 )

svyby( ~ one , ~ age_category , atus_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , atus_design )

svyby( ~ one , ~ age_category , atus_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ tuactdur24_1 , atus_design )

svyby( ~ tuactdur24_1 , ~ age_category , atus_design , svymean )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ tesex , atus_design )

svyby( ~ tesex , ~ age_category , atus_design , svymean )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ tuactdur24_1 , atus_design )

svyby( ~ tuactdur24_1 , ~ age_category , atus_design , svytotal )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ tesex , atus_design )

svyby( ~ tesex , ~ age_category , atus_design , svytotal )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ tuactdur24_1 , atus_design , 0.5 )

svyby( 
	~ tuactdur24_1 , 
	~ age_category , 
	atus_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE 
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ tuactdur24_5 , 
	denominator = ~ tuactdur24_12 , 
	atus_design 
)
```

### Subsetting {-}

Restrict the survey design to any time volunteering:
```{r eval = FALSE , results = "hide" }
sub_atus_design <- subset( atus_design , tuactdur24_15 > 0 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ tuactdur24_1 , sub_atus_design )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ tuactdur24_1 , atus_design )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ tuactdur24_1 , 
		~ age_category , 
		atus_design , 
		svymean 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( atus_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ tuactdur24_1 , atus_design )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ tuactdur24_1 , atus_design , deff = TRUE )

# SRS with replacement
svymean( ~ tuactdur24_1 , atus_design , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ any_care , atus_design ,
	method = "likelihood" )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( tuactdur24_1 ~ any_care , atus_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ any_care + tesex , 
	atus_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		tuactdur24_1 ~ any_care + tesex , 
		atus_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for ATUS users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
atus_srvyr_design <- as_survey( atus_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
atus_srvyr_design %>%
	summarize( mean = survey_mean( tuactdur24_1 ) )

atus_srvyr_design %>%
	group_by( age_category ) %>%
	summarize( mean = survey_mean( tuactdur24_1 ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:12-atus.Rmd-->

# Behavioral Risk Factor Surveillance System (BRFSS) {-}

[![Build Status](https://travis-ci.org/asdfree/brfss.svg?branch=master)](https://travis-ci.org/asdfree/brfss) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/brfss?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/brfss)

A health behavior telephone interview survey with enough sample size to examine all fifty states.

* One table with one row per telephone respondent.

* A complex sample survey designed to generalize to the civilian non-institutional adult population of the United States.

* Released annually since 1984 but all states did not participate until 1994.

* Administered by the [Centers for Disease Control and Prevention](http://www.cdc.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available BRFSS microdata by simply specifying `"brfss"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "brfss" , output_dir = file.path( path.expand( "~" ) , "BRFSS" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the BRFSS catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available BRFSS microdata files
brfss_cat <-
	get_catalog( "brfss" ,
		output_dir = file.path( path.expand( "~" ) , "BRFSS" ) )

# 2016 only
brfss_cat <- subset( brfss_cat , year == 2016 )
# download the microdata to your local computer
lodown( "brfss" , brfss_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
options( survey.lonely.psu = "adjust" )

library(survey)

brfss_df <- 
	readRDS( file.path( path.expand( "~" ) , "BRFSS" , "2016 main.rds" ) )

variables_to_keep <-
	c( 'one' , 'xpsu' , 'xststr' , 'xllcpwt' , 'genhlth' , 'medcost' , 
	'xstate' , 'xage80' , 'nummen' , 'numadult' , 'hlthpln1' )
	
brfss_df <- brfss_df[ variables_to_keep ] ; gc()
	
brfss_design <-
	svydesign(
		id = ~ xpsu ,
		strata = ~ xststr ,
		data = brfss_df ,
		weight = ~ xllcpwt ,
		nest = TRUE
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
brfss_design <- 
	update( 
		brfss_design ,
		
		fair_or_poor_health = ifelse( genhlth %in% 1:5 , as.numeric( genhlth > 3 ) , NA ) ,
		
		could_not_see_doctor_due_to_cost = 
			factor( 
				medcost , 
				levels = c( 1 , 2 , 7 , 9 ) , 
				labels = c( "yes" , "no" , "dk" , "rf" ) 
			) ,
		
		state_name =
		
			factor(
			
				xstate ,
				
				levels = 
					c(1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 
					21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 
					37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 
					55, 56, 66, 72, 78) ,
					
				labels = 
					c("ALABAMA", "ALASKA", "ARIZONA", "ARKANSAS", "CALIFORNIA", 
					"COLORADO", "CONNECTICUT", "DELAWARE", "DISTRICT OF COLUMBIA", 
					"FLORIDA", "GEORGIA", "HAWAII", "IDAHO", "ILLINOIS", "INDIANA",
					"IOWA", "KANSAS", "KENTUCKY", "LOUISIANA", "MAINE", "MARYLAND",
					"MASSACHUSETTS", "MICHIGAN", "MINNESOTA", "MISSISSIPPI", 
					"MISSOURI", "MONTANA", "NEBRASKA", "NEVADA", "NEW HAMPSHIRE",
					"NEW JERSEY", "NEW MEXICO", "NEW YORK", "NORTH CAROLINA", 
					"NORTH DAKOTA", "OHIO", "OKLAHOMA", "OREGON", "PENNSYLVANIA",
					"RHODE ISLAND", "SOUTH CAROLINA", "SOUTH DAKOTA", "TENNESSEE",
					"TEXAS", "UTAH", "VERMONT", "VIRGINIA", "WASHINGTON",
					"WEST VIRGINIA", "WISCONSIN", "WYOMING", "GUAM", "PUERTO RICO",
					"U.S. VIRGIN ISLANDS")
					
			)
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( brfss_design , "sampling" ) != 0 )

svyby( ~ one , ~ state_name , brfss_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , brfss_design )

svyby( ~ one , ~ state_name , brfss_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ xage80 , brfss_design )

svyby( ~ xage80 , ~ state_name , brfss_design , svymean )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ could_not_see_doctor_due_to_cost , brfss_design , na.rm = TRUE )

svyby( ~ could_not_see_doctor_due_to_cost , ~ state_name , brfss_design , svymean , na.rm = TRUE )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ xage80 , brfss_design )

svyby( ~ xage80 , ~ state_name , brfss_design , svytotal )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ could_not_see_doctor_due_to_cost , brfss_design , na.rm = TRUE )

svyby( ~ could_not_see_doctor_due_to_cost , ~ state_name , brfss_design , svytotal , na.rm = TRUE )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ xage80 , brfss_design , 0.5 )

svyby( 
	~ xage80 , 
	~ state_name , 
	brfss_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE 
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ nummen , 
	denominator = ~ numadult , 
	brfss_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to persons without health insurance:
```{r eval = FALSE , results = "hide" }
sub_brfss_design <- subset( brfss_design , hlthpln1 == 2 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ xage80 , sub_brfss_design )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ xage80 , brfss_design )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ xage80 , 
		~ state_name , 
		brfss_design , 
		svymean 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( brfss_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ xage80 , brfss_design )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ xage80 , brfss_design , deff = TRUE )

# SRS with replacement
svymean( ~ xage80 , brfss_design , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ fair_or_poor_health , brfss_design ,
	method = "likelihood" , na.rm = TRUE )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( xage80 ~ fair_or_poor_health , brfss_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ fair_or_poor_health + could_not_see_doctor_due_to_cost , 
	brfss_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		xage80 ~ fair_or_poor_health + could_not_see_doctor_due_to_cost , 
		brfss_design 
	)

summary( glm_result )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:13-brfss.Rmd-->

# Basic Stand Alone Public Use File (BSAPUF) {-}

The CMS Basic Stand Alone Public Use File (BSAPUF) contains a five percent sample of Medicare beneficiary spending and utilization in the enrolled population.

* Multiple non-linkable tables, each with one row per beneficiary event.

* The population of elderly and disabled individuals covered by [fee-for-service](https://en.wikipedia.org/wiki/Fee-for-service) Medicare in the United States.

* No listed update frequency.

* Maintained by the United States [Centers for Medicare & Medicaid Services (CMS)](http://www.cms.gov/)

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available BSAPUF microdata by simply specifying `"bsapuf"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "bsapuf" , output_dir = file.path( path.expand( "~" ) , "BSAPUF" ) )
```

## Analysis Examples with SQL and `MonetDBLite` {-}

Connect to a database:

```{r eval = FALSE }
library(DBI)
dbdir <- file.path( path.expand( "~" ) , "BSAPUF" , "MonetDB" )
db <- dbConnect( MonetDBLite::MonetDBLite() , dbdir )
```

```{r eval = FALSE }

```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
dbSendQuery( 
	db , 
	"ALTER TABLE bsa_partd_events_2008 ADD COLUMN brand_name_drug INTEGER" 
)

dbSendQuery( db , 
	"UPDATE bsa_partd_events_2008 
	SET brand_name_drug = 
		CASE 
			WHEN pde_drug_type_cd = 1 THEN 1 
			WHEN pde_drug_type_cd = 2 THEN 0 
			ELSE NULL 
		END" 
)
```

### Unweighted Counts {-}

Count the unweighted number of records in the SQL table, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT COUNT(*) FROM bsa_partd_events_2008" )

dbGetQuery( db ,
	"SELECT
		bene_sex_ident_cd ,
		COUNT(*) 
	FROM bsa_partd_events_2008
	GROUP BY bene_sex_ident_cd"
)
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT AVG( pde_drug_cost ) FROM bsa_partd_events_2008" )

dbGetQuery( db , 
	"SELECT 
		bene_sex_ident_cd , 
		AVG( pde_drug_cost ) AS mean_pde_drug_cost
	FROM bsa_partd_events_2008 
	GROUP BY bene_sex_ident_cd" 
)
```

Initiate a function that allows division by zero:
```{r eval = FALSE , results = "hide" }
dbSendQuery( db , 
	"CREATE FUNCTION 
		div_noerror(l DOUBLE, r DOUBLE) 
	RETURNS DOUBLE 
	EXTERNAL NAME calc.div_noerror" 
)
```

Calculate the distribution of a categorical variable:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		bene_age_cat_cd , 
		div_noerror( 
			COUNT(*) , 
			( SELECT COUNT(*) FROM bsa_partd_events_2008 ) 
		) AS share_bene_age_cat_cd
	FROM bsa_partd_events_2008 
	GROUP BY bene_age_cat_cd" 
)
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT SUM( pde_drug_cost ) FROM bsa_partd_events_2008" )

dbGetQuery( db , 
	"SELECT 
		bene_sex_ident_cd , 
		SUM( pde_drug_cost ) AS sum_pde_drug_cost 
	FROM bsa_partd_events_2008 
	GROUP BY bene_sex_ident_cd" 
)
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT QUANTILE( pde_drug_cost , 0.5 ) FROM bsa_partd_events_2008" )

dbGetQuery( db , 
	"SELECT 
		bene_sex_ident_cd , 
		QUANTILE( pde_drug_cost , 0.5 ) AS median_pde_drug_cost
	FROM bsa_partd_events_2008 
	GROUP BY bene_sex_ident_cd" 
)
```

### Subsetting {-}

Limit your SQL analysis to events where patient paid 100% of drug's cost with `WHERE`:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db ,
	"SELECT
		AVG( pde_drug_cost )
	FROM bsa_partd_events_2008
	WHERE pde_drug_pat_pay_cd = 3"
)
```

### Measures of Uncertainty {-}

Calculate the variance and standard deviation, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		VAR_SAMP( pde_drug_cost ) , 
		STDDEV_SAMP( pde_drug_cost ) 
	FROM bsa_partd_events_2008" 
)

dbGetQuery( db , 
	"SELECT 
		bene_sex_ident_cd , 
		VAR_SAMP( pde_drug_cost ) AS var_pde_drug_cost ,
		STDDEV_SAMP( pde_drug_cost ) AS stddev_pde_drug_cost
	FROM bsa_partd_events_2008 
	GROUP BY bene_sex_ident_cd" 
)
```

### Regression Models and Tests of Association {-}

Calculate the correlation between two variables, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		CORR( CAST( brand_name_drug AS DOUBLE ) , CAST( pde_drug_cost AS DOUBLE ) )
	FROM bsa_partd_events_2008" 
)

dbGetQuery( db , 
	"SELECT 
		bene_sex_ident_cd , 
		CORR( CAST( brand_name_drug AS DOUBLE ) , CAST( pde_drug_cost AS DOUBLE ) )
	FROM bsa_partd_events_2008 
	GROUP BY bene_sex_ident_cd" 
)
```

## Analysis Examples with `dplyr` {-}

The R `dplyr` library offers an alternative grammar of data manipulation to base R and SQL syntax. [dplyr](https://github.com/tidyverse/dplyr/) offers many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, and the `tidyverse` style of non-standard evaluation. [This vignette](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html) details the available features. As a starting point for BSAPUF users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(dplyr)
dplyr_db <- MonetDBLite::src_monetdblite( dbdir )
bsapuf_tbl <- tbl( dplyr_db , 'bsa_partd_events_2008' )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
bsapuf_tbl %>%
	summarize( mean = mean( pde_drug_cost ) )

bsapuf_tbl %>%
	group_by( bene_sex_ident_cd ) %>%
	summarize( mean = mean( pde_drug_cost ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT COUNT(*) FROM bsa_partd_events_2008" )
```

## Database Shutdown {-}

```{r eval = FALSE , results = "hide" }
dbDisconnect( db , shutdown = TRUE )
```

<!--chapter:end:14-bsapuf.Rmd-->

# Brazilian Censo Demografico (CENSO) {-}

*Contributed by Dr. Djalma Pessoa <<pessoad@gmail.com>>*

Brazil's decennial census.

* One table with one row per household and a second table with one row per individual within each household. The 2000 Censo also includes a table with one record per family inside each household.

* An enumeration of the civilian non-institutional population of Brazil.

* Released decennially by IBGE since 2000, however earlier extracts are available from IPUMS International.

* Administered by the [Instituto Brasileiro de Geografia e Estatistica](http://www.ibge.gov.br/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available CENSO microdata by simply specifying `"censo"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "censo" , output_dir = file.path( path.expand( "~" ) , "CENSO" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the CENSO catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available CENSO microdata files
censo_cat <-
	get_catalog( "censo" ,
		output_dir = file.path( path.expand( "~" ) , "CENSO" ) )

# 2010 only
censo_cat <- subset( censo_cat , year == 2010 )
# download the microdata to your local computer
lodown( "censo" , censo_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a database-backed complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
library(DBI)
library(MonetDBLite)
library(survey)

options( survey.lonely.psu = "adjust" )

censo_design <- readRDS( file.path( path.expand( "~" ) , "CENSO" , "pes 2010 design.rds" ) )

censo_design <- open( censo_design , driver = MonetDBLite() )
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
censo_design <-
	update(
		
		censo_design ,
		
		nmorpob1 = ifelse( v6531 >= 0 , as.numeric( v6531 < 70 ) , NA ) ,
		nmorpob2 = ifelse( v6531 >= 0 , as.numeric( v6531 < 80 ) , NA ) , 
		nmorpob3 = ifelse( v6531 >= 0 , as.numeric( v6531 < 90 ) , NA ) , 
		nmorpob4 = ifelse( v6531 >= 0 , as.numeric( v6531 < 100 ) , NA ) , 
		nmorpob5 = ifelse( v6531 >= 0 , as.numeric( v6531 < 140 ) , NA ) , 
		nmorpob6 = ifelse( v6531 >= 0 , as.numeric( v6531 < 272.50 ) , NA ) ,
		
		sexo = factor( v0601 , labels = c( "masculino" , "feminino" ) ) ,
		
		state_name = 
			factor( 
				v0001 , 
				levels = c( 11:17 , 21:29 , 31:33 , 35 , 41:43 , 50:53 ) ,
				labels = c( "Rondonia" , "Acre" , "Amazonas" , 
				"Roraima" , "Para" , "Amapa" , "Tocantins" , 
				"Maranhao" , "Piaui" , "Ceara" , "Rio Grande do Norte" , 
				"Paraiba" , "Pernambuco" , "Alagoas" , "Sergipe" , 
				"Bahia" , "Minas Gerais" , "Espirito Santo" , 
				"Rio de Janeiro" , "Sao Paulo" , "Parana" , 
				"Santa Catarina" , "Rio Grande do Sul" , 
				"Mato Grosso do Sul" , "Mato Grosso" , "Goias" , 
				"Distrito Federal" )
			)
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( censo_design , "sampling" ) != 0 )

svyby( ~ one , ~ state_name , censo_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , censo_design )

svyby( ~ one , ~ state_name , censo_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ v6033 , censo_design )

svyby( ~ v6033 , ~ state_name , censo_design , svymean )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ sexo , censo_design )

svyby( ~ sexo , ~ state_name , censo_design , svymean )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ v6033 , censo_design )

svyby( ~ v6033 , ~ state_name , censo_design , svytotal )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ sexo , censo_design )

svyby( ~ sexo , ~ state_name , censo_design , svytotal )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ v6033 , censo_design , 0.5 )

svyby( 
	~ v6033 , 
	~ state_name , 
	censo_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE 
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ nmorpob1 , 
	denominator = ~ nmorpob1 + one , 
	censo_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to married persons:
```{r eval = FALSE , results = "hide" }
sub_censo_design <- subset( censo_design , v0640 == 1 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ v6033 , sub_censo_design )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ v6033 , censo_design )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ v6033 , 
		~ state_name , 
		censo_design , 
		svymean 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( censo_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ v6033 , censo_design )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ v6033 , censo_design , deff = TRUE )

# SRS with replacement
svymean( ~ v6033 , censo_design , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ nmorpob6 , censo_design ,
	method = "likelihood" , na.rm = TRUE )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( v6033 ~ nmorpob6 , censo_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ nmorpob6 + sexo , 
	censo_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		v6033 ~ nmorpob6 + sexo , 
		censo_design 
	)

summary( glm_result )
```

## Poverty and Inequality Estimation with `convey` {-}

The R `convey` library estimates measures of income concentration, poverty, inequality, and wellbeing. [This textbook](https://guilhermejacob.github.io/context/) details the available features. As a starting point for CENSO users, this code calculates the gini coefficient on complex sample survey data:

```{r eval = FALSE , results = "hide" }
library(convey)
censo_design <- convey_prep( censo_design )

sub_censo_design <- 
	subset( censo_design , v6531 >= 0 )

svygini( ~ v6531 , sub_censo_design , na.rm = TRUE )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```

## Database Shutdown {-}

```{r eval = FALSE , results = "hide" }
close( censo_design , shutdown = TRUE )
```

<!--chapter:end:15-censo.Rmd-->

# Censo Escolar (CENSO_ESCOLAR) {-}

*Contributed by Guilherme Jacob <<guilhermejacob91@gmail.com>>*

The Brazilian School Census (Censo Escolar) is a massive source of information about basic education. Synthetic tables can be produced using the [InepData interface](http://inepdata.inep.gov.br/analytics/saw.dll?Dashboard).

* Since 2007, each year is composed of 4 tables, containing information about students, teachers, schools and classes.

* A census of schools, students, teachers and classes in Brazil.

* Released annually since 1995.

* Administered by the [Anisio Teixeira National Institute for Educational Study and Research (INEP)](http://portal.inep.gov.br), a branch of the Brazilian Ministry of Education.

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available CENSO_ESCOLAR microdata by simply specifying `"censo_escolar"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "censo_escolar" , output_dir = file.path( path.expand( "~" ) , "CENSO_ESCOLAR" ) )
```

## Analysis Examples with SQL and `MonetDBLite` {-}

Connect to a database:

```{r eval = FALSE }
library(DBI)
dbdir <- file.path( path.expand( "~" ) , "CENSO_ESCOLAR" , "MonetDB" )
db <- dbConnect( MonetDBLite::MonetDBLite() , dbdir )
```

```{r eval = FALSE }

```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
dbSendQuery( db , "ALTER TABLE matricula_2015 ADD COLUMN id_indigenous_area INTEGER" )

dbSendQuery( db ,
	"UPDATE matricula_2015
	SET id_indigenous_area =
		CASE WHEN ( tp_localizacao_diferenciada IN (2,5) ) THEN 1 ELSE 0 END"
)

dbSendQuery( db , "ALTER TABLE matricula_2015 ADD COLUMN id_public INTEGER" )

dbSendQuery( db ,
	"UPDATE matricula_2015
	SET id_public =
		CASE WHEN ( tp_dependencia <> 4 ) THEN 1 ELSE 0 END"
)
```

### Unweighted Counts {-}

Count the unweighted number of records in the SQL table, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT COUNT(*) FROM matricula_2015" )

dbGetQuery( db ,
	"SELECT
		tp_localizacao ,
		COUNT(*) 
	FROM matricula_2015
	GROUP BY tp_localizacao"
)
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT AVG( nu_idade ) FROM matricula_2015" )

dbGetQuery( db , 
	"SELECT 
		tp_localizacao , 
		AVG( nu_idade ) AS mean_nu_idade
	FROM matricula_2015 
	GROUP BY tp_localizacao" 
)
```

Initiate a function that allows division by zero:
```{r eval = FALSE , results = "hide" }
dbSendQuery( db , 
	"CREATE FUNCTION 
		div_noerror(l DOUBLE, r DOUBLE) 
	RETURNS DOUBLE 
	EXTERNAL NAME calc.div_noerror" 
)
```

Calculate the distribution of a categorical variable:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		tp_categoria_escola_privada , 
		div_noerror( 
			COUNT(*) , 
			( SELECT COUNT(*) FROM matricula_2015 ) 
		) AS share_tp_categoria_escola_privada
	FROM matricula_2015 
	GROUP BY tp_categoria_escola_privada" 
)
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT SUM( nu_idade ) FROM matricula_2015" )

dbGetQuery( db , 
	"SELECT 
		tp_localizacao , 
		SUM( nu_idade ) AS sum_nu_idade 
	FROM matricula_2015 
	GROUP BY tp_localizacao" 
)
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT QUANTILE( nu_idade , 0.5 ) FROM matricula_2015" )

dbGetQuery( db , 
	"SELECT 
		tp_localizacao , 
		QUANTILE( nu_idade , 0.5 ) AS median_nu_idade
	FROM matricula_2015 
	GROUP BY tp_localizacao" 
)
```

### Subsetting {-}

Limit your SQL analysis to students enrolled in public schools with `WHERE`:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db ,
	"SELECT
		AVG( nu_idade )
	FROM matricula_2015
	WHERE id_public = 1"
)
```

### Measures of Uncertainty {-}

Calculate the variance and standard deviation, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		VAR_SAMP( nu_idade ) , 
		STDDEV_SAMP( nu_idade ) 
	FROM matricula_2015" 
)

dbGetQuery( db , 
	"SELECT 
		tp_localizacao , 
		VAR_SAMP( nu_idade ) AS var_nu_idade ,
		STDDEV_SAMP( nu_idade ) AS stddev_nu_idade
	FROM matricula_2015 
	GROUP BY tp_localizacao" 
)
```

### Regression Models and Tests of Association {-}

Calculate the correlation between two variables, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		CORR( CAST( id_public AS DOUBLE ) , CAST( nu_idade AS DOUBLE ) )
	FROM matricula_2015" 
)

dbGetQuery( db , 
	"SELECT 
		tp_localizacao , 
		CORR( CAST( id_public AS DOUBLE ) , CAST( nu_idade AS DOUBLE ) )
	FROM matricula_2015 
	GROUP BY tp_localizacao" 
)
```

## Analysis Examples with `dplyr` {-}

The R `dplyr` library offers an alternative grammar of data manipulation to base R and SQL syntax. [dplyr](https://github.com/tidyverse/dplyr/) offers many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, and the `tidyverse` style of non-standard evaluation. [This vignette](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html) details the available features. As a starting point for CENSO_ESCOLAR users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(dplyr)
dplyr_db <- MonetDBLite::src_monetdblite( dbdir )
censo_escolar_tbl <- tbl( dplyr_db , 'matricula_2015' )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
censo_escolar_tbl %>%
	summarize( mean = mean( nu_idade ) )

censo_escolar_tbl %>%
	group_by( tp_localizacao ) %>%
	summarize( mean = mean( nu_idade ) )
```

---

## Replication Example {-}

This snippet replicates the first three rows of total enrollment in basic education, as in the Table 1.1 of [this spreadsheet](http://download.inep.gov.br/informacoes_estatisticas/sinopses_estatisticas/sinopses_educacao_basica/sinopse_estatistica_educacao_basica_2015.zip).

```{r eval = FALSE , results = "hide" }
# first row:
dbGetQuery( db ,"SELECT COUNT(*) AS n_mat_tot
 FROM matricula_2015
 WHERE TP_TIPO_TURMA NOT IN (4,5) " )

# second row:
dbGetQuery( db ,"SELECT COUNT(*) AS n_mat_tot
 FROM matricula_2015
 WHERE TP_TIPO_TURMA NOT IN (4,5) AND CO_REGIAO = 1" )

# third row:
dbGetQuery( db ,"SELECT COUNT(*) AS n_mat_tot
 FROM matricula_2015
 WHERE TP_TIPO_TURMA NOT IN (4,5) AND CO_UF = 11 " )
```

## Database Shutdown {-}

```{r eval = FALSE , results = "hide" }
dbDisconnect( db , shutdown = TRUE )
```

<!--chapter:end:16-censo_escolar.Rmd-->

# Consumer Expenditure Survey (CES) {-}

[![Build Status](https://travis-ci.org/asdfree/ces.svg?branch=master)](https://travis-ci.org/asdfree/ces) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/ces?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/ces)

The Consumer Expenditure Survey (CES) is the authoritative data source to understand how Americans spend money. Participating households keep a running diary about every purchase over fifteen months. Those diaries are then summed up into precise expenditure categories.

* One table of survey responses per quarter with one row per sampled household (consumer unit). Additional tables containing one record per expenditure

* A complex sample survey designed to generalize to the civilian non-institutional population of the United States.

* Released annually since 1996.

* Administered by the [Bureau of Labor Statistics](http://www.bls.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available CES microdata by simply specifying `"ces"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "ces" , output_dir = file.path( path.expand( "~" ) , "CES" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the CES catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available CES microdata files
ces_cat <-
	get_catalog( "ces" ,
		output_dir = file.path( path.expand( "~" ) , "CES" ) )

# 2016 only
ces_cat <- subset( ces_cat , year == 2016 )
# download the microdata to your local computer
lodown( "ces" , ces_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a multiply-imputed, complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
options( survey.replicates.mse = TRUE )

library(survey)
library(mitools)

# read in the five quarters of family data files (fmli)

fmli161x <- readRDS( file.path( path.expand( "~" ) , "CES" , "2016/fmli161x.rds" ) )
fmli162 <- readRDS( file.path( path.expand( "~" ) , "CES" , "2016/fmli162.rds" ) )
fmli163 <- readRDS( file.path( path.expand( "~" ) , "CES" , "2016/fmli163.rds" ) )
fmli164 <- readRDS( file.path( path.expand( "~" ) , "CES" , "2016/fmli164.rds" ) )
fmli171 <- readRDS( file.path( path.expand( "~" ) , "CES" , "2016/fmli171.rds" ) )

fmli161x$qtr <- 1
fmli162$qtr <- 2
fmli163$qtr <- 3
fmli164$qtr <- 4
fmli171$qtr <- 5

fmli171 <- fmli171[ , names( fmli161x ) ]

fmly <- rbind( fmli161x , fmli162 , fmli163 , fmli164 , fmli171 )

rm( fmli161x , fmli162 , fmli163 , fmli164 , fmli171 )

wtrep <- c( paste0( "wtrep" , stringr::str_pad( 1:44 , 2 , pad = "0" ) ) , "finlwt21" )

for ( i in wtrep ) fmly[ is.na( fmly[ , i ] ) , i ] <- 0

# create a new variable in the fmly data table called 'totalexp'
# that contains the sum of the total expenditure from the current and previous quarters
fmly$totalexp <- rowSums( fmly[ , c( "totexppq" , "totexpcq" ) ] , na.rm = TRUE )

# immediately convert missing values (NA) to zeroes
fmly[ is.na( fmly$totalexp ) , "totalexp" ] <- 0

# annualize the total expenditure by multiplying the total expenditure by four,
# creating a new variable 'annexp' in the fmly data table
fmly <- transform( fmly , annexp = totalexp * 4 )

# add a column of ones
fmly$one <- 1

# create a vector containing all of the multiply-imputed variables
# (leaving the numbers off the end)
mi_vars <- gsub( "5$" , "" , grep( "[a-z]5$" , names( fmly ) , value = TRUE ) )

# loop through each of the five variables..
for ( i in 1:5 ){

	# copy the 'fmly' table over to a new temporary data frame 'x'
	x <- fmly

	# loop through each of the multiply-imputed variables..
	for ( j in mi_vars ){
	
		# copy the contents of the current column (for example 'welfare1')
		# over to a new column ending in 'mi' (for example 'welfaremi')
		x[ , paste0( j , 'mi' ) ] <- x[ , paste0( j , i ) ]
		
		# delete the all five of the imputed variable columns
		x <- x[ , !( names( x ) %in% paste0( j , 1:5 ) ) ]

	}
	
	# save the current table in the sqlite database as 'imp1' 'imp2' etc.
	assign( paste0( 'imp' , i ) , x )

	# remove the temporary table
	rm( x )
	
}

	
# containing the five multiply-imputed data tables - imp1 through imp5
ces_design <- 
	svrepdesign( 
		weights = ~finlwt21 , 
		repweights = "wtrep[0-9]+" , 
		data = imputationList( list( imp1 , imp2 , imp3 , imp4 , imp5 ) ) , 
		type = "BRR" ,
		combined.weights = TRUE
	)

rm( imp1 , imp2 , imp3 , imp4 , imp5 )
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
ces_design <- 
	update( 
		ces_design , 
		
		any_food_stamp = as.numeric( jfs_amtmi > 0 )
		
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( ces_design , svyby( ~ one , ~ one , unwtd.count ) ) )

MIcombine( with( ces_design , svyby( ~ one , ~ bls_urbn , unwtd.count ) ) )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( ces_design , svytotal( ~ one ) ) )

MIcombine( with( ces_design ,
	svyby( ~ one , ~ bls_urbn , svytotal )
) )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( ces_design , svymean( ~ annexp ) ) )

MIcombine( with( ces_design ,
	svyby( ~ annexp , ~ bls_urbn , svymean )
) )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( ces_design , svymean( ~ sex_ref ) ) )

MIcombine( with( ces_design ,
	svyby( ~ sex_ref , ~ bls_urbn , svymean )
) )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( ces_design , svytotal( ~ annexp ) ) )

MIcombine( with( ces_design ,
	svyby( ~ annexp , ~ bls_urbn , svytotal )
) )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( ces_design , svytotal( ~ sex_ref ) ) )

MIcombine( with( ces_design ,
	svyby( ~ sex_ref , ~ bls_urbn , svytotal )
) )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( ces_design , svyquantile( ~ annexp , 0.5 , se = TRUE ) ) )

MIcombine( with( ces_design ,
	svyby( 
		~ annexp , ~ bls_urbn , svyquantile , 0.5 ,
		se = TRUE , keep.var = TRUE , ci = TRUE 
) ) )
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
MIcombine( with( ces_design ,
	svyratio( numerator = ~ annexp , denominator = ~ fincbtxmi )
) )
```

### Subsetting {-}

Restrict the survey design to california residents:
```{r eval = FALSE , results = "hide" }
sub_ces_design <- subset( ces_design , state == '06' )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
MIcombine( with( sub_ces_design , svymean( ~ annexp ) ) )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <-
	MIcombine( with( ces_design ,
		svymean( ~ annexp )
	) )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	MIcombine( with( ces_design ,
		svyby( ~ annexp , ~ bls_urbn , svymean )
	) )

coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( ces_design$designs[[1]] )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
MIcombine( with( ces_design , svyvar( ~ annexp ) ) )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
MIcombine( with( ces_design ,
	svymean( ~ annexp , deff = TRUE )
) )

# SRS with replacement
MIcombine( with( ces_design ,
	svymean( ~ annexp , deff = "replace" )
) )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
lodown:::MIsvyciprop( ~ any_food_stamp , ces_design ,
	method = "likelihood" )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
lodown:::MIsvyttest( annexp ~ any_food_stamp , ces_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
lodown:::MIsvychisq( ~ any_food_stamp + sex_ref , ces_design )
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	MIcombine( with( ces_design ,
		svyglm( annexp ~ any_food_stamp + sex_ref )
	) )
	
summary( glm_result )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:17-ces.Rmd-->

# California Health Interview Survey (CHIS) {-}

[![Build Status](https://travis-ci.org/asdfree/chis.svg?branch=master)](https://travis-ci.org/asdfree/chis) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/chis?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/chis)

*Contributed by Carl Ganz <<carlganz@gmail.com>>*

The State of California's edition of the National Health Interview Survey (NHIS), a regional healthcare survey for the nation's largest state.

* One adult, one teenage, and one child table, each with one row per sampled respondent.

* A complex sample survey designed to generalize to the civilian non-institutionalized population of California.

* Released annually since 2011, and biennially since 2001.

* Administered by the [UCLA Center for Health Policy Research](http://healthpolicy.ucla.edu/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available CHIS microdata by simply specifying `"chis"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "chis" , output_dir = file.path( path.expand( "~" ) , "CHIS" ) , 
	your_username = "username" , 
	your_password = "password" )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the CHIS catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available CHIS microdata files
chis_cat <-
	get_catalog( "chis" ,
		output_dir = file.path( path.expand( "~" ) , "CHIS" ) , 
		your_username = "username" , 
		your_password = "password" )

# 2014 only
chis_cat <- subset( chis_cat , year == 2014 )
# download the microdata to your local computer
lodown( "chis" , chis_cat , 
	your_username = "username" , 
	your_password = "password" )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
options( survey.replicates.mse = TRUE )

library(survey)

child <- readRDS( file.path( path.expand( "~" ) , "CHIS" , "2014 child.rds" ) )

child$ak7_p1 <- child$ak10_p <- NA
child$agecat <- "1 - child"
child$no_usual_source_of_care <- as.numeric( child$cd1 == 2 )

# four-category srhs (excellent / very good / good / fair+poor)
child$hlthcat <- child$ca6_p1

# load adolescents ages 12-17
teen <- readRDS( file.path( path.expand( "~" ) , "CHIS" , "2014 teen.rds" ) )

teen$ak7_p1 <- teen$ak10_p <- NA
teen$agecat <- "2 - adolescent"
teen$no_usual_source_of_care <- as.numeric( teen$tf1 == 2 )

# four-category srhs (excellent / very good / good / fair+poor)
teen$hlthcat <- teen$tb1_p1

# load adults ages 18+
adult <- readRDS( file.path( path.expand( "~" ) , "CHIS" , "2014 adult.rds" ) )

adult$agecat <- ifelse( adult$srage_p1 >= 65 , "4 - senior" , "3 - adult" )
adult$no_usual_source_of_care <- as.numeric( adult$ah1 == 2 )

# four-category srhs (excellent / very good / good / fair+poor)
adult$hlthcat <- c( 1 , 2 , 3 , 4 , 4 )[ adult$ab1 ]

# construct a character vector with only the variables needed for the analysis
vars_to_keep <- 
	c( grep( "rakedw" , names( adult ) , value = TRUE ) , 
		'hlthcat' , 'agecat' , 'ak7_p1' , 'ak10_p' ,
		'povgwd_p' , 'no_usual_source_of_care' )

chis_df <- 
	rbind( 
		child[ vars_to_keep ] , 
		teen[ vars_to_keep ] , 
		adult[ vars_to_keep ] 
	)

# remove labelled classes
labelled_cols <- 
	sapply( 
		chis_df , 
		function( w ) class( w ) == 'labelled' 
	)

chis_df[ labelled_cols ] <- 
	sapply( 
		chis_df[ labelled_cols ] , 
		as.numeric
	)

chis_design <- 
	svrepdesign( 
		data = chis_df , 
		weights = ~ rakedw0 , 
		repweights = "rakedw[1-9]" , 
		type = "other" , 
		scale = 1 , 
		rscales = 1 , 
		mse = TRUE 
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
chis_design <- 
	update( 
		chis_design , 
		one = 1 ,
		hlthcat = 
			factor( hlthcat , 
				labels = c( 'excellent' , 'very good' , 'good' , 'fair or poor' ) 
			)
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( chis_design , "sampling" ) != 0 )

svyby( ~ one , ~ hlthcat , chis_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , chis_design )

svyby( ~ one , ~ hlthcat , chis_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ povgwd_p , chis_design )

svyby( ~ povgwd_p , ~ hlthcat , chis_design , svymean )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ agecat , chis_design )

svyby( ~ agecat , ~ hlthcat , chis_design , svymean )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ povgwd_p , chis_design )

svyby( ~ povgwd_p , ~ hlthcat , chis_design , svytotal )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ agecat , chis_design )

svyby( ~ agecat , ~ hlthcat , chis_design , svytotal )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ povgwd_p , chis_design , 0.5 )

svyby( 
	~ povgwd_p , 
	~ hlthcat , 
	chis_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE 
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ ak10_p , 
	denominator = ~ ak7_p1 , 
	chis_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to seniors:
```{r eval = FALSE , results = "hide" }
sub_chis_design <- subset( chis_design , agecat == "4 - senior" )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ povgwd_p , sub_chis_design )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ povgwd_p , chis_design )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ povgwd_p , 
		~ hlthcat , 
		chis_design , 
		svymean 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( chis_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ povgwd_p , chis_design )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ povgwd_p , chis_design , deff = TRUE )

# SRS with replacement
svymean( ~ povgwd_p , chis_design , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ no_usual_source_of_care , chis_design ,
	method = "likelihood" )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( povgwd_p ~ no_usual_source_of_care , chis_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ no_usual_source_of_care + agecat , 
	chis_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		povgwd_p ~ no_usual_source_of_care + agecat , 
		chis_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for CHIS users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
chis_srvyr_design <- as_survey( chis_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
chis_srvyr_design %>%
	summarize( mean = survey_mean( povgwd_p ) )

chis_srvyr_design %>%
	group_by( hlthcat ) %>%
	summarize( mean = survey_mean( povgwd_p ) )
```

---

## Replication Example {-}

The example below matches statistics and confidence intervals from this table pulled from the [AskCHIS](http://ask.chis.ucla.edu/) online table creator:

`r knitr::include_graphics("images/askchis.png")`

Match the bottom right weighted count:

```{r eval = FALSE , results = "hide" }
stopifnot( round( coef( svytotal( ~ one , chis_design ) ) , -3 ) == 37582000 )
```
	
Compute the statistics and standard errors for excellent, very good, and good in the rightmost row:

```{r eval = FALSE , results = "hide" }
( total_population_ex_vg_good <- svymean( ~ hlthcat , chis_design ) )

# confirm these match
stopifnot( 
	identical( 
		as.numeric( round( coef( total_population_ex_vg_good ) * 100 , 1 )[ 1:3 ] ) ,
		c( 23.2 , 31.4 , 28.4 )
	)
)
```

Compute the confidence intervals in the rightmost row:

```{r eval = FALSE , results = "hide" }
( total_pop_ci <- confint( total_population_ex_vg_good , df = degf( chis_design ) ) )

# confirm these match
stopifnot(
	identical(
		as.numeric( 
			round( total_pop_ci * 100 , 1 )[ 1:3 , ] 
		) ,
		c( 22.1 , 30.1 , 27.1 , 24.2 , 32.7 , 29.6 )
	)
)
```



<!--chapter:end:18-chis.Rmd-->

# Current Population Survey - Annual Social and Economic Supplement (CPSASEC) {-}

[![Build Status](https://travis-ci.org/asdfree/cpsasec.svg?branch=master)](https://travis-ci.org/asdfree/cpsasec) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/cpsasec?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/cpsasec)

The March Current Population Survey Annual Social and Economic Supplement has supplied the statistics for the US Census Bureau's report on income, poverty, and health insurance coverage since 1948.

* One table with one row per sampled household, a second table with one row per family within each sampled household, and a third table with one row per individual within each of those families.

* A complex sample survey designed to generalize to the civilian non-institutional population of the United States

* Released annually since 1998.

* Administered jointly by the [US Census Bureau](http://www.census.gov/) and the [Bureau of Labor Statistics](http://www.bls.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available CPSASEC microdata by simply specifying `"cpsasec"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "cpsasec" , output_dir = file.path( path.expand( "~" ) , "CPSASEC" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the CPSASEC catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available CPSASEC microdata files
cpsasec_cat <-
	get_catalog( "cpsasec" ,
		output_dir = file.path( path.expand( "~" ) , "CPSASEC" ) )

# 2016 only
cpsasec_cat <- subset( cpsasec_cat , year == 2016 )
# download the microdata to your local computer
lodown( "cpsasec" , cpsasec_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
options( survey.replicates.mse = TRUE )

library(survey)

cpsasec_df <- 
	readRDS( file.path( path.expand( "~" ) , "CPSASEC" , "2016 cps asec.rds" ) )

variables_to_keep <-
	c( 'a_maritl' , 'gestfips' , 'a_sex' , 'ptotval' , 'moop' , 'a_age' , 'htotval' , 
	'one' , 'a_exprrp' , 'marsupwt' , 
	grep( "pwwgt" , names( cpsasec_df ) , value = TRUE ) )
	
cpsasec_df <- cpsasec_df[ variables_to_keep ] ; gc()
	
cpsasec_design <-
	svrepdesign(
		weights = ~ marsupwt ,
		repweights = "pwwgt[1-9]" ,
		type = "Fay" ,
		rho = ( 1 - 1 / sqrt( 4 ) ) ,
		data = cpsasec_df ,
		combined.weights = TRUE
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
cpsasec_design <- 
	update( 
		cpsasec_design , 

		a_maritl = 
			factor( 
				a_maritl , 
				labels = 
					c( 
						"married - civilian spouse present" ,
						"married - AF spouse present" ,
						"married - spouse absent" ,
						"widowed" ,
						"divorced" , 
						"separated" , 
						"never married"
					)
			) ,
			
		state_name =
			factor(
				gestfips ,
				levels = 
					c(1L, 2L, 4L, 5L, 6L, 8L, 9L, 10L, 
					11L, 12L, 13L, 15L, 16L, 17L, 18L, 
					19L, 20L, 21L, 22L, 23L, 24L, 25L, 
					26L, 27L, 28L, 29L, 30L, 31L, 32L, 
					33L, 34L, 35L, 36L, 37L, 38L, 39L, 
					40L, 41L, 42L, 44L, 45L, 46L, 47L, 
					48L, 49L, 50L, 51L, 53L, 54L, 55L, 
					56L) ,
				labels =
					c("Alabama", "Alaska", "Arizona", "Arkansas", "California", 
					"Colorado", "Connecticut", "Delaware", "District of Columbia", 
					"Florida", "Georgia", "Hawaii", "Idaho", "Illinois", "Indiana", 
					"Iowa", "Kansas", "Kentucky", "Louisiana", "Maine", "Maryland", 
					"Massachusetts", "Michigan", "Minnesota", "Mississippi", "Missouri", 
					"Montana", "Nebraska", "Nevada", "New Hampshire", "New Jersey", 
					"New Mexico", "New York", "North Carolina", "North Dakota", "Ohio", 
					"Oklahoma", "Oregon", "Pennsylvania", "Rhode Island", "South Carolina", 
					"South Dakota", "Tennessee", "Texas", "Utah", "Vermont", "Virginia", 
					"Washington", "West Virginia", "Wisconsin", "Wyoming")
			) ,

		male = as.numeric( a_sex == 1 )
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( cpsasec_design , "sampling" ) != 0 )

svyby( ~ one , ~ state_name , cpsasec_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , cpsasec_design )

svyby( ~ one , ~ state_name , cpsasec_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ ptotval , cpsasec_design )

svyby( ~ ptotval , ~ state_name , cpsasec_design , svymean )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ a_maritl , cpsasec_design )

svyby( ~ a_maritl , ~ state_name , cpsasec_design , svymean )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ ptotval , cpsasec_design )

svyby( ~ ptotval , ~ state_name , cpsasec_design , svytotal )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ a_maritl , cpsasec_design )

svyby( ~ a_maritl , ~ state_name , cpsasec_design , svytotal )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ ptotval , cpsasec_design , 0.5 )

svyby( 
	~ ptotval , 
	~ state_name , 
	cpsasec_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE 
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ moop , 
	denominator = ~ ptotval , 
	cpsasec_design 
)
```

### Subsetting {-}

Restrict the survey design to persons aged 18-64:
```{r eval = FALSE , results = "hide" }
sub_cpsasec_design <- subset( cpsasec_design , a_age %in% 18:64 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ ptotval , sub_cpsasec_design )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ ptotval , cpsasec_design )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ ptotval , 
		~ state_name , 
		cpsasec_design , 
		svymean 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( cpsasec_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ ptotval , cpsasec_design )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ ptotval , cpsasec_design , deff = TRUE )

# SRS with replacement
svymean( ~ ptotval , cpsasec_design , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ male , cpsasec_design ,
	method = "likelihood" )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( ptotval ~ male , cpsasec_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ male + a_maritl , 
	cpsasec_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		ptotval ~ male + a_maritl , 
		cpsasec_design 
	)

summary( glm_result )
```

## Poverty and Inequality Estimation with `convey` {-}

The R `convey` library estimates measures of income concentration, poverty, inequality, and wellbeing. [This textbook](https://guilhermejacob.github.io/context/) details the available features. As a starting point for CPSASEC users, this code calculates the gini coefficient on complex sample survey data:

```{r eval = FALSE , results = "hide" }
library(convey)
cpsasec_design <- convey_prep( cpsasec_design )

sub_cpsasec_design <- 
	subset( 
		cpsasec_design , 
		a_exprrp %in% 1:2
	)

svygini( ~ htotval , sub_cpsasec_design )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:19-cpsasec.Rmd-->

# Current Population Survey - Basic Monthly (CPSBASIC) {-}

[![Build Status](https://travis-ci.org/asdfree/cpsbasic.svg?branch=master)](https://travis-ci.org/asdfree/cpsbasic) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/cpsbasic?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/cpsbasic)

The Current Population Survey - Basic Monthly is the monthly labor force survey of the United States.

* One table with one row per sampled youth respondent.

* A complex sample survey designed to generalize to the civilian non-institutional population of the United States

* Released monthly since 1994.

* Administered jointly by the [US Census Bureau](http://www.census.gov/) and the [Bureau of Labor Statistics](http://www.bls.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available CPSBASIC microdata by simply specifying `"cpsbasic"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "cpsbasic" , output_dir = file.path( path.expand( "~" ) , "CPSBASIC" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the CPSBASIC catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available CPSBASIC microdata files
cpsbasic_cat <-
	get_catalog( "cpsbasic" ,
		output_dir = file.path( path.expand( "~" ) , "CPSBASIC" ) )

# march 2017 only
cpsbasic_cat <- subset( cpsbasic_cat , year == 2017 & month == 3 )
# download the microdata to your local computer
lodown( "cpsbasic" , cpsbasic_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
library(survey)

cpsbasic_df <- 
	readRDS( file.path( path.expand( "~" ) , "CPSBASIC" , "2017 03 cps basic.rds" ) )

# construct a fake survey design
warning( "this survey design produces correct point estimates
but incorrect standard errors." )
cpsbasic_design <- 
	svydesign( 
		~ 1 , 
		data = cpsbasic_df , 
		weights = ~ pwsswgt
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
cpsbasic_design <- 
	update( 
		cpsbasic_design , 
		
		one = 1 ,
		
		pesex = factor( pesex , levels = 1:2 , labels = c( 'male' , 'female' ) ) ,
		
		weekly_earnings = ifelse( prernwa == -.01 , NA , prernwa ) ,
		
		# exclude anyone whose hours vary
		weekly_hours = ifelse( pehrusl1 < 0 , NA , pehrusl1 ) ,
		
		class_of_worker =
			factor( peio1cow , levels = 1:8 ,
				labels = 
					c( "government - federal" , "government - state" ,
					"government - local" , "private, for profit" ,
					"private, nonprofit" , "self-employed, incorporated" ,
					"self-employed, unincorporated" , "without pay" )
			) ,
			
		part_time = ifelse( pemlr == 1 , as.numeric( pehruslt < 35 ) , NA )
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( cpsbasic_design , "sampling" ) != 0 )

svyby( ~ one , ~ pesex , cpsbasic_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , cpsbasic_design )

svyby( ~ one , ~ pesex , cpsbasic_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ weekly_earnings , cpsbasic_design , na.rm = TRUE )

svyby( ~ weekly_earnings , ~ pesex , cpsbasic_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ class_of_worker , cpsbasic_design , na.rm = TRUE )

svyby( ~ class_of_worker , ~ pesex , cpsbasic_design , svymean , na.rm = TRUE )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ weekly_earnings , cpsbasic_design , na.rm = TRUE )

svyby( ~ weekly_earnings , ~ pesex , cpsbasic_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ class_of_worker , cpsbasic_design , na.rm = TRUE )

svyby( ~ class_of_worker , ~ pesex , cpsbasic_design , svytotal , na.rm = TRUE )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ weekly_earnings , cpsbasic_design , 0.5 , na.rm = TRUE )

svyby( 
	~ weekly_earnings , 
	~ pesex , 
	cpsbasic_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ weekly_earnings , 
	denominator = ~ weekly_hours , 
	cpsbasic_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to california residents:
```{r eval = FALSE , results = "hide" }
sub_cpsbasic_design <- subset( cpsbasic_design , gestfips == 6 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ weekly_earnings , sub_cpsbasic_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ weekly_earnings , cpsbasic_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ weekly_earnings , 
		~ pesex , 
		cpsbasic_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( cpsbasic_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ weekly_earnings , cpsbasic_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ weekly_earnings , cpsbasic_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ weekly_earnings , cpsbasic_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ part_time , cpsbasic_design ,
	method = "likelihood" , na.rm = TRUE )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( weekly_earnings ~ part_time , cpsbasic_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ part_time + class_of_worker , 
	cpsbasic_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		weekly_earnings ~ part_time + class_of_worker , 
		cpsbasic_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for CPSBASIC users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
cpsbasic_srvyr_design <- as_survey( cpsbasic_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
cpsbasic_srvyr_design %>%
	summarize( mean = survey_mean( weekly_earnings , na.rm = TRUE ) )

cpsbasic_srvyr_design %>%
	group_by( pesex ) %>%
	summarize( mean = survey_mean( weekly_earnings , na.rm = TRUE ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:20-cpsbasic.Rmd-->

# Demographic and Health Surveys (DHS) {-}

[![Build Status](https://travis-ci.org/asdfree/dhs.svg?branch=master)](https://travis-ci.org/asdfree/dhs) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/dhs?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/dhs)

The Demographic and Health Surveys collect data on population, health, HIV, and nutrition in over 90 countries.

* Many tables, often with one row per male, per female, or per responding household.

* A complex sample survey designed to generalize to the residents of various countries.

* Many releases for different countries annually, since 1984.

* Administered by the [ICF International](http://www.icfi.com/) and funded by the [US Agency for International Development](http://www.usaid.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available DHS microdata by simply specifying `"dhs"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "dhs" , output_dir = file.path( path.expand( "~" ) , "DHS" ) , 
	your_email = "email@address.com" , 
	your_password = "password" , 
	your_project = "project" )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the DHS catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available DHS microdata files
dhs_cat <-
	get_catalog( "dhs" ,
		output_dir = file.path( path.expand( "~" ) , "DHS" ) , 
		your_email = "email@address.com" , 
		your_password = "password" , 
		your_project = "project" )

# Malawi only
dhs_cat <- subset( dhs_cat , country == 'Malawi' )
# download the microdata to your local computer
lodown( "dhs" , dhs_cat , 
	your_email = "email@address.com" , 
	your_password = "password" , 
	your_project = "project" )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
library(survey)

dhs_df <- 
	readRDS( 
		file.path( path.expand( "~" ) , "DHS" , 
		"Malawi/Standard DHS 2004/MWIR4EDT.rds" ) 
	)

# convert the weight column to a numeric type
dhs_df$weight <- as.numeric( dhs_df$v005 )

# paste the `sdist` and `v025` columns together
# into a single strata variable
dhs_df$strata <- do.call( paste , dhs_df[ , c( 'sdist' , 'v025' ) ] )
# as shown at
# http://userforum.dhsprogram.com/index.php?t=rview&goto=2154#msg_2154

dhs_design <- 
	svydesign( 
		~ v021 , 
		strata = ~strata , 
		data = dhs_df , 
		weights = ~weight
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
dhs_design <- 
	update( 
		dhs_design , 
		
		one = 1 ,
		
		total_children_ever_born = v201 ,
		
		surviving_children = v201 - v206 - v207 ,
		
		urban_rural = factor( v025 , labels = c( 'urban' , 'rural' ) ) ,
		
		ethnicity =
			factor( v131 , levels = c( 1:8 , 96 ) , labels =
				c( "Chewa" , "Tumbuka" , "Lomwe" , "Tonga" , 
				"Yao" , "Sena" , "Nkonde" , "Ngoni" , "Other" ) ) ,
				
		no_formal_education = as.numeric( v149 == 0 )
		
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( dhs_design , "sampling" ) != 0 )

svyby( ~ one , ~ urban_rural , dhs_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , dhs_design )

svyby( ~ one , ~ urban_rural , dhs_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ surviving_children , dhs_design )

svyby( ~ surviving_children , ~ urban_rural , dhs_design , svymean )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ ethnicity , dhs_design , na.rm = TRUE )

svyby( ~ ethnicity , ~ urban_rural , dhs_design , svymean , na.rm = TRUE )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ surviving_children , dhs_design )

svyby( ~ surviving_children , ~ urban_rural , dhs_design , svytotal )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ ethnicity , dhs_design , na.rm = TRUE )

svyby( ~ ethnicity , ~ urban_rural , dhs_design , svytotal , na.rm = TRUE )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ surviving_children , dhs_design , 0.5 )

svyby( 
	~ surviving_children , 
	~ urban_rural , 
	dhs_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE 
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ surviving_children , 
	denominator = ~ total_children_ever_born , 
	dhs_design 
)
```

### Subsetting {-}

Restrict the survey design to 40-49 year old females only:
```{r eval = FALSE , results = "hide" }
sub_dhs_design <- subset( dhs_design , v447a %in% 40:49 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ surviving_children , sub_dhs_design )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ surviving_children , dhs_design )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ surviving_children , 
		~ urban_rural , 
		dhs_design , 
		svymean 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( dhs_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ surviving_children , dhs_design )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ surviving_children , dhs_design , deff = TRUE )

# SRS with replacement
svymean( ~ surviving_children , dhs_design , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ no_formal_education , dhs_design ,
	method = "likelihood" )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( surviving_children ~ no_formal_education , dhs_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ no_formal_education + ethnicity , 
	dhs_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		surviving_children ~ no_formal_education + ethnicity , 
		dhs_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for DHS users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
dhs_srvyr_design <- as_survey( dhs_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dhs_srvyr_design %>%
	summarize( mean = survey_mean( surviving_children ) )

dhs_srvyr_design %>%
	group_by( urban_rural ) %>%
	summarize( mean = survey_mean( surviving_children ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:21-dhs.Rmd-->

# Exame Nacional de Desempenho de Estudantes (ENADE) {-}

[![Build Status](https://travis-ci.org/asdfree/enade.svg?branch=master)](https://travis-ci.org/asdfree/enade) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/enade?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/enade)

The Exame Nacional de Desempenho de Estudantes (ENADE) evaluates the performance of undergraduate students in relation to the program content, skills and competences acquired in their training. The exam is mandatory and the student's regularity in the exam must be included in his or her school record.

* One table with one row per individual undergraduate student in Brazil.

* An enumeration of undergraduate students in Brazil.

* Released annually since 2004.

* Compiled by the Brazilian [Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira (INEP)](http://www.inep.gov.br/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available ENADE microdata by simply specifying `"enade"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "enade" , output_dir = file.path( path.expand( "~" ) , "ENADE" ) )
```

## Analysis Examples with base R {-}

Load a data frame:

```{r eval = FALSE }
enade_df <- readRDS( file.path( path.expand( "~" ) , "ENADE" , "2015 main.rds" ) )
```

```{r eval = FALSE }

```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
enade_df <- 
	transform( 
		enade_df , 
		
		# qual foi o tempo gasto por voce para concluir a prova?
		less_than_two_hours = as.numeric( qp_i9 %in% c( 'A' , 'B' ) ) ,
		

		state_name = 
			factor( 
				co_uf_curso , 
				levels = c( 11:17 , 21:29 , 31:33 , 35 , 41:43 , 50:53 ) ,
				labels = c( "Rondonia" , "Acre" , "Amazonas" , 
				"Roraima" , "Para" , "Amapa" , "Tocantins" , 
				"Maranhao" , "Piaui" , "Ceara" , "Rio Grande do Norte" , 
				"Paraiba" , "Pernambuco" , "Alagoas" , "Sergipe" , 
				"Bahia" , "Minas Gerais" , "Espirito Santo" , 
				"Rio de Janeiro" , "Sao Paulo" , "Parana" , 
				"Santa Catarina" , "Rio Grande do Sul" , 
				"Mato Grosso do Sul" , "Mato Grosso" , "Goias" , 
				"Distrito Federal" )
			)

	)
	
```

### Unweighted Counts {-}

Count the unweighted number of records in the table, overall and by groups:
```{r eval = FALSE , results = "hide" }
nrow( enade_df )

table( enade_df[ , "tp_sexo" ] , useNA = "always" )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
mean( enade_df[ , "nt_obj_fg" ] , na.rm = TRUE )

tapply(
	enade_df[ , "nt_obj_fg" ] ,
	enade_df[ , "tp_sexo" ] ,
	mean ,
	na.rm = TRUE 
)
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
prop.table( table( enade_df[ , "state_name" ] ) )

prop.table(
	table( enade_df[ , c( "state_name" , "tp_sexo" ) ] ) ,
	margin = 2
)
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( enade_df[ , "nt_obj_fg" ] , na.rm = TRUE )

tapply(
	enade_df[ , "nt_obj_fg" ] ,
	enade_df[ , "tp_sexo" ] ,
	sum ,
	na.rm = TRUE 
)
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
quantile( enade_df[ , "nt_obj_fg" ] , 0.5 , na.rm = TRUE )

tapply(
	enade_df[ , "nt_obj_fg" ] ,
	enade_df[ , "tp_sexo" ] ,
	quantile ,
	0.5 ,
	na.rm = TRUE 
)
```

### Subsetting {-}

Limit your `data.frame` to Students reporting that the general training section of the test was easy or very easy:
```{r eval = FALSE , results = "hide" }
sub_enade_df <- subset( enade_df , qp_i1 %in% c( "A" , "B" ) )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
mean( sub_enade_df[ , "nt_obj_fg" ] , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Calculate the variance, overall and by groups:
```{r eval = FALSE , results = "hide" }
var( enade_df[ , "nt_obj_fg" ] , na.rm = TRUE )

tapply(
	enade_df[ , "nt_obj_fg" ] ,
	enade_df[ , "tp_sexo" ] ,
	var ,
	na.rm = TRUE 
)
```

### Regression Models and Tests of Association {-}

Perform a t-test:
```{r eval = FALSE , results = "hide" }
t.test( nt_obj_fg ~ less_than_two_hours , enade_df )
```

Perform a chi-squared test of association:
```{r eval = FALSE , results = "hide" }
this_table <- table( enade_df[ , c( "less_than_two_hours" , "state_name" ) ] )

chisq.test( this_table )
```

Perform a generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	glm( 
		nt_obj_fg ~ less_than_two_hours + state_name , 
		data = enade_df
	)

summary( glm_result )
```

## Analysis Examples with `dplyr` {-}

The R `dplyr` library offers an alternative grammar of data manipulation to base R and SQL syntax. [dplyr](https://github.com/tidyverse/dplyr/) offers many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, and the `tidyverse` style of non-standard evaluation. [This vignette](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html) details the available features. As a starting point for ENADE users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(dplyr)
enade_tbl <- tbl_df( enade_df )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
enade_tbl %>%
	summarize( mean = mean( nt_obj_fg , na.rm = TRUE ) )

enade_tbl %>%
	group_by( tp_sexo ) %>%
	summarize( mean = mean( nt_obj_fg , na.rm = TRUE ) )
```



<!--chapter:end:22-enade.Rmd-->

# Exame Nacional do Ensino Medio (ENEM) {-}

*Contributed by Dr. Djalma Pessoa <<pessoad@gmail.com>>*

The Exame Nacional do Ensino Medio (ENEM) contains the standardized test results of most Brazilian high school students.

* An annual table with one row per student.

* Updated annually since 1998.

* Maintained by the Brazil's [Instituto Nacional de Estudos e Pesquisas Educacionais Anisio Teixeira (INEP)](http://www.inep.gov.br/)

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available ENEM microdata by simply specifying `"enem"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "enem" , output_dir = file.path( path.expand( "~" ) , "ENEM" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the ENEM catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available ENEM microdata files
enem_cat <-
	get_catalog( "enem" ,
		output_dir = file.path( path.expand( "~" ) , "ENEM" ) )

# 2015 only
enem_cat <- subset( enem_cat , year == 2015 )
# download the microdata to your local computer
lodown( "enem" , enem_cat )
```

## Analysis Examples with SQL and `MonetDBLite` {-}

Connect to a database:

```{r eval = FALSE }
library(DBI)
dbdir <- file.path( path.expand( "~" ) , "ENEM" , "MonetDB" )
db <- dbConnect( MonetDBLite::MonetDBLite() , dbdir )
```

```{r eval = FALSE }

```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
dbSendQuery( db , "ALTER TABLE microdados_enem_2015 ADD COLUMN female INTEGER" )

dbSendQuery( db , 
	"UPDATE microdados_enem_2015 
	SET female = 
		CASE WHEN tp_sexo = 2 THEN 1 ELSE 0 END" 
)

dbSendQuery( db , "ALTER TABLE microdados_enem_2015 ADD COLUMN fathers_education INTEGER" )

dbSendQuery( db , 
	"UPDATE microdados_enem_2015 
	SET fathers_education = 
		CASE WHEN q001 = 1 THEN '01 - nao estudou'
			WHEN q001 = 2 THEN '02 - 1 a 4 serie'
			WHEN q001 = 3 THEN '03 - 5 a 8 serie'
			WHEN q001 = 4 THEN '04 - ensino medio incompleto'
			WHEN q001 = 5 THEN '05 - ensino medio'
			WHEN q001 = 6 THEN '06 - ensino superior incompleto'
			WHEN q001 = 7 THEN '07 - ensino superior'
			WHEN q001 = 8 THEN '08 - pos-graduacao'
			WHEN q001 = 9 THEN '09 - nao estudou' ELSE NULL END" 
)
```

### Unweighted Counts {-}

Count the unweighted number of records in the SQL table, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT COUNT(*) FROM microdados_enem_2015" )

dbGetQuery( db ,
	"SELECT
		fathers_education ,
		COUNT(*) 
	FROM microdados_enem_2015
	GROUP BY fathers_education"
)
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT AVG( nota_mt ) FROM microdados_enem_2015" )

dbGetQuery( db , 
	"SELECT 
		fathers_education , 
		AVG( nota_mt ) AS mean_nota_mt
	FROM microdados_enem_2015 
	GROUP BY fathers_education" 
)
```

Initiate a function that allows division by zero:
```{r eval = FALSE , results = "hide" }
dbSendQuery( db , 
	"CREATE FUNCTION 
		div_noerror(l DOUBLE, r DOUBLE) 
	RETURNS DOUBLE 
	EXTERNAL NAME calc.div_noerror" 
)
```

Calculate the distribution of a categorical variable:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		uf_residencia , 
		div_noerror( 
			COUNT(*) , 
			( SELECT COUNT(*) FROM microdados_enem_2015 ) 
		) AS share_uf_residencia
	FROM microdados_enem_2015 
	GROUP BY uf_residencia" 
)
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT SUM( nota_mt ) FROM microdados_enem_2015" )

dbGetQuery( db , 
	"SELECT 
		fathers_education , 
		SUM( nota_mt ) AS sum_nota_mt 
	FROM microdados_enem_2015 
	GROUP BY fathers_education" 
)
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT QUANTILE( nota_mt , 0.5 ) FROM microdados_enem_2015" )

dbGetQuery( db , 
	"SELECT 
		fathers_education , 
		QUANTILE( nota_mt , 0.5 ) AS median_nota_mt
	FROM microdados_enem_2015 
	GROUP BY fathers_education" 
)
```

### Subsetting {-}

Limit your SQL analysis to took mathematics exam with `WHERE`:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db ,
	"SELECT
		AVG( nota_mt )
	FROM microdados_enem_2015
	WHERE in_presenca_mt = 1"
)
```

### Measures of Uncertainty {-}

Calculate the variance and standard deviation, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		VAR_SAMP( nota_mt ) , 
		STDDEV_SAMP( nota_mt ) 
	FROM microdados_enem_2015" 
)

dbGetQuery( db , 
	"SELECT 
		fathers_education , 
		VAR_SAMP( nota_mt ) AS var_nota_mt ,
		STDDEV_SAMP( nota_mt ) AS stddev_nota_mt
	FROM microdados_enem_2015 
	GROUP BY fathers_education" 
)
```

### Regression Models and Tests of Association {-}

Calculate the correlation between two variables, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		CORR( CAST( female AS DOUBLE ) , CAST( nota_mt AS DOUBLE ) )
	FROM microdados_enem_2015" 
)

dbGetQuery( db , 
	"SELECT 
		fathers_education , 
		CORR( CAST( female AS DOUBLE ) , CAST( nota_mt AS DOUBLE ) )
	FROM microdados_enem_2015 
	GROUP BY fathers_education" 
)
```

## Analysis Examples with `dplyr` {-}

The R `dplyr` library offers an alternative grammar of data manipulation to base R and SQL syntax. [dplyr](https://github.com/tidyverse/dplyr/) offers many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, and the `tidyverse` style of non-standard evaluation. [This vignette](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html) details the available features. As a starting point for ENEM users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(dplyr)
dplyr_db <- MonetDBLite::src_monetdblite( dbdir )
enem_tbl <- tbl( dplyr_db , 'microdados_enem_2015' )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
enem_tbl %>%
	summarize( mean = mean( nota_mt ) )

enem_tbl %>%
	group_by( fathers_education ) %>%
	summarize( mean = mean( nota_mt ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT COUNT(*) FROM microdados_enem_2015" )
```

## Database Shutdown {-}

```{r eval = FALSE , results = "hide" }
dbDisconnect( db , shutdown = TRUE )
```

<!--chapter:end:23-enem.Rmd-->

# European Social Survey (ESS) {-}

[![Build Status](https://travis-ci.org/asdfree/ess.svg?branch=master)](https://travis-ci.org/asdfree/ess) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/ess?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/ess)

*Contributed by Dr. Daniel Oberski <<daniel.oberski@gmail.com>>*

The European Social Survey measures political opinion and behavior across the continent.

* One table per country with one row per sampled respondent.

* A complex sample survey designed to generalize to residents aged 15 and older in participating nations.

* Released biennially since 2002.

* Headquartered at [City, University of London](http://www.city.ac.uk/) and governed by [a scientific team across Europe](http://www.europeansocialsurvey.org/about/structure_and_governance.html).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available ESS microdata by simply specifying `"ess"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "ess" , output_dir = file.path( path.expand( "~" ) , "ESS" ) , 
	your_email = "email@address.com" )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the ESS catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available ESS microdata files
ess_cat <-
	get_catalog( "ess" ,
		output_dir = file.path( path.expand( "~" ) , "ESS" ) , 
		your_email = "email@address.com" )

# 2014 only
ess_cat <- subset( ess_cat , year == 2014 )
# download the microdata to your local computer
lodown( "ess" , ess_cat , 
	your_email = "email@address.com" )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
library(survey)

ess_be_df <- 
	readRDS( file.path( path.expand( "~" ) , "ESS" , "2014/ESS7BE.rds" ) )

ess_sddf_df <- 
	readRDS( file.path( path.expand( "~" ) , "ESS" , "2014/ESS7SDDFe01_1.rds" ) )

ess_df <-
	merge( 
		ess_be_df , 
		ess_sddf_df , 
		by = c( 'cntry' , 'idno' ) 
	)

stopifnot( nrow( ess_df ) == nrow( ess_be_df ) )

ess_design <- 
	svydesign(
		ids = ~psu ,
		strata = ~stratify ,
		probs = ~prob ,
		data = ess_df
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
ess_design <- 
	update( 
		ess_design , 
		
		one = 1 ,
		
		non_european_immigrants =
			factor( impcntr ,
				labels = c( 'Allow many to come and live here' , 
				'Allow some' , 'Allow a few' , 'Allow none' )
			) ,
		
		sex = factor( icgndra , labels = c( 'male' , 'female' ) ) ,
			
		more_than_one_hour_tv_daily = as.numeric( tvtot >= 3 )
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( ess_design , "sampling" ) != 0 )

svyby( ~ one , ~ non_european_immigrants , ess_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , ess_design )

svyby( ~ one , ~ non_european_immigrants , ess_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ ppltrst , ess_design )

svyby( ~ ppltrst , ~ non_european_immigrants , ess_design , svymean )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ sex , ess_design , na.rm = TRUE )

svyby( ~ sex , ~ non_european_immigrants , ess_design , svymean , na.rm = TRUE )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ ppltrst , ess_design )

svyby( ~ ppltrst , ~ non_european_immigrants , ess_design , svytotal )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ sex , ess_design , na.rm = TRUE )

svyby( ~ sex , ~ non_european_immigrants , ess_design , svytotal , na.rm = TRUE )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ ppltrst , ess_design , 0.5 )

svyby( 
	~ ppltrst , 
	~ non_european_immigrants , 
	ess_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE 
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ ppltrst , 
	denominator = ~ pplfair , 
	ess_design 
)
```

### Subsetting {-}

Restrict the survey design to voters:
```{r eval = FALSE , results = "hide" }
sub_ess_design <- subset( ess_design , vote == 1 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ ppltrst , sub_ess_design )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ ppltrst , ess_design )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ ppltrst , 
		~ non_european_immigrants , 
		ess_design , 
		svymean 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( ess_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ ppltrst , ess_design )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ ppltrst , ess_design , deff = TRUE )

# SRS with replacement
svymean( ~ ppltrst , ess_design , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ more_than_one_hour_tv_daily , ess_design ,
	method = "likelihood" , na.rm = TRUE )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( ppltrst ~ more_than_one_hour_tv_daily , ess_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ more_than_one_hour_tv_daily + sex , 
	ess_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		ppltrst ~ more_than_one_hour_tv_daily + sex , 
		ess_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for ESS users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
ess_srvyr_design <- as_survey( ess_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
ess_srvyr_design %>%
	summarize( mean = survey_mean( ppltrst ) )

ess_srvyr_design %>%
	group_by( non_european_immigrants ) %>%
	summarize( mean = survey_mean( ppltrst ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:24-ess.Rmd-->

# FDA Adverse Event Reporting System (FAERS) {-}

[![Build Status](https://travis-ci.org/asdfree/faers.svg?branch=master)](https://travis-ci.org/asdfree/faers) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/faers?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/faers)

The FDA Adverse Event Reporting System (FAERS) compiles all prescription drug-related side-effects reported by either physicians or patients in the United States. Either party can make a (voluntary) submission to the FDA or the manufacturer (who then must report that event). This is the post-marketing safety surveillance program for drug and therapeutic biological products.

* Multiple tables linkable by the `primaryid` field with patient demographics, drug/biologic information, patient outcomes, reporting source, drug start and end dates.

* Published quarterly with the latest events reported to the FDA since 2004, with a revised system beginning in the fourth quarter of 2012.

* Maintained by the United States [Food and Drug Administration (FDA)](http://www.fda.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available FAERS microdata by simply specifying `"faers"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "faers" , output_dir = file.path( path.expand( "~" ) , "FAERS" ) )
```

## Analysis Examples with base R {-}

Load a data frame:

```{r eval = FALSE }
faers_drug_df <- 
	readRDS( file.path( path.expand( "~" ) , "FAERS" , "2016 q4/drug16q4.rds" ) )

faers_outcome_df <- 
	readRDS( file.path( path.expand( "~" ) , "FAERS" , "2016 q4/outc16q4.rds" ) )

faers_demo_df <- 
	readRDS( file.path( path.expand( "~" ) , "FAERS" , "2016 q4/demo16q4.rds" ) )

faers_df <- merge( faers_drug_df , faers_outcome_df )

faers_df <- merge( faers_df , faers_demo_df , all.x = TRUE )
```

```{r eval = FALSE }

```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
faers_df <- 
	transform( 
		faers_df , 
		
		physician_reported = as.numeric( occp_cod == "MD" ) ,
		
		init_fda_year = as.numeric( substr( init_fda_dt , 1 , 4 ) )
		
	)
	
```

### Unweighted Counts {-}

Count the unweighted number of records in the table, overall and by groups:
```{r eval = FALSE , results = "hide" }
nrow( faers_df )

table( faers_df[ , "outc_code" ] , useNA = "always" )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
mean( faers_df[ , "init_fda_year" ] , na.rm = TRUE )

tapply(
	faers_df[ , "init_fda_year" ] ,
	faers_df[ , "outc_code" ] ,
	mean ,
	na.rm = TRUE 
)
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
prop.table( table( faers_df[ , "sex" ] ) )

prop.table(
	table( faers_df[ , c( "sex" , "outc_code" ) ] ) ,
	margin = 2
)
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( faers_df[ , "init_fda_year" ] , na.rm = TRUE )

tapply(
	faers_df[ , "init_fda_year" ] ,
	faers_df[ , "outc_code" ] ,
	sum ,
	na.rm = TRUE 
)
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
quantile( faers_df[ , "init_fda_year" ] , 0.5 , na.rm = TRUE )

tapply(
	faers_df[ , "init_fda_year" ] ,
	faers_df[ , "outc_code" ] ,
	quantile ,
	0.5 ,
	na.rm = TRUE 
)
```

### Subsetting {-}

Limit your `data.frame` to elderly persons:
```{r eval = FALSE , results = "hide" }
sub_faers_df <- subset( faers_df , age_grp == "E" )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
mean( sub_faers_df[ , "init_fda_year" ] , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Calculate the variance, overall and by groups:
```{r eval = FALSE , results = "hide" }
var( faers_df[ , "init_fda_year" ] , na.rm = TRUE )

tapply(
	faers_df[ , "init_fda_year" ] ,
	faers_df[ , "outc_code" ] ,
	var ,
	na.rm = TRUE 
)
```

### Regression Models and Tests of Association {-}

Perform a t-test:
```{r eval = FALSE , results = "hide" }
t.test( init_fda_year ~ physician_reported , faers_df )
```

Perform a chi-squared test of association:
```{r eval = FALSE , results = "hide" }
this_table <- table( faers_df[ , c( "physician_reported" , "sex" ) ] )

chisq.test( this_table )
```

Perform a generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	glm( 
		init_fda_year ~ physician_reported + sex , 
		data = faers_df
	)

summary( glm_result )
```

## Analysis Examples with `dplyr` {-}

The R `dplyr` library offers an alternative grammar of data manipulation to base R and SQL syntax. [dplyr](https://github.com/tidyverse/dplyr/) offers many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, and the `tidyverse` style of non-standard evaluation. [This vignette](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html) details the available features. As a starting point for FAERS users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(dplyr)
faers_tbl <- tbl_df( faers_df )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
faers_tbl %>%
	summarize( mean = mean( init_fda_year , na.rm = TRUE ) )

faers_tbl %>%
	group_by( outc_code ) %>%
	summarize( mean = mean( init_fda_year , na.rm = TRUE ) )
```



<!--chapter:end:25-faers.Rmd-->

# General Social Survey (GSS) {-}

[![Build Status](https://travis-ci.org/asdfree/gss.svg?branch=master)](https://travis-ci.org/asdfree/gss) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/gss?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/gss)

The General Social Survey (GSS) has captured political beliefs and social attitudes since 1972. In contrast to non-trendable tracking polls that capture newspaper headlines, the GSS has sustained a set of questions over four decades.

* One table with one row per sampled respondent.

* A complex sample survey designed to generalize to the non-institutional population of adults (18+) in the United States.

* Updated biennially since 1972.

* Funded by the [National Science Foundation](http://www.nsf.gov/) and administered by the [National Opinion Research Center](http://www.norc.org/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available GSS microdata by simply specifying `"gss"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "gss" , output_dir = file.path( path.expand( "~" ) , "GSS" ) )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
library(survey)

gss_df <- 
	readRDS( file.path( path.expand( "~" ) , "GSS" , 
		"gss 1972 2016 cross sectional cumulative data release 2 september 29 2017.rds" ) )

gss_df <- 
	transform( 
		gss_df , 
		
		# the calculation for compwt comes from
		# http://sda.berkeley.edu/D3/GSS10/Doc/gs100195.htm#COMPWT
		compwt = oversamp * formwt * wtssall , 
		
		# the calculation for samplerc comes from
		# http://sda.berkeley.edu/D3/GSS10/Doc/gs100195.htm#SAMPLERC
		samplerc = 
			# if sample is a three or a four, samplerc should be a three
			ifelse( sample %in% 3:4 , 3 , 
			# if sample is a six or a seven, samplerc should be a six
			ifelse( sample %in% 6:7 , 6 , 
			# otherwise, samplerc should just be set to sample
				sample ) )

	)

# keep only the variables you need
keep_vars <- 
	c( "sampcode" , "samplerc" , "compwt" , "polviews" , 
		"born" , "adults" , "hompop" , "race" , "region" ,
		"age" , "sex" , "one" )
		
gss_df <- gss_df[ keep_vars ] ; gc()
# this step conserves RAM

gss_design <- 
	svydesign( 
		~sampcode , 
		strata = ~samplerc , 
		data = subset( gss_df , !is.na( sampcode ) ) , 
		weights = ~compwt , 
		nest = TRUE 
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
gss_design <- 
	update( 
		gss_design , 

		polviews = 
			factor( polviews ,
				labels = c( "Extremely liberal" , "Liberal" ,
				"Slightly liberal" , "Moderate, middle of the road" ,
				"Slightly conservative" , "Conservative" ,
				"Extremely conservative" )
			) ,
		
		born_in_usa = ifelse( born %in% 1:2 , as.numeric( born == 1 ) , NA ) ,
		
		adults_in_hh = ifelse( adults > 8 , NA , adults ) ,
		
		persons_in_hh = ifelse( hompop > 11 , NA , hompop ) ,
		
		race = factor( race , labels = c( "white" , "black" , "other" ) ) ,
		
		region = 
			factor( region , 
				labels = c( "New England" , "Middle Atlantic" ,
					"East North Central" , "West North Central" ,
					"South Atlantic" , "East South Central" ,
					"West South Central" , "Mountain" , "Pacific" )
			)

	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( gss_design , "sampling" ) != 0 )

svyby( ~ one , ~ region , gss_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , gss_design )

svyby( ~ one , ~ region , gss_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ age , gss_design , na.rm = TRUE )

svyby( ~ age , ~ region , gss_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ race , gss_design , na.rm = TRUE )

svyby( ~ race , ~ region , gss_design , svymean , na.rm = TRUE )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ age , gss_design , na.rm = TRUE )

svyby( ~ age , ~ region , gss_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ race , gss_design , na.rm = TRUE )

svyby( ~ race , ~ region , gss_design , svytotal , na.rm = TRUE )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ age , gss_design , 0.5 , na.rm = TRUE )

svyby( 
	~ age , 
	~ region , 
	gss_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ adults_in_hh , 
	denominator = ~ persons_in_hh , 
	gss_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to females:
```{r eval = FALSE , results = "hide" }
sub_gss_design <- subset( gss_design , sex == 2 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ age , sub_gss_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ age , gss_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ age , 
		~ region , 
		gss_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( gss_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ age , gss_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ age , gss_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ age , gss_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ born_in_usa , gss_design ,
	method = "likelihood" , na.rm = TRUE )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( age ~ born_in_usa , gss_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ born_in_usa + race , 
	gss_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		age ~ born_in_usa + race , 
		gss_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for GSS users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
gss_srvyr_design <- as_survey( gss_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
gss_srvyr_design %>%
	summarize( mean = survey_mean( age , na.rm = TRUE ) )

gss_srvyr_design %>%
	group_by( region ) %>%
	summarize( mean = survey_mean( age , na.rm = TRUE ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:26-gss.Rmd-->

# Home Mortgage Disclosure Act (HMDA) {-}

*Contributed by Max Weselcouch <<mweselco@gmail.com>>*

Responding to discriminatory lending practices, the United States Congress mandated that financial organizations originating home mortgages report some basic operational statistics. The Home Mortgage Disclosure Act (HMDA) increased the transparency of home-lending activity across the country.

* A loan application record (LAR) table with one record per public loan application, with secondary tables containing both private loan applications (PMIC) and one record per institution tables (INS).

* A public compilation of [more than ninety percent](http://www.huduser.gov/portal/periodicals/ushmc/spring11/USHMC_1q11.pdf#page=6) of all Federal Housing Authority (FHA) loans in the United States.

* Updated every September with a new year of microdata. Data prior to 2006 require a special order from the [United States National Archives](https://www.ffiec.gov/hmda/nationalarchives.htm).

* Maintained by the United States [Federal Financial Institutions Examination Council (FEIEC)](https://www.ffiec.gov/)

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available HMDA microdata by simply specifying `"hmda"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "hmda" , output_dir = file.path( path.expand( "~" ) , "HMDA" ) )
```

## Analysis Examples with SQL and `MonetDBLite` {-}

Connect to a database:

```{r eval = FALSE }
library(DBI)
dbdir <- file.path( path.expand( "~" ) , "HMDA" , "MonetDB" )
db <- dbConnect( MonetDBLite::MonetDBLite() , dbdir )
```

```{r eval = FALSE }

```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
dbSendQuery( db , "ALTER TABLE hmda_2015 ADD COLUMN multifamily_home INTEGER" )

dbSendQuery( db , 
	"UPDATE hmda_2015 
	SET multifamily_home = 
		CASE WHEN ( propertytype = 3 ) THEN 1 ELSE 0 END" 
)
```

### Unweighted Counts {-}

Count the unweighted number of records in the SQL table, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT COUNT(*) FROM hmda_2015" )

dbGetQuery( db ,
	"SELECT
		loanpurpose ,
		COUNT(*) 
	FROM hmda_2015
	GROUP BY loanpurpose"
)
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT AVG( loanamount ) FROM hmda_2015" )

dbGetQuery( db , 
	"SELECT 
		loanpurpose , 
		AVG( loanamount ) AS mean_loanamount
	FROM hmda_2015 
	GROUP BY loanpurpose" 
)
```

Initiate a function that allows division by zero:
```{r eval = FALSE , results = "hide" }
dbSendQuery( db , 
	"CREATE FUNCTION 
		div_noerror(l DOUBLE, r DOUBLE) 
	RETURNS DOUBLE 
	EXTERNAL NAME calc.div_noerror" 
)
```

Calculate the distribution of a categorical variable:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		actiontype , 
		div_noerror( 
			COUNT(*) , 
			( SELECT COUNT(*) FROM hmda_2015 ) 
		) AS share_actiontype
	FROM hmda_2015 
	GROUP BY actiontype" 
)
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT SUM( loanamount ) FROM hmda_2015" )

dbGetQuery( db , 
	"SELECT 
		loanpurpose , 
		SUM( loanamount ) AS sum_loanamount 
	FROM hmda_2015 
	GROUP BY loanpurpose" 
)
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT QUANTILE( loanamount , 0.5 ) FROM hmda_2015" )

dbGetQuery( db , 
	"SELECT 
		loanpurpose , 
		QUANTILE( loanamount , 0.5 ) AS median_loanamount
	FROM hmda_2015 
	GROUP BY loanpurpose" 
)
```

### Subsetting {-}

Limit your SQL analysis to non-Hispanic White persons with `WHERE`:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db ,
	"SELECT
		AVG( loanamount )
	FROM hmda_2015
	WHERE race = 5 AND ethnicity = 2"
)
```

### Measures of Uncertainty {-}

Calculate the variance and standard deviation, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		VAR_SAMP( loanamount ) , 
		STDDEV_SAMP( loanamount ) 
	FROM hmda_2015" 
)

dbGetQuery( db , 
	"SELECT 
		loanpurpose , 
		VAR_SAMP( loanamount ) AS var_loanamount ,
		STDDEV_SAMP( loanamount ) AS stddev_loanamount
	FROM hmda_2015 
	GROUP BY loanpurpose" 
)
```

### Regression Models and Tests of Association {-}

Calculate the correlation between two variables, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		CORR( CAST( multifamily_home AS DOUBLE ) , CAST( loanamount AS DOUBLE ) )
	FROM hmda_2015" 
)

dbGetQuery( db , 
	"SELECT 
		loanpurpose , 
		CORR( CAST( multifamily_home AS DOUBLE ) , CAST( loanamount AS DOUBLE ) )
	FROM hmda_2015 
	GROUP BY loanpurpose" 
)
```

## Analysis Examples with `dplyr` {-}

The R `dplyr` library offers an alternative grammar of data manipulation to base R and SQL syntax. [dplyr](https://github.com/tidyverse/dplyr/) offers many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, and the `tidyverse` style of non-standard evaluation. [This vignette](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html) details the available features. As a starting point for HMDA users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(dplyr)
dplyr_db <- MonetDBLite::src_monetdblite( dbdir )
hmda_tbl <- tbl( dplyr_db , 'hmda_2015' )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
hmda_tbl %>%
	summarize( mean = mean( loanamount ) )

hmda_tbl %>%
	group_by( loanpurpose ) %>%
	summarize( mean = mean( loanamount ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT COUNT(*) FROM hmda_2015" )
```

## Database Shutdown {-}

```{r eval = FALSE , results = "hide" }
dbDisconnect( db , shutdown = TRUE )
```

<!--chapter:end:27-hmda.Rmd-->

# Health and Retirement Study (HRS) {-}

[![Build Status](https://travis-ci.org/asdfree/hrs.svg?branch=master)](https://travis-ci.org/asdfree/hrs) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/hrs?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/hrs)

The Health and Retirement Study interviews Americans aged 50+ for their entire life. Allows for findings like, "Among Americans who were 50-74 years old in 1998, X% lived in nursing homes by 2010."

* Many tables, most with one row per sampled respondent and linkable over time. Use the RAND HRS data file for a cleaner, cross-wave data set.

* A complex sample survey designed to generalize to Americans aged 50+ at each interview, but longitudinal analysts can observe outcomes.

* Released biennially since 1992.

* Administered by the [University of Michigan's Institute for Social Research](http://isr.umich.edu/) with data management by the [RAND Corporation](http://www.rand.org/). Funded by the [National Institute on Aging](https://www.nia.nih.gov/) and the [Social Security Administration](https://www.ssa.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available HRS microdata by simply specifying `"hrs"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "hrs" , output_dir = file.path( path.expand( "~" ) , "HRS" ) , 
	your_username = "username" , 
	your_password = "password" )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the HRS catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available HRS microdata files
hrs_cat <-
	get_catalog( "hrs" ,
		output_dir = file.path( path.expand( "~" ) , "HRS" ) , 
		your_username = "username" , 
		your_password = "password" )

# RAND consolidated file only
hrs_cat <- subset( hrs_cat , grepl( 'rand([a-z]+)stata\\.zip' , file_name ) )
# download the microdata to your local computer
lodown( "hrs" , hrs_cat , 
	your_username = "username" , 
	your_password = "password" )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
library(survey)

hrs_df <- 
	readRDS( list.files( hrs_cat$output_folder , full.names = TRUE ) )
	
# RAM cleanup
keep_vars <- 
	c( "raehsamp" , "raestrat" , "r3wtresp" , 
		"r3work" , "r12work" , "h12ahous" ,
		"r3mstat" , "r12mstat" , "h4ahous" )

hrs_df <- hrs_df[ keep_vars ]
	
# community residents aged 50+ in 1996
hrs_design <- 
	svydesign(
		id = ~ raehsamp ,
		strata = ~ raestrat ,
		weights = ~ r3wtresp , 
		nest = TRUE ,
		data = subset( hrs_df , r3wtresp > 0 )
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
hrs_design <- 
	update( 
		hrs_design , 

		one = 1 ,
		
		working_in_1996 = r3work ,

		working_in_2014 = r12work ,

		marital_status_in_1996 =
			factor( r3mstat , levels = 1:8 , labels =
				c( "Married" , "Married, spouse absent" ,
				"Partnered" , "Separated" , "Divorced" ,
				"Separated/divorced" , "Widowed" ,
				"Never married" ) ) ,
				
		marital_status_in_2014 =
			factor( r12mstat , levels = 1:8 , labels =
				c( "Married" , "Married, spouse absent" ,
				"Partnered" , "Separated" , "Divorced" ,
				"Separated/divorced" , "Widowed" ,
				"Never married" ) )
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( hrs_design , "sampling" ) != 0 )

svyby( ~ one , ~ marital_status_in_1996 , hrs_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , hrs_design )

svyby( ~ one , ~ marital_status_in_1996 , hrs_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ h12ahous , hrs_design , na.rm = TRUE )

svyby( ~ h12ahous , ~ marital_status_in_1996 , hrs_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ marital_status_in_2014 , hrs_design , na.rm = TRUE )

svyby( ~ marital_status_in_2014 , ~ marital_status_in_1996 , hrs_design , svymean , na.rm = TRUE )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ h12ahous , hrs_design , na.rm = TRUE )

svyby( ~ h12ahous , ~ marital_status_in_1996 , hrs_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ marital_status_in_2014 , hrs_design , na.rm = TRUE )

svyby( ~ marital_status_in_2014 , ~ marital_status_in_1996 , hrs_design , svytotal , na.rm = TRUE )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ h12ahous , hrs_design , 0.5 , na.rm = TRUE )

svyby( 
	~ h12ahous , 
	~ marital_status_in_1996 , 
	hrs_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ h4ahous , 
	denominator = ~ h12ahous , 
	hrs_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to :
```{r eval = FALSE , results = "hide" }
sub_hrs_design <- subset( hrs_design , working_in_1996 == 1 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ h12ahous , sub_hrs_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ h12ahous , hrs_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ h12ahous , 
		~ marital_status_in_1996 , 
		hrs_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( hrs_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ h12ahous , hrs_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ h12ahous , hrs_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ h12ahous , hrs_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ working_in_2014 , hrs_design ,
	method = "likelihood" , na.rm = TRUE )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( h12ahous ~ working_in_2014 , hrs_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ working_in_2014 + marital_status_in_2014 , 
	hrs_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		h12ahous ~ working_in_2014 + marital_status_in_2014 , 
		hrs_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for HRS users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
hrs_srvyr_design <- as_survey( hrs_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
hrs_srvyr_design %>%
	summarize( mean = survey_mean( h12ahous , na.rm = TRUE ) )

hrs_srvyr_design %>%
	group_by( marital_status_in_1996 ) %>%
	summarize( mean = survey_mean( h12ahous , na.rm = TRUE ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:28-hrs.Rmd-->

# Medical Expenditure Panel Survey (MEPS) {-}

[![Build Status](https://travis-ci.org/asdfree/meps.svg?branch=master)](https://travis-ci.org/asdfree/meps) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/meps?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/meps)

The Medical Expenditure Panel Survey's Household Component (MEPS-HC) captures person-level medical expenditures by payor and type of service with more detail than any other publicly-available data set.

* The annual consolidated file contains one row per individual within each sampled household. Other available mergeable tables contain one record per medical event, one record per job, one record per insurance held.

* A complex sample survey designed to generalize to the civilian non-institutionalized population of the United States.

* Released annually since 1996.

* Administered by the [Agency for Healthcare Research and Quality](http://www.ahrq.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available MEPS microdata by simply specifying `"meps"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "meps" , output_dir = file.path( path.expand( "~" ) , "MEPS" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the MEPS catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available MEPS microdata files
meps_cat <-
	get_catalog( "meps" ,
		output_dir = file.path( path.expand( "~" ) , "MEPS" ) )

# 2015 only
meps_cat <- subset( meps_cat , year == 2015 )
# download the microdata to your local computer
lodown( "meps" , meps_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
options( survey.replicates.mse = TRUE )

library(survey)

meps_cons_df <- 
	readRDS( file.path( path.expand( "~" ) , "MEPS" , 
		"2015/full year consolidated.rds" ) )

meps_brr <- 
	readRDS( file.path( path.expand( "~" ) , "MEPS" , 
		"meps 1996-2015 replicates for variance estimation.rds" ) )

meps_brr <- 
	meps_brr[ , 
		c( "dupersid" , "panel" , 
			names( meps_brr )[ !( names( meps_brr ) %in% names( meps_cons_df ) ) ] 
		)
	]

meps_df <- merge( meps_cons_df , meps_brr )

stopifnot( nrow( meps_df ) == nrow( meps_cons_df ) )

meps_design <-
	svrepdesign(
		data = meps_df ,
		weights = ~ perwt15f ,
		type = "BRR" ,
		combined.weights = FALSE ,
		repweights = "brr[1-9]+"
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
meps_design <- 
	update( 
		meps_design , 
		
		one = 1 ,
		
		insured_december_31st = ifelse( ins15x %in% 1:2 , as.numeric( ins15x == 1 ) , NA )
		
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( meps_design , "sampling" ) != 0 )

svyby( ~ one , ~ region15 , meps_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , meps_design )

svyby( ~ one , ~ region15 , meps_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ totexp15 , meps_design )

svyby( ~ totexp15 , ~ region15 , meps_design , svymean )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ sex , meps_design )

svyby( ~ sex , ~ region15 , meps_design , svymean )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ totexp15 , meps_design )

svyby( ~ totexp15 , ~ region15 , meps_design , svytotal )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ sex , meps_design )

svyby( ~ sex , ~ region15 , meps_design , svytotal )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ totexp15 , meps_design , 0.5 )

svyby( 
	~ totexp15 , 
	~ region15 , 
	meps_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE 
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ totmcd15 , 
	denominator = ~ totexp15 , 
	meps_design 
)
```

### Subsetting {-}

Restrict the survey design to seniors:
```{r eval = FALSE , results = "hide" }
sub_meps_design <- subset( meps_design , agelast >= 65 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ totexp15 , sub_meps_design )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ totexp15 , meps_design )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ totexp15 , 
		~ region15 , 
		meps_design , 
		svymean 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( meps_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ totexp15 , meps_design )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ totexp15 , meps_design , deff = TRUE )

# SRS with replacement
svymean( ~ totexp15 , meps_design , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ insured_december_31st , meps_design ,
	method = "likelihood" , na.rm = TRUE )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( totexp15 ~ insured_december_31st , meps_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ insured_december_31st + sex , 
	meps_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		totexp15 ~ insured_december_31st + sex , 
		meps_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for MEPS users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
meps_srvyr_design <- as_survey( meps_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
meps_srvyr_design %>%
	summarize( mean = survey_mean( totexp15 ) )

meps_srvyr_design %>%
	group_by( region15 ) %>%
	summarize( mean = survey_mean( totexp15 ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:29-meps.Rmd-->

# Medical Large Claims Experience Study (MLCES) {-}

[![Build Status](https://travis-ci.org/asdfree/mlces.svg?branch=master)](https://travis-ci.org/asdfree/mlces) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/mlces?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/mlces)

The Medical Large Claims Experience Study (MLCES) might be the best private health insurance claims data available to the public. This data should be used to calibrate other data sets, and probably nothing more.

* One table with one row per individual with nonzero total paid charges.

* A convenience sample of group (employer-sponsored) health insurance claims from seven private health insurers in the United States.

* 1997 thru 1999 with no expected updates in the future.

* Provided by the [Society of Actuaries (SOA)](http://www.soa.org/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available MLCES microdata by simply specifying `"mlces"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "mlces" , output_dir = file.path( path.expand( "~" ) , "MLCES" ) )
```

## Analysis Examples with base R {-}

Load a data frame:

```{r eval = FALSE }
mlces_df <- readRDS( file.path( path.expand( "~" ) , "MLCES" , "mcles1997.rds" ) )
```

```{r eval = FALSE }

```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
mlces_df <- 
	transform( 
		mlces_df , 
		
		claimant_relationship_to_policyholder =
			ifelse( relation == "E" , "covered employee" ,
			ifelse( relation == "S" , "spouse of covered employee" ,
			ifelse( relation == "D" , "dependent of covered employee" , NA ) ) ) ,
			
		ppo_plan = as.numeric( ppo == 'Y' )
	)
	
```

### Unweighted Counts {-}

Count the unweighted number of records in the table, overall and by groups:
```{r eval = FALSE , results = "hide" }
nrow( mlces_df )

table( mlces_df[ , "claimant_relationship_to_policyholder" ] , useNA = "always" )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
mean( mlces_df[ , "totpdchg" ] )

tapply(
	mlces_df[ , "totpdchg" ] ,
	mlces_df[ , "claimant_relationship_to_policyholder" ] ,
	mean 
)
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
prop.table( table( mlces_df[ , "patsex" ] ) )

prop.table(
	table( mlces_df[ , c( "patsex" , "claimant_relationship_to_policyholder" ) ] ) ,
	margin = 2
)
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( mlces_df[ , "totpdchg" ] )

tapply(
	mlces_df[ , "totpdchg" ] ,
	mlces_df[ , "claimant_relationship_to_policyholder" ] ,
	sum 
)
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
quantile( mlces_df[ , "totpdchg" ] , 0.5 )

tapply(
	mlces_df[ , "totpdchg" ] ,
	mlces_df[ , "claimant_relationship_to_policyholder" ] ,
	quantile ,
	0.5 
)
```

### Subsetting {-}

Limit your `data.frame` to persons under 18:
```{r eval = FALSE , results = "hide" }
sub_mlces_df <- subset( mlces_df , ( ( claimyr - patbrtyr ) < 18 ) )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
mean( sub_mlces_df[ , "totpdchg" ] )
```

### Measures of Uncertainty {-}

Calculate the variance, overall and by groups:
```{r eval = FALSE , results = "hide" }
var( mlces_df[ , "totpdchg" ] )

tapply(
	mlces_df[ , "totpdchg" ] ,
	mlces_df[ , "claimant_relationship_to_policyholder" ] ,
	var 
)
```

### Regression Models and Tests of Association {-}

Perform a t-test:
```{r eval = FALSE , results = "hide" }
t.test( totpdchg ~ ppo_plan , mlces_df )
```

Perform a chi-squared test of association:
```{r eval = FALSE , results = "hide" }
this_table <- table( mlces_df[ , c( "ppo_plan" , "patsex" ) ] )

chisq.test( this_table )
```

Perform a generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	glm( 
		totpdchg ~ ppo_plan + patsex , 
		data = mlces_df
	)

summary( glm_result )
```

## Analysis Examples with `dplyr` {-}

The R `dplyr` library offers an alternative grammar of data manipulation to base R and SQL syntax. [dplyr](https://github.com/tidyverse/dplyr/) offers many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, and the `tidyverse` style of non-standard evaluation. [This vignette](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html) details the available features. As a starting point for MLCES users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(dplyr)
mlces_tbl <- tbl_df( mlces_df )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
mlces_tbl %>%
	summarize( mean = mean( totpdchg ) )

mlces_tbl %>%
	group_by( claimant_relationship_to_policyholder ) %>%
	summarize( mean = mean( totpdchg ) )
```



<!--chapter:end:30-mlces.Rmd-->

# National Plan and Provider Enumeration System (MTPS) {-}

The National Plan and Provider Enumeration System (NPPES) contains information about every medical provider, insurance plan, and clearinghouse actively operating in the United States healthcare industry.

* A single large table with one row per enumerated health care provider.

* A census of individuals and organizations who bill for medical services in the United States.

* Updated monthly with new providers.

* Maintained by the United States [Centers for Medicare & Medicaid Services (CMS)](http://www.cms.gov/)

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available MTPS microdata by simply specifying `"mtps"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "mtps" , output_dir = file.path( path.expand( "~" ) , "MTPS" ) )
```

## Analysis Examples with SQL and `MonetDBLite` {-}

Connect to a database:

```{r eval = FALSE }
library(DBI)
dbdir <- file.path( path.expand( "~" ) , "MTPS" , "MonetDB" )
db <- dbConnect( MonetDBLite::MonetDBLite() , dbdir )
```

```{r eval = FALSE }

```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
dbSendQuery( db , "ALTER TABLE npi ADD COLUMN individual INTEGER" )

dbSendQuery( db , 
	"UPDATE npi 
	SET individual = 
		CASE WHEN entity_type_code = 1 THEN 1 ELSE 0 END" 
)

dbSendQuery( db , "ALTER TABLE npi ADD COLUMN provider_enumeration_year INTEGER" )

dbSendQuery( db , 
	"UPDATE npi 
	SET provider_enumeration_year = 
		CAST( SUBSTRING( provider_enumeration_date , 7 , 10 ) AS INTEGER )" 
)
```

### Unweighted Counts {-}

Count the unweighted number of records in the SQL table, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT COUNT(*) FROM npi" )

dbGetQuery( db ,
	"SELECT
		provider_gender_code ,
		COUNT(*) 
	FROM npi
	GROUP BY provider_gender_code"
)
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT AVG( provider_enumeration_year ) FROM npi" )

dbGetQuery( db , 
	"SELECT 
		provider_gender_code , 
		AVG( provider_enumeration_year ) AS mean_provider_enumeration_year
	FROM npi 
	GROUP BY provider_gender_code" 
)
```

Initiate a function that allows division by zero:
```{r eval = FALSE , results = "hide" }
dbSendQuery( db , 
	"CREATE FUNCTION 
		div_noerror(l DOUBLE, r DOUBLE) 
	RETURNS DOUBLE 
	EXTERNAL NAME calc.div_noerror" 
)
```

Calculate the distribution of a categorical variable:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		is_sole_proprietor , 
		div_noerror( 
			COUNT(*) , 
			( SELECT COUNT(*) FROM npi ) 
		) AS share_is_sole_proprietor
	FROM npi 
	GROUP BY is_sole_proprietor" 
)
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT SUM( provider_enumeration_year ) FROM npi" )

dbGetQuery( db , 
	"SELECT 
		provider_gender_code , 
		SUM( provider_enumeration_year ) AS sum_provider_enumeration_year 
	FROM npi 
	GROUP BY provider_gender_code" 
)
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT QUANTILE( provider_enumeration_year , 0.5 ) FROM npi" )

dbGetQuery( db , 
	"SELECT 
		provider_gender_code , 
		QUANTILE( provider_enumeration_year , 0.5 ) AS median_provider_enumeration_year
	FROM npi 
	GROUP BY provider_gender_code" 
)
```

### Subsetting {-}

Limit your SQL analysis to California with `WHERE`:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db ,
	"SELECT
		AVG( provider_enumeration_year )
	FROM npi
	WHERE provider_business_practice_location_address_state_name = 'CA'"
)
```

### Measures of Uncertainty {-}

Calculate the variance and standard deviation, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		VAR_SAMP( provider_enumeration_year ) , 
		STDDEV_SAMP( provider_enumeration_year ) 
	FROM npi" 
)

dbGetQuery( db , 
	"SELECT 
		provider_gender_code , 
		VAR_SAMP( provider_enumeration_year ) AS var_provider_enumeration_year ,
		STDDEV_SAMP( provider_enumeration_year ) AS stddev_provider_enumeration_year
	FROM npi 
	GROUP BY provider_gender_code" 
)
```

### Regression Models and Tests of Association {-}

Calculate the correlation between two variables, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		CORR( CAST( individual AS DOUBLE ) , CAST( provider_enumeration_year AS DOUBLE ) )
	FROM npi" 
)

dbGetQuery( db , 
	"SELECT 
		provider_gender_code , 
		CORR( CAST( individual AS DOUBLE ) , CAST( provider_enumeration_year AS DOUBLE ) )
	FROM npi 
	GROUP BY provider_gender_code" 
)
```

## Analysis Examples with `dplyr` {-}

The R `dplyr` library offers an alternative grammar of data manipulation to base R and SQL syntax. [dplyr](https://github.com/tidyverse/dplyr/) offers many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, and the `tidyverse` style of non-standard evaluation. [This vignette](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html) details the available features. As a starting point for MTPS users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(dplyr)
dplyr_db <- MonetDBLite::src_monetdblite( dbdir )
mtps_tbl <- tbl( dplyr_db , 'npi' )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
mtps_tbl %>%
	summarize( mean = mean( provider_enumeration_year ) )

mtps_tbl %>%
	group_by( provider_gender_code ) %>%
	summarize( mean = mean( provider_enumeration_year ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT COUNT(*) FROM npi" )
```

## Database Shutdown {-}

```{r eval = FALSE , results = "hide" }
dbDisconnect( db , shutdown = TRUE )
```

<!--chapter:end:31-mtps.Rmd-->

# National Beneficiary Survey (NBS) {-}

[![Build Status](https://travis-ci.org/asdfree/nbs.svg?branch=master)](https://travis-ci.org/asdfree/nbs) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/nbs?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/nbs)

The National Beneficiary Survey (NBS) is the principal microdata for disability researchers in the United States interested in Social Security program performance.

* One table with one row per sampled youth respondent.

* A complex sample survey designed to generalize to Americans covered by either Social Security Disability Insurance (SSDI) or Supplemental Security Income (SSI). Note that the public use files do not include individuals sampled for ticket-to-work (TTW) programs.

* Released at irregular intervals, with 2004, 2005, 2006, and 2010 available and 2015, 2017, and 2019 forthcoming.

* Administered by the [Social Security Administration](http://www.ssa.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available NBS microdata by simply specifying `"nbs"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "nbs" , output_dir = file.path( path.expand( "~" ) , "NBS" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the NBS catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available NBS microdata files
nbs_cat <-
	get_catalog( "nbs" ,
		output_dir = file.path( path.expand( "~" ) , "NBS" ) )

# 2010 only
nbs_cat <- subset( nbs_cat , this_round == 4 )
# download the microdata to your local computer
lodown( "nbs" , nbs_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
library(survey)

nbs_df <- readRDS( file.path( path.expand( "~" ) , "NBS" , "round 04.rds" ) )

nbs_design <- 
	svydesign( 
		~ a_psu_pub , 
		strata = ~ a_strata , 
		data = nbs_df , 
		weights = ~ wtr4_ben 
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
nbs_design <- 
	update( 
		nbs_design , 
		
		male = as.numeric( orgsampinfo_sex == 1 ) ,
		
		age_categories = 
			factor( 
				c_intage_pub ,
				labels = 
					c( "18-25" , "26-40" , "41-55" , "56 and older" )
			)
		
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( nbs_design , "sampling" ) != 0 )

svyby( ~ one , ~ age_categories , nbs_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , nbs_design )

svyby( ~ one , ~ age_categories , nbs_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ n_totssbenlastmnth_pub , nbs_design )

svyby( ~ n_totssbenlastmnth_pub , ~ age_categories , nbs_design , svymean )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ c_hhsize_pub , nbs_design )

svyby( ~ c_hhsize_pub , ~ age_categories , nbs_design , svymean )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ n_totssbenlastmnth_pub , nbs_design )

svyby( ~ n_totssbenlastmnth_pub , ~ age_categories , nbs_design , svytotal )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ c_hhsize_pub , nbs_design )

svyby( ~ c_hhsize_pub , ~ age_categories , nbs_design , svytotal )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ n_totssbenlastmnth_pub , nbs_design , 0.5 )

svyby( 
	~ n_totssbenlastmnth_pub , 
	~ age_categories , 
	nbs_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE 
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ n_ssilastmnth_pub , 
	denominator = ~ n_totssbenlastmnth_pub , 
	nbs_design 
)
```

### Subsetting {-}

Restrict the survey design to currently covered by Medicare:
```{r eval = FALSE , results = "hide" }
sub_nbs_design <- subset( nbs_design , c_curmedicare == 1 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ n_totssbenlastmnth_pub , sub_nbs_design )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ n_totssbenlastmnth_pub , nbs_design )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ n_totssbenlastmnth_pub , 
		~ age_categories , 
		nbs_design , 
		svymean 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( nbs_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ n_totssbenlastmnth_pub , nbs_design )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ n_totssbenlastmnth_pub , nbs_design , deff = TRUE )

# SRS with replacement
svymean( ~ n_totssbenlastmnth_pub , nbs_design , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ male , nbs_design ,
	method = "likelihood" )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( n_totssbenlastmnth_pub ~ male , nbs_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ male + c_hhsize_pub , 
	nbs_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		n_totssbenlastmnth_pub ~ male + c_hhsize_pub , 
		nbs_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for NBS users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
nbs_srvyr_design <- as_survey( nbs_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
nbs_srvyr_design %>%
	summarize( mean = survey_mean( n_totssbenlastmnth_pub ) )

nbs_srvyr_design %>%
	group_by( age_categories ) %>%
	summarize( mean = survey_mean( n_totssbenlastmnth_pub ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:32-nbs.Rmd-->

# Youth Risk Behavior Surveillance System (NCVS) {-}

The Youth Risk Behavior Surveillance System is the high school edition of the Behavioral Risk Factor Surveillance System (BRFSS), a scientific study of good kids who do bad things.

* One table with one row per sampled youth respondent.

* A complex sample survey designed to generalize to all public and private school students in grades 9-12 in the United States.

* Released biennially since 1993.

* Administered by the [Centers for Disease Control and Prevention](http://www.cdc.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available NCVS microdata by simply specifying `"ncvs"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "ncvs" , output_dir = file.path( path.expand( "~" ) , "NCVS" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the NCVS catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available NCVS microdata files
ncvs_cat <-
	get_catalog( "ncvs" ,
		output_dir = file.path( path.expand( "~" ) , "NCVS" ) )

# 2015 only
ncvs_cat <- subset( ncvs_cat , year == 2015 )
# download the microdata to your local computer
lodown( "ncvs" , ncvs_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
library(survey)

ncvs_df <- readRDS( file.path( path.expand( "~" ) , "NCVS" , "2015 main.rds" ) )

ncvs_design <- 
	svydesign( 
		~ psu , 
		strata = ~ stratum , 
		data = ncvs_df , 
		weights = ~ weight , 
		nest = TRUE 
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
ncvs_design <- 
	update( 
		ncvs_design , 
		q2 = q2 ,
		never_rarely_wore_bike_helmet = as.numeric( qn8 == 1 ) ,
		ever_smoked_marijuana = as.numeric( qn47 == 1 ) ,
		ever_tried_to_quit_cigarettes = as.numeric( q36 > 2 ) ,
		smoked_cigarettes_past_year = as.numeric( q36 > 1 )
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( ncvs_design , "sampling" ) != 0 )

svyby( ~ one , ~ ever_smoked_marijuana , ncvs_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , ncvs_design )

svyby( ~ one , ~ ever_smoked_marijuana , ncvs_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ bmipct , ncvs_design , na.rm = TRUE )

svyby( ~ bmipct , ~ ever_smoked_marijuana , ncvs_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ q2 , ncvs_design , na.rm = TRUE )

svyby( ~ q2 , ~ ever_smoked_marijuana , ncvs_design , svymean , na.rm = TRUE )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ bmipct , ncvs_design , na.rm = TRUE )

svyby( ~ bmipct , ~ ever_smoked_marijuana , ncvs_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ q2 , ncvs_design , na.rm = TRUE )

svyby( ~ q2 , ~ ever_smoked_marijuana , ncvs_design , svytotal , na.rm = TRUE )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ bmipct , ncvs_design , 0.5 , na.rm = TRUE )

svyby( 
	~ bmipct , 
	~ ever_smoked_marijuana , 
	ncvs_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ ever_tried_to_quit_cigarettes , 
	denominator = ~ smoked_cigarettes_past_year , 
	ncvs_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to youths who ever drank alcohol:
```{r eval = FALSE , results = "hide" }
sub_ncvs_design <- subset( ncvs_design , qn41 == 1 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ bmipct , sub_ncvs_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ bmipct , ncvs_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ bmipct , 
		~ ever_smoked_marijuana , 
		ncvs_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( ncvs_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ bmipct , ncvs_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ bmipct , ncvs_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ bmipct , ncvs_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ never_rarely_wore_bike_helmet , ncvs_design ,
	method = "likelihood" , na.rm = TRUE )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( bmipct ~ never_rarely_wore_bike_helmet , ncvs_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ never_rarely_wore_bike_helmet + q2 , 
	ncvs_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		bmipct ~ never_rarely_wore_bike_helmet + q2 , 
		ncvs_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for NCVS users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
ncvs_srvyr_design <- as_survey( ncvs_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
ncvs_srvyr_design %>%
	summarize( mean = survey_mean( bmipct , na.rm = TRUE ) )

ncvs_srvyr_design %>%
	group_by( ever_smoked_marijuana ) %>%
	summarize( mean = survey_mean( bmipct , na.rm = TRUE ) )
```

---

## Replication Example {-}

This snippet replicates the "never/rarely wore bicycle helmet" row of [PDF page 29 of this CDC analysis software document](https://www.cdc.gov/healthyyouth/data/yrbs/pdf/2015/2015_yrbs_analysis_software.pdf#page=29).

```{r eval = FALSE , results = "hide" }

unwtd.count( ~ never_rarely_wore_bike_helmet , yrbss_design )

svytotal( ~ one , subset( yrbss_design , !is.na( never_rarely_wore_bike_helmet ) ) )
 
svymean( ~ never_rarely_wore_bike_helmet , yrbss_design , na.rm = TRUE )

svyciprop( ~ never_rarely_wore_bike_helmet , yrbss_design , na.rm = TRUE , method = "beta" )

```



<!--chapter:end:33-ncvs.Rmd-->

# National Health and Nutrition Examination Survey (NHANES) {-}

[![Build Status](https://travis-ci.org/asdfree/nhanes.svg?branch=master)](https://travis-ci.org/asdfree/nhanes) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/nhanes?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/nhanes)

The National Health and Nutrition Examination Survey (NHANES) is this fascinating survey where doctors and dentists accompany survey interviewers in a little mobile medical center that drives around the country. While the survey methodologists interview people, the medical professionals administer laboratory tests and conduct a thorough physical examination. The blood work and medical exam allow researchers to answer tough questions like, "how many people have diabetes but don't know they have diabetes?"

* Many tables containing information gathered from the various examinations, generally with one row per individual respondent.

* A complex sample survey designed to generalize to the civilian non-institutionalized population of the United States.

* Released biennially since 1999-2000.

* Administered by the [Centers for Disease Control and Prevention](http://www.cdc.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available NHANES microdata by simply specifying `"nhanes"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "nhanes" , output_dir = file.path( path.expand( "~" ) , "NHANES" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the NHANES catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available NHANES microdata files
nhanes_cat <-
	get_catalog( "nhanes" ,
		output_dir = file.path( path.expand( "~" ) , "NHANES" ) )

# 2015-2016 only
nhanes_cat <- subset( nhanes_cat , years == "2015-2016" )
# download the microdata to your local computer
lodown( "nhanes" , nhanes_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
options( survey.lonely.psu = "adjust" )

library(survey)

nhanes_demo_df <- 
	readRDS( file.path( path.expand( "~" ) , "NHANES" , "2015-2016/demo_i.rds" ) )

nhanes_tchol_df <- 
	readRDS( file.path( path.expand( "~" ) , "NHANES" , "2015-2016/tchol_i.rds" ) )

nhanes_df <- merge( nhanes_demo_df , nhanes_tchol_df , all = TRUE )

stopifnot( nrow( nhanes_df ) == nrow( nhanes_demo_df ) )

# keep only individuals who took the "mobile examination center" component
nhanes_df <- subset( nhanes_df , ridstatr %in% 2 )

nhanes_design <- 
	svydesign(
		id = ~sdmvpsu , 
		strata = ~sdmvstra ,
		nest = TRUE ,
		weights = ~wtmec2yr ,
		data = nhanes_df
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
nhanes_design <- 
	update( 
		nhanes_design , 
		
		one = 1 ,
		
		pregnant_at_interview = 
			ifelse( ridexprg %in% 1:2 , as.numeric( ridexprg == 1 ) , NA ) ,
		
		race_ethnicity = 
			factor( 
				c( 3 , 3 , 1 , 2 , 4 )[ ridreth1 ] ,
				levels = 1:4 , 
				labels = 
					c( 'non-hispanic white' , 'non-hispanic black' , 
						'hispanic' , 'other' )
			) ,
		
		age_category =
			factor(
				findInterval( ridageyr , c( 20 , 40 , 60 ) ) ,
				labels = c( "0-19" , "20-39" , "40-59" , "60+" )
			)
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( nhanes_design , "sampling" ) != 0 )

svyby( ~ one , ~ race_ethnicity , nhanes_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , nhanes_design )

svyby( ~ one , ~ race_ethnicity , nhanes_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ lbxtc , nhanes_design , na.rm = TRUE )

svyby( ~ lbxtc , ~ race_ethnicity , nhanes_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ riagendr , nhanes_design )

svyby( ~ riagendr , ~ race_ethnicity , nhanes_design , svymean )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ lbxtc , nhanes_design , na.rm = TRUE )

svyby( ~ lbxtc , ~ race_ethnicity , nhanes_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ riagendr , nhanes_design )

svyby( ~ riagendr , ~ race_ethnicity , nhanes_design , svytotal )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ lbxtc , nhanes_design , 0.5 , na.rm = TRUE )

svyby( 
	~ lbxtc , 
	~ race_ethnicity , 
	nhanes_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ lbxtc , 
	denominator = ~ ridageyr , 
	nhanes_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to respondents aged 60 or older:
```{r eval = FALSE , results = "hide" }
sub_nhanes_design <- subset( nhanes_design , age_category == "60+" )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ lbxtc , sub_nhanes_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ lbxtc , nhanes_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ lbxtc , 
		~ race_ethnicity , 
		nhanes_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( nhanes_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ lbxtc , nhanes_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ lbxtc , nhanes_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ lbxtc , nhanes_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ pregnant_at_interview , nhanes_design ,
	method = "likelihood" , na.rm = TRUE )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( lbxtc ~ pregnant_at_interview , nhanes_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ pregnant_at_interview + riagendr , 
	nhanes_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		lbxtc ~ pregnant_at_interview + riagendr , 
		nhanes_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for NHANES users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
nhanes_srvyr_design <- as_survey( nhanes_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
nhanes_srvyr_design %>%
	summarize( mean = survey_mean( lbxtc , na.rm = TRUE ) )

nhanes_srvyr_design %>%
	group_by( race_ethnicity ) %>%
	summarize( mean = survey_mean( lbxtc , na.rm = TRUE ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:34-nhanes.Rmd-->

# National Health Interview Survey (NHIS) {-}

[![Build Status](https://travis-ci.org/asdfree/nhis.svg?branch=master)](https://travis-ci.org/asdfree/nhis) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/nhis?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/nhis)

The National Health Interview Survey (NHIS) is America's most detailed household survey of health status and medical experience.

* A main table with one row for each person within each sampled household, mergeable other tables like the sample child table with a more detailed questionnaire for only one child (when available) within each sampled household.

* A complex sample survey designed to generalize to the civilian non-institutionalized population of the United States.

* Released annually since 1963, the most recent major re-design in 1997.

* Administered by the [Centers for Disease Control and Prevention](http://www.cdc.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available NHIS microdata by simply specifying `"nhis"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "nhis" , output_dir = file.path( path.expand( "~" ) , "NHIS" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the NHIS catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available NHIS microdata files
nhis_cat <-
	get_catalog( "nhis" ,
		output_dir = file.path( path.expand( "~" ) , "NHIS" ) )

# 2016 only
nhis_cat <- subset( nhis_cat , year == 2016 )
# download the microdata to your local computer
lodown( "nhis" , nhis_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a multiply-imputed, complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
options( survey.lonely.psu = "adjust" )

library(survey)
library(mitools)

nhis_personsx_df <- 
	readRDS( file.path( path.expand( "~" ) , "NHIS" , "2016/personsx.rds" ) )

nhis_income_list <- 
	readRDS( file.path( path.expand( "~" ) , "NHIS" , "2016/incmimp.rds" ) )

merge_variables <- c( "hhx" , "fmx" , "fpx" )

nhis_personsx_df[ merge_variables ] <- 
	sapply( nhis_personsx_df[ merge_variables ] , as.numeric )

inc_vars_to_keep <- 
	c( 
		merge_variables , 
		setdiff( 
			names( nhis_income_list[[ 1 ]] ) , 
			names( nhis_personsx_df )
		)
	)

# personsx variables to keep
vars_to_keep <- 
	c( merge_variables , "ppsu" , "pstrat" , "wtfa" ,
		'phstat' , 'sex' , 'hospno' , 'age_p' , 'hinotmyr' , 'notcov' )

nhis_personsx_df <- nhis_personsx_df[ vars_to_keep ]
	
nhis_personsx_list <-
	lapply( nhis_income_list ,
		function( w ){
			w <- w[ inc_vars_to_keep ]
			w[ merge_variables ] <- sapply( w[ merge_variables ] , as.numeric )
			result <- merge( nhis_personsx_df , w )
			stopifnot( nrow( result ) == nrow( nhis_personsx_df ) )
			result
		} )

# personsx design		
nhis_design <- 
	svydesign( 
		id = ~ppsu , 
		strata = ~pstrat ,
		nest = TRUE ,
		weights = ~wtfa ,
		data = imputationList( nhis_personsx_list )
	)

rm( nhis_personsx_list ) ; gc()

nhis_samadult_df <- 
	readRDS( file.path( path.expand( "~" ) , "NHIS" , "2016/samadult.rds" ) )

nhis_samadult_df[ merge_variables ] <- 
	sapply( nhis_samadult_df[ merge_variables ] , as.numeric )

samadult_vars_to_keep <- 
	c( 
		merge_variables , 
		setdiff( 
			names( nhis_samadult_df ) , 
			names( nhis_personsx_df ) 
		) 
	)

nhis_personsx_samadult_df <-
	merge( nhis_personsx_df , nhis_samadult_df[ samadult_vars_to_keep ] )

stopifnot( nrow( nhis_personsx_samadult_df ) == nrow( nhis_samadult_df ) )

rm( nhis_personsx_df , nhis_samadult_df ) ; gc()

nhis_samadult_list <-
	lapply( nhis_income_list ,
		function( w ){
			w <- w[ inc_vars_to_keep ]
			w[ merge_variables ] <- sapply( w[ merge_variables ] , as.numeric )
			result <- merge( nhis_personsx_samadult_df , w )
			stopifnot( nrow( result ) == nrow( nhis_personsx_samadult_df ) )
			result
		} )

rm( nhis_income_list , nhis_personsx_samadult_df ) ; gc()

# sample adult design (commented out)
# nhis_samadult_design <- 
	# svydesign( 
		# id = ~ppsu , 
		# strata = ~pstrat ,
		# nest = TRUE ,
		# weights = ~wtfa_sa ,
		# data = imputationList( nhis_samadult_list )
	# )
	
rm( nhis_samadult_list ) ; gc()
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
nhis_design <- 
	update( 
		nhis_design , 
		
		one = 1 ,
		
		poverty_category =
			factor( 
				findInterval( povrati3 , 1:4 ) ,
				labels = 
					c( "below poverty" , "100-199%" , "200-299%" , "300-399%" , "400%+" )
			) ,
			
		fair_or_poor_reported_health = 
			ifelse( phstat %in% 1:5 , as.numeric( phstat >= 4 ) , NA ) ,
			
		sex = factor( sex , labels = c( "male" , "female" ) ) ,
		
		hospno = ifelse( hospno > 366 , NA , hospno )

	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( nhis_design , svyby( ~ one , ~ one , unwtd.count ) ) )

MIcombine( with( nhis_design , svyby( ~ one , ~ poverty_category , unwtd.count ) ) )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( nhis_design , svytotal( ~ one ) ) )

MIcombine( with( nhis_design ,
	svyby( ~ one , ~ poverty_category , svytotal )
) )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( nhis_design , svymean( ~ age_p ) ) )

MIcombine( with( nhis_design ,
	svyby( ~ age_p , ~ poverty_category , svymean )
) )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( nhis_design , svymean( ~ sex ) ) )

MIcombine( with( nhis_design ,
	svyby( ~ sex , ~ poverty_category , svymean )
) )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( nhis_design , svytotal( ~ age_p ) ) )

MIcombine( with( nhis_design ,
	svyby( ~ age_p , ~ poverty_category , svytotal )
) )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( nhis_design , svytotal( ~ sex ) ) )

MIcombine( with( nhis_design ,
	svyby( ~ sex , ~ poverty_category , svytotal )
) )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( nhis_design , svyquantile( ~ age_p , 0.5 , se = TRUE ) ) )

MIcombine( with( nhis_design ,
	svyby( 
		~ age_p , ~ poverty_category , svyquantile , 0.5 ,
		se = TRUE , keep.var = TRUE , ci = TRUE 
) ) )
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
MIcombine( with( nhis_design ,
	svyratio( numerator = ~ hinotmyr , denominator = ~ hospno , na.rm = TRUE )
) )
```

### Subsetting {-}

Restrict the survey design to uninsured:
```{r eval = FALSE , results = "hide" }
sub_nhis_design <- subset( nhis_design , notcov == 1 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
MIcombine( with( sub_nhis_design , svymean( ~ age_p ) ) )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <-
	MIcombine( with( nhis_design ,
		svymean( ~ age_p )
	) )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	MIcombine( with( nhis_design ,
		svyby( ~ age_p , ~ poverty_category , svymean )
	) )

coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( nhis_design$designs[[1]] )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
MIcombine( with( nhis_design , svyvar( ~ age_p ) ) )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
MIcombine( with( nhis_design ,
	svymean( ~ age_p , deff = TRUE )
) )

# SRS with replacement
MIcombine( with( nhis_design ,
	svymean( ~ age_p , deff = "replace" )
) )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
lodown:::MIsvyciprop( ~ fair_or_poor_reported_health , nhis_design ,
	method = "likelihood" , na.rm = TRUE )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
lodown:::MIsvyttest( age_p ~ fair_or_poor_reported_health , nhis_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
lodown:::MIsvychisq( ~ fair_or_poor_reported_health + sex , nhis_design )
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	MIcombine( with( nhis_design ,
		svyglm( age_p ~ fair_or_poor_reported_health + sex )
	) )
	
summary( glm_result )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:35-nhis.Rmd-->

# Pesquisa Nacional por Amostra de Domicilios (NHTS) {-}

*Contributed by Dr. Djalma Pessoa <<pessoad@gmail.com>>*

Brazil's previous principal household survey, the Pesquisa Nacional por Amostra de Domicilios (PNAD) measures general education, labor, income, and housing characteristics of the population.

* One table with one row per sampled household and a second table with one row per individual within each sampled household.

* A complex sample survey designed to generalize to the civilian non-institutional population of Brazil, although the rural north was not included prior to 2004.

* Released annually since 2001 except for years ending in zero, when the decennial census takes its place.

* Administered by the [Instituto Brasileiro de Geografia e Estatistica](http://www.ibge.gov.br/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available NHTS microdata by simply specifying `"nhts"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "nhts" , output_dir = file.path( path.expand( "~" ) , "NHTS" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the NHTS catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available NHTS microdata files
nhts_cat <-
	get_catalog( "nhts" ,
		output_dir = file.path( path.expand( "~" ) , "NHTS" ) )

# 2011 only
nhts_cat <- subset( nhts_cat , year == 2011 )
# download the microdata to your local computer
lodown( "nhts" , nhts_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a database-backed complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
library(DBI)
library(MonetDBLite)
library(survey)

options( survey.lonely.psu = "adjust" )

prestratified_design <-
	svydesign(
		id = ~v4618 ,
		strata = ~v4617 ,
		data = nhts_cat[ 1 , "db_tablename" ] ,
		weights = ~pre_wgt ,
		nest = TRUE ,
		dbtype = "MonetDBLite" ,
		dbname = nhts_cat[ 1 , "dbfolder" ]
	)
	
nhts_design <- 
	lodown:::pnad_postStratify( 
		design = prestratified_design ,
		strata.col = 'v4609' ,
		oldwgt = 'pre_wgt'
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
nhts_design <- 
	update( 
		nhts_design , 
		age_categories = factor( 1 + findInterval( v8005 , seq( 5 , 60 , 5 ) ) ) ,
		male = as.numeric( v0302 == 2 ) ,
		teenagers = as.numeric( v8005 > 12 & v8005 < 20 ) ,
		started_working_before_thirteen = as.numeric( v9892 < 13 )
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( nhts_design , "sampling" ) != 0 )

svyby( ~ one , ~ region , nhts_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , nhts_design )

svyby( ~ one , ~ region , nhts_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ v4720 , nhts_design , na.rm = TRUE )

svyby( ~ v4720 , ~ region , nhts_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ age_categories , nhts_design )

svyby( ~ age_categories , ~ region , nhts_design , svymean )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ v4720 , nhts_design , na.rm = TRUE )

svyby( ~ v4720 , ~ region , nhts_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ age_categories , nhts_design )

svyby( ~ age_categories , ~ region , nhts_design , svytotal )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ v4720 , nhts_design , 0.5 , na.rm = TRUE )

svyby( 
	~ v4720 , 
	~ region , 
	nhts_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ started_working_before_thirteen , 
	denominator = ~ teenagers , 
	nhts_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to married persons:
```{r eval = FALSE , results = "hide" }
sub_nhts_design <- subset( nhts_design , v4011 == 1 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ v4720 , sub_nhts_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ v4720 , nhts_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ v4720 , 
		~ region , 
		nhts_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( nhts_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ v4720 , nhts_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ v4720 , nhts_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ v4720 , nhts_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ male , nhts_design ,
	method = "likelihood" )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( v4720 ~ male , nhts_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ male + age_categories , 
	nhts_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		v4720 ~ male + age_categories , 
		nhts_design 
	)

summary( glm_result )
```

## Poverty and Inequality Estimation with `convey` {-}

The R `convey` library estimates measures of income concentration, poverty, inequality, and wellbeing. [This textbook](https://guilhermejacob.github.io/context/) details the available features. As a starting point for NHTS users, this code calculates the gini coefficient on complex sample survey data:

```{r eval = FALSE , results = "hide" }
library(convey)
nhts_design <- convey_prep( nhts_design )

sub_nhts_design <- 
	subset( 
		nhts_design , 
		!is.na( v4720 ) & v4720 != 0 & v8005 >= 15
	)

svygini( ~ v4720 , sub_nhts_design , na.rm = TRUE )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }
svytotal( ~one , nhts_design )
svytotal( ~factor( v0302 ) , nhts_design )
cv( svytotal( ~factor( v0302 ) , nhts_design ) )
```

## Database Shutdown {-}

```{r eval = FALSE , results = "hide" }
close( nhts_design , shutdown = TRUE )
```

<!--chapter:end:36-nhts.Rmd-->

# National Immunization Survey (NIS) {-}

[![Build Status](https://travis-ci.org/asdfree/nis.svg?branch=master)](https://travis-ci.org/asdfree/nis) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/nis?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/nis)

*Contributed by Joe Walsh <<jtwalsh@protonmail.com>>*

The National Immunization Survey tracks childhood vaccination rates at the state-level.

* One table with one row per sampled toddler.

* A complex sample survey designed to generalize to children aged 19-35 months in the United States.

* Released biennially since 1995.

* Administered by the [Centers for Disease Control and Prevention](http://www.cdc.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available NIS microdata by simply specifying `"nis"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "nis" , output_dir = file.path( path.expand( "~" ) , "NIS" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the NIS catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available NIS microdata files
nis_cat <-
	get_catalog( "nis" ,
		output_dir = file.path( path.expand( "~" ) , "NIS" ) )

# 2015 only
nis_cat <- subset( nis_cat , year == 2015 )
# download the microdata to your local computer
lodown( "nis" , nis_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
options( survey.lonely.psu = "adjust" )

library(survey)

nis_df <- readRDS( file.path( path.expand( "~" ) , "NIS" , "2015 main.rds" ) )

nis_design <- 
	svydesign(
		id = ~ seqnumhh , 
		strata = ~ stratum , 
		weights = ~ provwt_d , 
		data = subset( nis_df , provwt_d > 0 ) 
	) 
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
nis_design <- 
	
	update( 
		
		nis_design , 
		
		state_name =
		
			factor(
			
				state ,
				
				levels = 
					c(1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 
					21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 
					37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 
					55, 56, 66, 72, 78) ,
					
				labels = 
					c("ALABAMA", "ALASKA", "ARIZONA", "ARKANSAS", "CALIFORNIA", 
					"COLORADO", "CONNECTICUT", "DELAWARE", "DISTRICT OF COLUMBIA", 
					"FLORIDA", "GEORGIA", "HAWAII", "IDAHO", "ILLINOIS", "INDIANA",
					"IOWA", "KANSAS", "KENTUCKY", "LOUISIANA", "MAINE", "MARYLAND",
					"MASSACHUSETTS", "MICHIGAN", "MINNESOTA", "MISSISSIPPI", 
					"MISSOURI", "MONTANA", "NEBRASKA", "NEVADA", "NEW HAMPSHIRE",
					"NEW JERSEY", "NEW MEXICO", "NEW YORK", "NORTH CAROLINA", 
					"NORTH DAKOTA", "OHIO", "OKLAHOMA", "OREGON", "PENNSYLVANIA",
					"RHODE ISLAND", "SOUTH CAROLINA", "SOUTH DAKOTA", "TENNESSEE",
					"TEXAS", "UTAH", "VERMONT", "VIRGINIA", "WASHINGTON",
					"WEST VIRGINIA", "WISCONSIN", "WYOMING", "GUAM", "PUERTO RICO",
					"U.S. VIRGIN ISLANDS")
					
			) ,
			
		sex = 
			factor( 
				ifelse( sex %in% 1:2 , sex , NA ) , 
				labels = c( "male" , "female" )
			) ,
			
		dtap_3p =

			as.numeric(

				( p_numdah >= 3 ) |
				( p_numdhi >= 3 ) |
				( p_numdih >= 3 ) |
				( p_numdta >= 3 ) |
				( p_numdtp >= 3 )

			) ,
		
		dtap_4p =

			as.numeric(

				( p_numdah >= 4 ) |
				( p_numdhi >= 4 ) |
				( p_numdih >= 4 ) |
				( p_numdta >= 4 ) |
				( p_numdtp >= 4 )

			)
			
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( nis_design , "sampling" ) != 0 )

svyby( ~ one , ~ state_name , nis_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , nis_design )

svyby( ~ one , ~ state_name , nis_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ p_nuhepx , nis_design , na.rm = TRUE )

svyby( ~ p_nuhepx , ~ state_name , nis_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ sex , nis_design , na.rm = TRUE )

svyby( ~ sex , ~ state_name , nis_design , svymean , na.rm = TRUE )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ p_nuhepx , nis_design , na.rm = TRUE )

svyby( ~ p_nuhepx , ~ state_name , nis_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ sex , nis_design , na.rm = TRUE )

svyby( ~ sex , ~ state_name , nis_design , svytotal , na.rm = TRUE )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ p_nuhepx , nis_design , 0.5 , na.rm = TRUE )

svyby( 
	~ p_nuhepx , 
	~ state_name , 
	nis_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ childnm , 
	denominator = ~ bf_endr06 , 
	nis_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to toddlers up to date on polio shots:
```{r eval = FALSE , results = "hide" }
sub_nis_design <- subset( nis_design , p_utdpol == 1 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ p_nuhepx , sub_nis_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ p_nuhepx , nis_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ p_nuhepx , 
		~ state_name , 
		nis_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( nis_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ p_nuhepx , nis_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ p_nuhepx , nis_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ p_nuhepx , nis_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ dtap_3p , nis_design ,
	method = "likelihood" )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( p_nuhepx ~ dtap_3p , nis_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ dtap_3p + sex , 
	nis_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		p_nuhepx ~ dtap_3p + sex , 
		nis_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for NIS users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
nis_srvyr_design <- as_survey( nis_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
nis_srvyr_design %>%
	summarize( mean = survey_mean( p_nuhepx , na.rm = TRUE ) )

nis_srvyr_design %>%
	group_by( state_name ) %>%
	summarize( mean = survey_mean( p_nuhepx , na.rm = TRUE ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:37-nis.Rmd-->

# Youth Risk Behavior Surveillance System (NLS) {-}

[![Build Status](https://travis-ci.org/asdfree/nls.svg?branch=master)](https://travis-ci.org/asdfree/nls) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/nls?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/nls)

The Youth Risk Behavior Surveillance System is the high school edition of the Behavioral Risk Factor Surveillance System (BRFSS), a scientific study of good kids who do bad things.

* One table with one row per sampled youth respondent.

* A complex sample survey designed to generalize to all public and private school students in grades 9-12 in the United States.

* Released biennially since 1993.

* Administered by the [Centers for Disease Control and Prevention](http://www.cdc.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available NLS microdata by simply specifying `"nls"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "nls" , output_dir = file.path( path.expand( "~" ) , "NLS" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the NLS catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available NLS microdata files
nls_cat <-
	get_catalog( "nls" ,
		output_dir = file.path( path.expand( "~" ) , "NLS" ) )

# 2015 only
nls_cat <- subset( nls_cat , year == 2015 )
# download the microdata to your local computer
lodown( "nls" , nls_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
library(survey)

nls_df <- readRDS( file.path( path.expand( "~" ) , "NLS" , "2015 main.rds" ) )

nls_design <- 
	svydesign( 
		~ psu , 
		strata = ~ stratum , 
		data = nls_df , 
		weights = ~ weight , 
		nest = TRUE 
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
nls_design <- 
	update( 
		nls_design , 
		q2 = q2 ,
		never_rarely_wore_bike_helmet = as.numeric( qn8 == 1 ) ,
		ever_smoked_marijuana = as.numeric( qn47 == 1 ) ,
		ever_tried_to_quit_cigarettes = as.numeric( q36 > 2 ) ,
		smoked_cigarettes_past_year = as.numeric( q36 > 1 )
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( nls_design , "sampling" ) != 0 )

svyby( ~ one , ~ ever_smoked_marijuana , nls_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , nls_design )

svyby( ~ one , ~ ever_smoked_marijuana , nls_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ bmipct , nls_design , na.rm = TRUE )

svyby( ~ bmipct , ~ ever_smoked_marijuana , nls_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ q2 , nls_design , na.rm = TRUE )

svyby( ~ q2 , ~ ever_smoked_marijuana , nls_design , svymean , na.rm = TRUE )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ bmipct , nls_design , na.rm = TRUE )

svyby( ~ bmipct , ~ ever_smoked_marijuana , nls_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ q2 , nls_design , na.rm = TRUE )

svyby( ~ q2 , ~ ever_smoked_marijuana , nls_design , svytotal , na.rm = TRUE )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ bmipct , nls_design , 0.5 , na.rm = TRUE )

svyby( 
	~ bmipct , 
	~ ever_smoked_marijuana , 
	nls_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ ever_tried_to_quit_cigarettes , 
	denominator = ~ smoked_cigarettes_past_year , 
	nls_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to youths who ever drank alcohol:
```{r eval = FALSE , results = "hide" }
sub_nls_design <- subset( nls_design , qn41 == 1 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ bmipct , sub_nls_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ bmipct , nls_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ bmipct , 
		~ ever_smoked_marijuana , 
		nls_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( nls_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ bmipct , nls_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ bmipct , nls_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ bmipct , nls_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ never_rarely_wore_bike_helmet , nls_design ,
	method = "likelihood" , na.rm = TRUE )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( bmipct ~ never_rarely_wore_bike_helmet , nls_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ never_rarely_wore_bike_helmet + q2 , 
	nls_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		bmipct ~ never_rarely_wore_bike_helmet + q2 , 
		nls_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for NLS users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
nls_srvyr_design <- as_survey( nls_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
nls_srvyr_design %>%
	summarize( mean = survey_mean( bmipct , na.rm = TRUE ) )

nls_srvyr_design %>%
	group_by( ever_smoked_marijuana ) %>%
	summarize( mean = survey_mean( bmipct , na.rm = TRUE ) )
```

---

## Replication Example {-}

This snippet replicates the "never/rarely wore bicycle helmet" row of [PDF page 29 of this CDC analysis software document](https://www.cdc.gov/healthyyouth/data/yrbs/pdf/2015/2015_yrbs_analysis_software.pdf#page=29).

```{r eval = FALSE , results = "hide" }

unwtd.count( ~ never_rarely_wore_bike_helmet , yrbss_design )

svytotal( ~ one , subset( yrbss_design , !is.na( never_rarely_wore_bike_helmet ) ) )
 
svymean( ~ never_rarely_wore_bike_helmet , yrbss_design , na.rm = TRUE )

svyciprop( ~ never_rarely_wore_bike_helmet , yrbss_design , na.rm = TRUE , method = "beta" )

```



<!--chapter:end:38-nls.Rmd-->

# National Plan and Provider Enumeration System (NPPES) {-}

The National Plan and Provider Enumeration System (NPPES) contains information about every medical provider, insurance plan, and clearinghouse actively operating in the United States healthcare industry.

* A single large table with one row per enumerated health care provider.

* A census of individuals and organizations who bill for medical services in the United States.

* Updated monthly with new providers.

* Maintained by the United States [Centers for Medicare & Medicaid Services (CMS)](http://www.cms.gov/)

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available NPPES microdata by simply specifying `"nppes"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "nppes" , output_dir = file.path( path.expand( "~" ) , "NPPES" ) )
```

## Analysis Examples with SQL and `MonetDBLite` {-}

Connect to a database:

```{r eval = FALSE }
library(DBI)
dbdir <- file.path( path.expand( "~" ) , "NPPES" , "MonetDB" )
db <- dbConnect( MonetDBLite::MonetDBLite() , dbdir )
```

```{r eval = FALSE }

```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
dbSendQuery( db , "ALTER TABLE npi ADD COLUMN individual INTEGER" )

dbSendQuery( db , 
	"UPDATE npi 
	SET individual = 
		CASE WHEN entity_type_code = 1 THEN 1 ELSE 0 END" 
)

dbSendQuery( db , "ALTER TABLE npi ADD COLUMN provider_enumeration_year INTEGER" )

dbSendQuery( db , 
	"UPDATE npi 
	SET provider_enumeration_year = 
		CAST( SUBSTRING( provider_enumeration_date , 7 , 10 ) AS INTEGER )" 
)
```

### Unweighted Counts {-}

Count the unweighted number of records in the SQL table, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT COUNT(*) FROM npi" )

dbGetQuery( db ,
	"SELECT
		provider_gender_code ,
		COUNT(*) 
	FROM npi
	GROUP BY provider_gender_code"
)
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT AVG( provider_enumeration_year ) FROM npi" )

dbGetQuery( db , 
	"SELECT 
		provider_gender_code , 
		AVG( provider_enumeration_year ) AS mean_provider_enumeration_year
	FROM npi 
	GROUP BY provider_gender_code" 
)
```

Initiate a function that allows division by zero:
```{r eval = FALSE , results = "hide" }
dbSendQuery( db , 
	"CREATE FUNCTION 
		div_noerror(l DOUBLE, r DOUBLE) 
	RETURNS DOUBLE 
	EXTERNAL NAME calc.div_noerror" 
)
```

Calculate the distribution of a categorical variable:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		is_sole_proprietor , 
		div_noerror( 
			COUNT(*) , 
			( SELECT COUNT(*) FROM npi ) 
		) AS share_is_sole_proprietor
	FROM npi 
	GROUP BY is_sole_proprietor" 
)
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT SUM( provider_enumeration_year ) FROM npi" )

dbGetQuery( db , 
	"SELECT 
		provider_gender_code , 
		SUM( provider_enumeration_year ) AS sum_provider_enumeration_year 
	FROM npi 
	GROUP BY provider_gender_code" 
)
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT QUANTILE( provider_enumeration_year , 0.5 ) FROM npi" )

dbGetQuery( db , 
	"SELECT 
		provider_gender_code , 
		QUANTILE( provider_enumeration_year , 0.5 ) AS median_provider_enumeration_year
	FROM npi 
	GROUP BY provider_gender_code" 
)
```

### Subsetting {-}

Limit your SQL analysis to California with `WHERE`:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db ,
	"SELECT
		AVG( provider_enumeration_year )
	FROM npi
	WHERE provider_business_practice_location_address_state_name = 'CA'"
)
```

### Measures of Uncertainty {-}

Calculate the variance and standard deviation, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		VAR_SAMP( provider_enumeration_year ) , 
		STDDEV_SAMP( provider_enumeration_year ) 
	FROM npi" 
)

dbGetQuery( db , 
	"SELECT 
		provider_gender_code , 
		VAR_SAMP( provider_enumeration_year ) AS var_provider_enumeration_year ,
		STDDEV_SAMP( provider_enumeration_year ) AS stddev_provider_enumeration_year
	FROM npi 
	GROUP BY provider_gender_code" 
)
```

### Regression Models and Tests of Association {-}

Calculate the correlation between two variables, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		CORR( CAST( individual AS DOUBLE ) , CAST( provider_enumeration_year AS DOUBLE ) )
	FROM npi" 
)

dbGetQuery( db , 
	"SELECT 
		provider_gender_code , 
		CORR( CAST( individual AS DOUBLE ) , CAST( provider_enumeration_year AS DOUBLE ) )
	FROM npi 
	GROUP BY provider_gender_code" 
)
```

## Analysis Examples with `dplyr` {-}

The R `dplyr` library offers an alternative grammar of data manipulation to base R and SQL syntax. [dplyr](https://github.com/tidyverse/dplyr/) offers many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, and the `tidyverse` style of non-standard evaluation. [This vignette](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html) details the available features. As a starting point for NPPES users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(dplyr)
dplyr_db <- MonetDBLite::src_monetdblite( dbdir )
nppes_tbl <- tbl( dplyr_db , 'npi' )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
nppes_tbl %>%
	summarize( mean = mean( provider_enumeration_year ) )

nppes_tbl %>%
	group_by( provider_gender_code ) %>%
	summarize( mean = mean( provider_enumeration_year ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT COUNT(*) FROM npi" )
```

## Database Shutdown {-}

```{r eval = FALSE , results = "hide" }
dbDisconnect( db , shutdown = TRUE )
```

<!--chapter:end:39-nppes.Rmd-->

# National Survey of OAA Participants (NPS) {-}

[![Build Status](https://travis-ci.org/asdfree/nps.svg?branch=master)](https://travis-ci.org/asdfree/nps) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/nps?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/nps)

The National Survey of OAA Participants measures program satisfaction with state agency community services for American seniors.

* One table with one row per sampled senior respondent.

* A complex sample survey designed to generalize to non-institutionalized beneficiaries of Area Agencies on Aging (AAA) within the United States.

* Released annually since 2003.

* Administered by the [U.S. Administration on Aging](http://www.aoa.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available NPS microdata by simply specifying `"nps"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "nps" , output_dir = file.path( path.expand( "~" ) , "NPS" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the NPS catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available NPS microdata files
nps_cat <-
	get_catalog( "nps" ,
		output_dir = file.path( path.expand( "~" ) , "NPS" ) )

# 2015 only 
nps_cat <- subset( nps_cat , year == 2015 )
# download the microdata to your local computer
lodown( "nps" , nps_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
library(survey)

nps_df <- 
	readRDS( 
		file.path( path.expand( "~" ) , "NPS" , 
			"2015 transportation.rds" ) )

nps_design <- 
	svrepdesign( 
		data = nps_df , 
		repweights = "pstotwgt[0-9]" , 
		weights = ~ pstotwgt , 
		type = "Fay" , 
		rho = 0.29986 , 
		mse = TRUE
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
nps_design <- 
	update( 
		nps_design , 
		
		age_category =
			factor( agec , levels = 2:5 , labels =
			c( "60-64" , "65-74" , "75-84" , "85+" ) ) ,
		
		gender = factor( gender , labels = c( "male" , "female" ) ) ,
		
		trip_this_week = as.numeric( trdays %in% 1:2 )

	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( nps_design , "sampling" ) != 0 )

svyby( ~ one , ~ age_category , nps_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , nps_design )

svyby( ~ one , ~ age_category , nps_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ adlaoa6p , nps_design , na.rm = TRUE )

svyby( ~ adlaoa6p , ~ age_category , nps_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ gender , nps_design )

svyby( ~ gender , ~ age_category , nps_design , svymean )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ adlaoa6p , nps_design , na.rm = TRUE )

svyby( ~ adlaoa6p , ~ age_category , nps_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ gender , nps_design )

svyby( ~ gender , ~ age_category , nps_design , svytotal )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ adlaoa6p , nps_design , 0.5 , na.rm = TRUE )

svyby( 
	~ adlaoa6p , 
	~ age_category , 
	nps_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ adlaoa6p , 
	denominator = ~ iadlaoa7 , 
	nps_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to beneficiaries who live alone:
```{r eval = FALSE , results = "hide" }
sub_nps_design <- subset( nps_design , livealone == 1 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ adlaoa6p , sub_nps_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ adlaoa6p , nps_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ adlaoa6p , 
		~ age_category , 
		nps_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( nps_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ adlaoa6p , nps_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ adlaoa6p , nps_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ adlaoa6p , nps_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ trip_this_week , nps_design ,
	method = "likelihood" )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( adlaoa6p ~ trip_this_week , nps_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ trip_this_week + gender , 
	nps_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		adlaoa6p ~ trip_this_week + gender , 
		nps_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for NPS users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
nps_srvyr_design <- as_survey( nps_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
nps_srvyr_design %>%
	summarize( mean = survey_mean( adlaoa6p , na.rm = TRUE ) )

nps_srvyr_design %>%
	group_by( age_category ) %>%
	summarize( mean = survey_mean( adlaoa6p , na.rm = TRUE ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:40-nps.Rmd-->

# National Survey of Children's Health (NSCH) {-}

[![Build Status](https://travis-ci.org/asdfree/nsch.svg?branch=master)](https://travis-ci.org/asdfree/nsch) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/nsch?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/nsch)

*Contributed by Emily Wiegand <<erowewiegand@gmail.com>>*

The National Survey of Children's Health (NSCH) offers state-level estimates of children's health care and the family environment.

* One row per sampled child under eighteen.

* A complex sample survey designed to generalize to non-institutionalized children in the United States at the state-level.

* Released every four or five years since 2003.

* Sponsored by the [Maternal and Child Health Bureau of the Health Resources and Services Administration](http://www.mchb.hrsa.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available NSCH microdata by simply specifying `"nsch"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "nsch" , output_dir = file.path( path.expand( "~" ) , "NSCH" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the NSCH catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available NSCH microdata files
nsch_cat <-
	get_catalog( "nsch" ,
		output_dir = file.path( path.expand( "~" ) , "NSCH" ) )

# 2012 only
nsch_cat <- subset( nsch_cat , year == 2012 )
# download the microdata to your local computer
lodown( "nsch" , nsch_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a multiply-imputed, complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
library(survey)
library(mitools)

nsch_imp <- readRDS( file.path( path.expand( "~" ) , "NSCH" , "2012 main.rds" ) )

nsch_design <- 
	svydesign( 
		id = ~ 1 , 
		strata = ~ state + sample , 
		weights = ~ nschwt , 
		data = imputationList( nsch_imp )
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
nsch_design <-
	update(
		nsch_design ,
		
		indicator_1_3 = ifelse( k6q40 > 1 , NA , k6q40 ) ,

		indicator_5_2 =
			ifelse( k7q05r %in% 1:5 , 1 ,
			ifelse( k7q05r %in% 0 , 0 , NA ) ) ,
			
		indicator_5_3 =
			ifelse( k7q30 == 1 | k7q31 == 1 | k7q32 == 1 , 1 ,
			ifelse( k7q30 == 0 | k7q31 == 0 | k7q32 == 0 , 0 , NA ) ) ,
			
		povcat = 
			factor( 
				findInterval( povlevel_i , c( 1 , 2 , 6 , 8 ) ) ,
				labels = 
					c( "below poverty" , "100-199% fpl" , "200-399% fpl" , "400%+ fpl" )
			) ,
		
		sex = factor( ifelse( sex %in% 1:2 , sex , NA ) , labels = c( "male" , "female" ) )
		
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( nsch_design , svyby( ~ one , ~ one , unwtd.count ) ) )

MIcombine( with( nsch_design , svyby( ~ one , ~ state , unwtd.count ) ) )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( nsch_design , svytotal( ~ one ) ) )

MIcombine( with( nsch_design ,
	svyby( ~ one , ~ state , svytotal )
) )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( nsch_design , svymean( ~ ageyr_child ) ) )

MIcombine( with( nsch_design ,
	svyby( ~ ageyr_child , ~ state , svymean )
) )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( nsch_design , svymean( ~ povcat ) ) )

MIcombine( with( nsch_design ,
	svyby( ~ povcat , ~ state , svymean )
) )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( nsch_design , svytotal( ~ ageyr_child ) ) )

MIcombine( with( nsch_design ,
	svyby( ~ ageyr_child , ~ state , svytotal )
) )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( nsch_design , svytotal( ~ povcat ) ) )

MIcombine( with( nsch_design ,
	svyby( ~ povcat , ~ state , svytotal )
) )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( nsch_design , svyquantile( ~ ageyr_child , 0.5 , se = TRUE ) ) )

MIcombine( with( nsch_design ,
	svyby( 
		~ ageyr_child , ~ state , svyquantile , 0.5 ,
		se = TRUE , keep.var = TRUE , ci = TRUE 
) ) )
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
MIcombine( with( nsch_design ,
	svyratio( numerator = ~ k6q63 , denominator = ~ totkids4 )
) )
```

### Subsetting {-}

Restrict the survey design to only children:
```{r eval = FALSE , results = "hide" }
sub_nsch_design <- subset( nsch_design , agepos4 == 1 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
MIcombine( with( sub_nsch_design , svymean( ~ ageyr_child ) ) )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <-
	MIcombine( with( nsch_design ,
		svymean( ~ ageyr_child )
	) )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	MIcombine( with( nsch_design ,
		svyby( ~ ageyr_child , ~ state , svymean )
	) )

coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( nsch_design$designs[[1]] )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
MIcombine( with( nsch_design , svyvar( ~ ageyr_child ) ) )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
MIcombine( with( nsch_design ,
	svymean( ~ ageyr_child , deff = TRUE )
) )

# SRS with replacement
MIcombine( with( nsch_design ,
	svymean( ~ ageyr_child , deff = "replace" )
) )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
lodown:::MIsvyciprop( ~ indicator_5_2 , nsch_design ,
	method = "likelihood" )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
lodown:::MIsvyttest( ageyr_child ~ indicator_5_2 , nsch_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
lodown:::MIsvychisq( ~ indicator_5_2 + povcat , nsch_design )
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	MIcombine( with( nsch_design ,
		svyglm( ageyr_child ~ indicator_5_2 + povcat )
	) )
	
summary( glm_result )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:41-nsch.Rmd-->

# National Study on Drug Use and Health (NSDUH) {-}

[![Build Status](https://travis-ci.org/asdfree/nsduh.svg?branch=master)](https://travis-ci.org/asdfree/nsduh) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/nsduh?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/nsduh)

The National Study on Drug Use and Health measures the prevalence and correlates of drug use in the United States.

* One table with one row per sampled respondent.

* A complex sample survey designed to generalize to the civilian, noninstitutionalized population of the United States aged 12 and older.

* Released periodically since 1979 and annually since 1990.

* Administered by the [Substance Abuse and Mental Health Services Administration](http://www.samhsa.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available NSDUH microdata by simply specifying `"nsduh"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "nsduh" , output_dir = file.path( path.expand( "~" ) , "NSDUH" ) , 
	your_email = "email@address.com" , 
	your_password = "password" )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the NSDUH catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available NSDUH microdata files
nsduh_cat <-
	get_catalog( "nsduh" ,
		output_dir = file.path( path.expand( "~" ) , "NSDUH" ) , 
		your_email = "email@address.com" , 
		your_password = "password" )

# 2014 only
nsduh_cat <- subset( nsduh_cat , temporalCoverage == 2014 )
# download the microdata to your local computer
lodown( "nsduh" , nsduh_cat , 
	your_email = "email@address.com" , 
	your_password = "password" )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
library(survey)

nsduh_df <- 
	readRDS( file.path( path.expand( "~" ) , "NSDUH" , "2014 main.rds" ) )

nsduh_design <- 
	svydesign( 
		id = ~ verep , 
		strata = ~ vestr , 
		data = nsduh_df , 
		weights = ~ analwt_c , 
		nest = TRUE 
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
nsduh_design <- 
	update( 
		nsduh_design , 
		
		one = 1 ,
		
		health = 
			factor( 
				health , 
				levels = 1:5 , 
				labels = c( "excellent" , "very good" , "good" ,
					"fair" , "poor" )
			) ,
			
		age_tried_first_cigarette = ifelse( cigtry > 99 , NA , cigtry ) ,
		
		age_tried_cocaine = ifelse( cocage > 99 , NA , cocage ) ,

		ever_used_marijuana = as.numeric( mjever == 1 ) ,
		
		county_type =
			factor(
				coutyp2 ,
				levels = 1:3 ,
				labels = c( "large metro" , "small metro" , "nonmetro" )
			)
			
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( nsduh_design , "sampling" ) != 0 )

svyby( ~ one , ~ county_type , nsduh_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , nsduh_design )

svyby( ~ one , ~ county_type , nsduh_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ age_tried_first_cigarette , nsduh_design , na.rm = TRUE )

svyby( ~ age_tried_first_cigarette , ~ county_type , nsduh_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ health , nsduh_design , na.rm = TRUE )

svyby( ~ health , ~ county_type , nsduh_design , svymean , na.rm = TRUE )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ age_tried_first_cigarette , nsduh_design , na.rm = TRUE )

svyby( ~ age_tried_first_cigarette , ~ county_type , nsduh_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ health , nsduh_design , na.rm = TRUE )

svyby( ~ health , ~ county_type , nsduh_design , svytotal , na.rm = TRUE )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ age_tried_first_cigarette , nsduh_design , 0.5 , na.rm = TRUE )

svyby( 
	~ age_tried_first_cigarette , 
	~ county_type , 
	nsduh_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ age_tried_first_cigarette , 
	denominator = ~ age_tried_cocaine , 
	nsduh_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to individuals who are pregnant:
```{r eval = FALSE , results = "hide" }
sub_nsduh_design <- subset( nsduh_design , preg == 1 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ age_tried_first_cigarette , sub_nsduh_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ age_tried_first_cigarette , nsduh_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ age_tried_first_cigarette , 
		~ county_type , 
		nsduh_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( nsduh_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ age_tried_first_cigarette , nsduh_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ age_tried_first_cigarette , nsduh_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ age_tried_first_cigarette , nsduh_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ ever_used_marijuana , nsduh_design ,
	method = "likelihood" , na.rm = TRUE )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( age_tried_first_cigarette ~ ever_used_marijuana , nsduh_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ ever_used_marijuana + health , 
	nsduh_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		age_tried_first_cigarette ~ ever_used_marijuana + health , 
		nsduh_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for NSDUH users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
nsduh_srvyr_design <- as_survey( nsduh_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
nsduh_srvyr_design %>%
	summarize( mean = survey_mean( age_tried_first_cigarette , na.rm = TRUE ) )

nsduh_srvyr_design %>%
	group_by( county_type ) %>%
	summarize( mean = survey_mean( age_tried_first_cigarette , na.rm = TRUE ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:42-nsduh.Rmd-->

# National Survey of Family Growth (NSFG) {-}

[![Build Status](https://travis-ci.org/asdfree/nsfg.svg?branch=master)](https://travis-ci.org/asdfree/nsfg) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/nsfg?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/nsfg)

The National Survey of Family Growth (NSFG) is the principal survey to measure reproductive behavior in the United States population.

* Multiple tables with one row per respondent for the female and male tables, then a separate table with one row per pregnancy.

* A complex sample survey designed to generalize to the 15-44 year old population of the United States, by gender.

* Released every couple of years since 1973.

* Administered by the [Centers for Disease Control and Prevention](http://www.cdc.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available NSFG microdata by simply specifying `"nsfg"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "nsfg" , output_dir = file.path( path.expand( "~" ) , "NSFG" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the NSFG catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available NSFG microdata files
nsfg_cat <-
	get_catalog( "nsfg" ,
		output_dir = file.path( path.expand( "~" ) , "NSFG" ) )

# 2013-2015 only
nsfg_cat <- subset( nsfg_cat , grepl( "2013_2015" , full_url ) )
# download the microdata to your local computer
lodown( "nsfg" , nsfg_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
options( survey.lonely.psu = "adjust" )

library(survey)

nsfg_df <- readRDS( file.path( path.expand( "~" ) , "NSFG" , "2013_2015_FemRespData.rds" ) )

nsfg_design <- 
	svydesign( 
		id = ~ secu , 
		strata = ~ sest , 
		data = nsfg_df , 
		weights = ~ wgt2013_2015 , 
		nest = TRUE 
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
nsfg_design <- 
	update( 
		nsfg_design , 

		one = 1 ,
		
		birth_control_pill = as.numeric( constat1 == 6 ) ,
		
		age_categories = 
			factor( findInterval( ager , c( 15 , 20 , 25 , 30 , 35 , 40 ) ) ,
				labels = c( '15-19' , '20-24' , '25-29' , '30-34' , '35-39' , '40-44' ) ) ,
		
		marstat =
			factor( marstat , levels = c( 1:6 , 8:9 ) ,
				labels = c(
					"Married to a person of the opposite sex" ,
					"Not married but living together with a partner of the opposite sex" ,
					"Widowed" ,
					"Divorced or annulled" ,
					"Separated, because you and your spouse are not getting along" ,
					"Never been married" ,
					"Refused" ,
					"Don't know" )
			)
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( nsfg_design , "sampling" ) != 0 )

svyby( ~ one , ~ age_categories , nsfg_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , nsfg_design )

svyby( ~ one , ~ age_categories , nsfg_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ npregs_s , nsfg_design , na.rm = TRUE )

svyby( ~ npregs_s , ~ age_categories , nsfg_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ marstat , nsfg_design )

svyby( ~ marstat , ~ age_categories , nsfg_design , svymean )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ npregs_s , nsfg_design , na.rm = TRUE )

svyby( ~ npregs_s , ~ age_categories , nsfg_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ marstat , nsfg_design )

svyby( ~ marstat , ~ age_categories , nsfg_design , svytotal )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ npregs_s , nsfg_design , 0.5 , na.rm = TRUE )

svyby( 
	~ npregs_s , 
	~ age_categories , 
	nsfg_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ npregs_s , 
	denominator = ~ nbabes_s , 
	nsfg_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to ever cohabited:
```{r eval = FALSE , results = "hide" }
sub_nsfg_design <- subset( nsfg_design , timescoh > 0 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ npregs_s , sub_nsfg_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ npregs_s , nsfg_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ npregs_s , 
		~ age_categories , 
		nsfg_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( nsfg_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ npregs_s , nsfg_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ npregs_s , nsfg_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ npregs_s , nsfg_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ birth_control_pill , nsfg_design ,
	method = "likelihood" , na.rm = TRUE )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( npregs_s ~ birth_control_pill , nsfg_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ birth_control_pill + marstat , 
	nsfg_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		npregs_s ~ birth_control_pill + marstat , 
		nsfg_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for NSFG users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
nsfg_srvyr_design <- as_survey( nsfg_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
nsfg_srvyr_design %>%
	summarize( mean = survey_mean( npregs_s , na.rm = TRUE ) )

nsfg_srvyr_design %>%
	group_by( age_categories ) %>%
	summarize( mean = survey_mean( npregs_s , na.rm = TRUE ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:43-nsfg.Rmd-->

# National Plan and Provider Enumeration System (NVSS) {-}

The National Plan and Provider Enumeration System (NPPES) contains information about every medical provider, insurance plan, and clearinghouse actively operating in the United States healthcare industry.

* A single large table with one row per enumerated health care provider.

* A census of individuals and organizations who bill for medical services in the United States.

* Updated monthly with new providers.

* Maintained by the United States [Centers for Medicare & Medicaid Services (CMS)](http://www.cms.gov/)

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available NVSS microdata by simply specifying `"nvss"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "nvss" , output_dir = file.path( path.expand( "~" ) , "NVSS" ) )
```

## Analysis Examples with SQL and `MonetDBLite` {-}

Connect to a database:

```{r eval = FALSE }
library(DBI)
dbdir <- file.path( path.expand( "~" ) , "NVSS" , "MonetDB" )
db <- dbConnect( MonetDBLite::MonetDBLite() , dbdir )
```

```{r eval = FALSE }

```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
dbSendQuery( db , "ALTER TABLE npi ADD COLUMN individual INTEGER" )

dbSendQuery( db , 
	"UPDATE npi 
	SET individual = 
		CASE WHEN entity_type_code = 1 THEN 1 ELSE 0 END" 
)

dbSendQuery( db , "ALTER TABLE npi ADD COLUMN provider_enumeration_year INTEGER" )

dbSendQuery( db , 
	"UPDATE npi 
	SET provider_enumeration_year = 
		CAST( SUBSTRING( provider_enumeration_date , 7 , 10 ) AS INTEGER )" 
)
```

### Unweighted Counts {-}

Count the unweighted number of records in the SQL table, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT COUNT(*) FROM npi" )

dbGetQuery( db ,
	"SELECT
		provider_gender_code ,
		COUNT(*) 
	FROM npi
	GROUP BY provider_gender_code"
)
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT AVG( provider_enumeration_year ) FROM npi" )

dbGetQuery( db , 
	"SELECT 
		provider_gender_code , 
		AVG( provider_enumeration_year ) AS mean_provider_enumeration_year
	FROM npi 
	GROUP BY provider_gender_code" 
)
```

Initiate a function that allows division by zero:
```{r eval = FALSE , results = "hide" }
dbSendQuery( db , 
	"CREATE FUNCTION 
		div_noerror(l DOUBLE, r DOUBLE) 
	RETURNS DOUBLE 
	EXTERNAL NAME calc.div_noerror" 
)
```

Calculate the distribution of a categorical variable:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		is_sole_proprietor , 
		div_noerror( 
			COUNT(*) , 
			( SELECT COUNT(*) FROM npi ) 
		) AS share_is_sole_proprietor
	FROM npi 
	GROUP BY is_sole_proprietor" 
)
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT SUM( provider_enumeration_year ) FROM npi" )

dbGetQuery( db , 
	"SELECT 
		provider_gender_code , 
		SUM( provider_enumeration_year ) AS sum_provider_enumeration_year 
	FROM npi 
	GROUP BY provider_gender_code" 
)
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT QUANTILE( provider_enumeration_year , 0.5 ) FROM npi" )

dbGetQuery( db , 
	"SELECT 
		provider_gender_code , 
		QUANTILE( provider_enumeration_year , 0.5 ) AS median_provider_enumeration_year
	FROM npi 
	GROUP BY provider_gender_code" 
)
```

### Subsetting {-}

Limit your SQL analysis to California with `WHERE`:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db ,
	"SELECT
		AVG( provider_enumeration_year )
	FROM npi
	WHERE provider_business_practice_location_address_state_name = 'CA'"
)
```

### Measures of Uncertainty {-}

Calculate the variance and standard deviation, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		VAR_SAMP( provider_enumeration_year ) , 
		STDDEV_SAMP( provider_enumeration_year ) 
	FROM npi" 
)

dbGetQuery( db , 
	"SELECT 
		provider_gender_code , 
		VAR_SAMP( provider_enumeration_year ) AS var_provider_enumeration_year ,
		STDDEV_SAMP( provider_enumeration_year ) AS stddev_provider_enumeration_year
	FROM npi 
	GROUP BY provider_gender_code" 
)
```

### Regression Models and Tests of Association {-}

Calculate the correlation between two variables, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		CORR( CAST( individual AS DOUBLE ) , CAST( provider_enumeration_year AS DOUBLE ) )
	FROM npi" 
)

dbGetQuery( db , 
	"SELECT 
		provider_gender_code , 
		CORR( CAST( individual AS DOUBLE ) , CAST( provider_enumeration_year AS DOUBLE ) )
	FROM npi 
	GROUP BY provider_gender_code" 
)
```

## Analysis Examples with `dplyr` {-}

The R `dplyr` library offers an alternative grammar of data manipulation to base R and SQL syntax. [dplyr](https://github.com/tidyverse/dplyr/) offers many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, and the `tidyverse` style of non-standard evaluation. [This vignette](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html) details the available features. As a starting point for NVSS users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(dplyr)
dplyr_db <- MonetDBLite::src_monetdblite( dbdir )
nvss_tbl <- tbl( dplyr_db , 'npi' )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
nvss_tbl %>%
	summarize( mean = mean( provider_enumeration_year ) )

nvss_tbl %>%
	group_by( provider_gender_code ) %>%
	summarize( mean = mean( provider_enumeration_year ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT COUNT(*) FROM npi" )
```

## Database Shutdown {-}

```{r eval = FALSE , results = "hide" }
dbDisconnect( db , shutdown = TRUE )
```

<!--chapter:end:44-nvss.Rmd-->

# New York City Housing and Vacancy Survey (NYCHVS) {-}

[![Build Status](https://travis-ci.org/asdfree/nychvs.svg?branch=master)](https://travis-ci.org/asdfree/nychvs) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/nychvs?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/nychvs)

The New York City Housing and Vacancy Survey (NYCHVS) covers the city-wide rental vacancy rate and other characteristics like neighborhood housing stock.

* One table with one record per occupied housing unit, a second table with one record per person inside each occupied housing unit, and a third table with one record per unoccupied housing unit.

* A complex sample survey designed to generalize to all occupied and unoccupied housing units in the five boroughs of New York City.

* Released triennially since 1998.

* Funded by the [New York City Department of Housing Preservation and Development](www.nyc.gov/hpd) and conducted by the [United States Census Bureau](http://www.census.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available NYCHVS microdata by simply specifying `"nychvs"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "nychvs" , output_dir = file.path( path.expand( "~" ) , "NYCHVS" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the NYCHVS catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available NYCHVS microdata files
nychvs_cat <-
	get_catalog( "nychvs" ,
		output_dir = file.path( path.expand( "~" ) , "NYCHVS" ) )

# 2014 only
nychvs_cat <- subset( nychvs_cat , year == 2014 )
# download the microdata to your local computer
lodown( "nychvs" , nychvs_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
options( survey.lonely.psu = "adjust" )

library(survey)

# load the occupied units table
nychvs_df <- readRDS( file.path( path.expand( "~" ) , "NYCHVS" , "2014/occ.rds" ) )

nychvs_design <- 
	svydesign( ~ 1 , data = nychvs_df , weights = ~ fw )
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
nychvs_design <- 
	update( 
		nychvs_design , 
		
		one = 1 ,
		
		home_owners = as.numeric( sc115 == 1 ) ,

		yearly_household_income = ifelse( uf42 == 9999999 , 0 , as.numeric( uf42 ) ) ,
		
		gross_monthly_rent = ifelse( uf17 == 99999 , NA , as.numeric( uf17 ) ) ,
		
		borough =
			factor( boro , levels = 1:5 , labels =
				c( 'Bronx' , 'Brooklyn' , 'Manhattan' , 
				'Queens' , 'Staten Island' )
			) ,
			
		householder_sex = factor( hhr2 , labels = c( 'male' , 'female' ) )
			
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( nychvs_design , "sampling" ) != 0 )

svyby( ~ one , ~ borough , nychvs_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , nychvs_design )

svyby( ~ one , ~ borough , nychvs_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ yearly_household_income , nychvs_design , na.rm = TRUE )

svyby( ~ yearly_household_income , ~ borough , nychvs_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ householder_sex , nychvs_design )

svyby( ~ householder_sex , ~ borough , nychvs_design , svymean )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ yearly_household_income , nychvs_design , na.rm = TRUE )

svyby( ~ yearly_household_income , ~ borough , nychvs_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ householder_sex , nychvs_design )

svyby( ~ householder_sex , ~ borough , nychvs_design , svytotal )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ yearly_household_income , nychvs_design , 0.5 , na.rm = TRUE )

svyby( 
	~ yearly_household_income , 
	~ borough , 
	nychvs_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ gross_monthly_rent , 
	denominator = ~ yearly_household_income , 
	nychvs_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to Manhattan:
```{r eval = FALSE , results = "hide" }
sub_nychvs_design <- subset( nychvs_design , boro == 3 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ yearly_household_income , sub_nychvs_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ yearly_household_income , nychvs_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ yearly_household_income , 
		~ borough , 
		nychvs_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( nychvs_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ yearly_household_income , nychvs_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ yearly_household_income , nychvs_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ yearly_household_income , nychvs_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ home_owners , nychvs_design ,
	method = "likelihood" )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( yearly_household_income ~ home_owners , nychvs_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ home_owners + householder_sex , 
	nychvs_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		yearly_household_income ~ home_owners + householder_sex , 
		nychvs_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for NYCHVS users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
nychvs_srvyr_design <- as_survey( nychvs_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
nychvs_srvyr_design %>%
	summarize( mean = survey_mean( yearly_household_income , na.rm = TRUE ) )

nychvs_srvyr_design %>%
	group_by( borough ) %>%
	summarize( mean = survey_mean( yearly_household_income , na.rm = TRUE ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:45-nychvs.Rmd-->

# Pew Research Center (PEW) {-}

[![Build Status](https://travis-ci.org/asdfree/pew.svg?branch=master)](https://travis-ci.org/asdfree/pew) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/pew?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/pew)

The Pew Research Center releases its survey microdata on U.S. Politics & Policy, Journalism & Media, Internet, Science & Tech, Religion & Public Life, Hispanic Trends, Global Attitudes & Trends, and Social & Demographic Trends.

* Generally one table per survey, with one row per sampled respondent.

* Complex sample surveys, often designed to generalize to the U.S. adult population or the adult populations of the nations surveyed.

* Administered by the [Pew Research Center](http://www.pewresearch.org/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available PEW microdata by simply specifying `"pew"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "pew" , output_dir = file.path( path.expand( "~" ) , "PEW" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the PEW catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available PEW microdata files
pew_cat <-
	get_catalog( "pew" ,
		output_dir = file.path( path.expand( "~" ) , "PEW" ) )

# spring 2015 only
pew_cat <- subset( pew_cat , name == "Spring 2015 Survey Data" )
# download the microdata to your local computer
lodown( "pew" , pew_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
options( survey.lonely.psu = "adjust" )

library(survey)

pew_df <- 
	readRDS( 
		file.path( path.expand( "~" ) , "PEW" , 
		"Global Attitudes & Trends/2015/Spring 2015 Survey Data" ,
		"Pew Research Global Attitudes Spring 2015 Dataset for Web FINAL.rds" )
	)

# limit the global attitudes data set to just israel
israel_df <- subset( pew_df , country == 14 )
	
pew_design <- 
	svydesign(
		id = ~ psu , 
		strata = ~ stratum , 
		weight = ~ weight , 
		data = israel_df 
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
pew_design <- 
	update( 
		pew_design , 
		
		one = 1 ,
		
		your_day_today =
			factor( 
				q1 , 
				levels = 1:3 ,
				labels = 
					c( 
						'a typical day' , 
						'a particularly good day' , 
						'a particularly bad day' 
					)
			) ,

		school_years = ifelse( q163b %in% 98:99 , NA , q163b ) ,
		
		age_in_years = ifelse( q146 %in% 98:99 , NA , q146 ) ,

		climate_change_concern = ifelse( q13a %in% 1:5 , as.numeric( q13a < 3 ) , NA ) ,
		
		country_economic_situation =
			factor(
				q3 ,
				levels = 1:4 ,
				labels = 
					c( 
						'very good' , 
						'somewhat good' , 
						'somewhat bad' , 
						'very bad' 
					)
			)
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( pew_design , "sampling" ) != 0 )

svyby( ~ one , ~ your_day_today , pew_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , pew_design )

svyby( ~ one , ~ your_day_today , pew_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ school_years , pew_design , na.rm = TRUE )

svyby( ~ school_years , ~ your_day_today , pew_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ country_economic_situation , pew_design , na.rm = TRUE )

svyby( ~ country_economic_situation , ~ your_day_today , pew_design , svymean , na.rm = TRUE )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ school_years , pew_design , na.rm = TRUE )

svyby( ~ school_years , ~ your_day_today , pew_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ country_economic_situation , pew_design , na.rm = TRUE )

svyby( ~ country_economic_situation , ~ your_day_today , pew_design , svytotal , na.rm = TRUE )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ school_years , pew_design , 0.5 , na.rm = TRUE )

svyby( 
	~ school_years , 
	~ your_day_today , 
	pew_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ school_years , 
	denominator = ~ age_in_years , 
	pew_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to seniors:
```{r eval = FALSE , results = "hide" }
sub_pew_design <- subset( pew_design , q146 >= 65 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ school_years , sub_pew_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ school_years , pew_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ school_years , 
		~ your_day_today , 
		pew_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( pew_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ school_years , pew_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ school_years , pew_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ school_years , pew_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ climate_change_concern , pew_design ,
	method = "likelihood" , na.rm = TRUE )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( school_years ~ climate_change_concern , pew_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ climate_change_concern + country_economic_situation , 
	pew_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		school_years ~ climate_change_concern + country_economic_situation , 
		pew_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for PEW users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
pew_srvyr_design <- as_survey( pew_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
pew_srvyr_design %>%
	summarize( mean = survey_mean( school_years , na.rm = TRUE ) )

pew_srvyr_design %>%
	group_by( your_day_today ) %>%
	summarize( mean = survey_mean( school_years , na.rm = TRUE ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:46-pew.Rmd-->

# Programme for the International Assessment of Adult Competencies (PIAAC) {-}

[![Build Status](https://travis-ci.org/asdfree/piaac.svg?branch=master)](https://travis-ci.org/asdfree/piaac) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/piaac?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/piaac)

The Programme for the International Assessment of Adult Competencies (PIAAC) offers cross-national comparisons for the serious study of advanced-nation labor markets.

* One row per sampled adult.

* A multiply-imputed, complex sample survey designed to generalize to the population aged 16 to 65 across thirty three OECD nations.

* No expected release timeline.

* Administered by the [Organisation for Economic Co-operation and Development](http://www.oecd.org/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available PIAAC microdata by simply specifying `"piaac"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "piaac" , output_dir = file.path( path.expand( "~" ) , "PIAAC" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the PIAAC catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available PIAAC microdata files
piaac_cat <-
	get_catalog( "piaac" ,
		output_dir = file.path( path.expand( "~" ) , "PIAAC" ) )

# download the microdata to your local computer
lodown( "piaac" , piaac_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a multiply-imputed, complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
library(survey)
library(mitools)

piaac_design <- readRDS( file.path( path.expand( "~" ) , "PIAAC" , "prgusap1 design.rds" ) )
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
piaac_design <-
	update(
		piaac_design ,
		
		one = 1 ,
		
		sex = factor( gender_r , labels = c( "male" , "female" ) ) ,

		age_categories = factor( ageg10lfs , levels = 1:5 , labels = c( "24 or less" , "25-34" , "35-44" , "45-54" , "55 plus" ) ) ,
		
		working_at_paid_job_last_week = as.numeric( c_q01a == 1 )
		
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( piaac_design , svyby( ~ one , ~ one , unwtd.count ) ) )

MIcombine( with( piaac_design , svyby( ~ one , ~ age_categories , unwtd.count ) ) )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( piaac_design , svytotal( ~ one ) ) )

MIcombine( with( piaac_design ,
	svyby( ~ one , ~ age_categories , svytotal )
) )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( piaac_design , svymean( ~ pvnum , na.rm = TRUE ) ) )

MIcombine( with( piaac_design ,
	svyby( ~ pvnum , ~ age_categories , svymean , na.rm = TRUE )
) )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( piaac_design , svymean( ~ sex ) ) )

MIcombine( with( piaac_design ,
	svyby( ~ sex , ~ age_categories , svymean )
) )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( piaac_design , svytotal( ~ pvnum , na.rm = TRUE ) ) )

MIcombine( with( piaac_design ,
	svyby( ~ pvnum , ~ age_categories , svytotal , na.rm = TRUE )
) )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( piaac_design , svytotal( ~ sex ) ) )

MIcombine( with( piaac_design ,
	svyby( ~ sex , ~ age_categories , svytotal )
) )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( piaac_design , svyquantile( ~ pvnum , 0.5 , se = TRUE , na.rm = TRUE ) ) )

MIcombine( with( piaac_design ,
	svyby( 
		~ pvnum , ~ age_categories , svyquantile , 0.5 ,
		se = TRUE , keep.var = TRUE , ci = TRUE , na.rm = TRUE
) ) )
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
MIcombine( with( piaac_design ,
	svyratio( numerator = ~ pvnum , denominator = ~ pvlit , na.rm = TRUE )
) )
```

### Subsetting {-}

Restrict the survey design to self-reported fair or poor health:
```{r eval = FALSE , results = "hide" }
sub_piaac_design <- subset( piaac_design , i_q08 %in% 4:5 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
MIcombine( with( sub_piaac_design , svymean( ~ pvnum , na.rm = TRUE ) ) )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <-
	MIcombine( with( piaac_design ,
		svymean( ~ pvnum , na.rm = TRUE )
	) )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	MIcombine( with( piaac_design ,
		svyby( ~ pvnum , ~ age_categories , svymean , na.rm = TRUE )
	) )

coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( piaac_design$designs[[1]] )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
MIcombine( with( piaac_design , svyvar( ~ pvnum , na.rm = TRUE ) ) )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
MIcombine( with( piaac_design ,
	svymean( ~ pvnum , na.rm = TRUE , deff = TRUE )
) )

# SRS with replacement
MIcombine( with( piaac_design ,
	svymean( ~ pvnum , na.rm = TRUE , deff = "replace" )
) )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
lodown:::MIsvyciprop( ~ working_at_paid_job_last_week , piaac_design ,
	method = "likelihood" )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
lodown:::MIsvyttest( pvnum ~ working_at_paid_job_last_week , piaac_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
lodown:::MIsvychisq( ~ working_at_paid_job_last_week + sex , piaac_design )
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	MIcombine( with( piaac_design ,
		svyglm( pvnum ~ working_at_paid_job_last_week + sex )
	) )
	
summary( glm_result )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:47-piaac.Rmd-->

# Progress in International Reading Literacy Study (PIRLS) {-}

[![Build Status](https://travis-ci.org/asdfree/pirls.svg?branch=master)](https://travis-ci.org/asdfree/pirls) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/pirls?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/pirls)

The Progress in International Reading Literacy Study (PIRLS) tracks the reading competency of fourth graders across about fifty nations.

* A series of tables with one record per school (ACG), per student (ASG), per teacher (ATG), as well as files containing student achievement (ASA), home background (ASH), student-teacher linkage (AST), and within-country scoring reliability (ASR).

* A complex sample survey designed to generalize to the fourth-grade student population of participating countries.

* Released quinquennially since 2001.

* Funded by the [International Association for the Evaluation of Educational Achievement](https://www.iea.nl/) and compiled by the [Lynch School of Education at Boston College](http://www.bc.edu/bc-web/schools/lsoe.html).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available PIRLS microdata by simply specifying `"pirls"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "pirls" , output_dir = file.path( path.expand( "~" ) , "PIRLS" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the PIRLS catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available PIRLS microdata files
pirls_cat <-
	get_catalog( "pirls" ,
		output_dir = file.path( path.expand( "~" ) , "PIRLS" ) )

# 2011 only
pirls_cat <- subset( pirls_cat , year == 2011 )
# download the microdata to your local computer
lodown( "pirls" , pirls_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a multiply-imputed, complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
library(survey)
library(mitools)

# load the ASG (student background) + ASH (home background) merged design
pirls_design <- readRDS( file.path( path.expand( "~" ) , "PIRLS" , "2011/asg_design.rds" ) )

# optional step to limit memory usage
variables_to_keep <-
	c( 'idcntry' , 'itsex' , 'itbirthy' , 'asrrea' , 'asrlit' )
	
pirls_design$designs <-
	lapply( 
		pirls_design$designs ,
		function( w ) {
			w$variables <- w$variables[ variables_to_keep ]
			w
		}
	)

gc()

```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
pirls_design <- 
	update( 
		pirls_design , 
		
		one = 1 ,
		
		idcntry = factor( idcntry ) ,
		
		sex = factor( itsex , labels = c( "male" , "female" ) ) ,
		
		born_2001_or_later = as.numeric( itbirthy >= 2001 )

	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::pirls_MIcombine( with( pirls_design , svyby( ~ one , ~ one , unwtd.count ) ) )

lodown:::pirls_MIcombine( with( pirls_design , svyby( ~ one , ~ idcntry , unwtd.count ) ) )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::pirls_MIcombine( with( pirls_design , svytotal( ~ one ) ) )

lodown:::pirls_MIcombine( with( pirls_design ,
	svyby( ~ one , ~ idcntry , svytotal )
) )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::pirls_MIcombine( with( pirls_design , svymean( ~ asrrea ) ) )

lodown:::pirls_MIcombine( with( pirls_design ,
	svyby( ~ asrrea , ~ idcntry , svymean )
) )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::pirls_MIcombine( with( pirls_design , svymean( ~ sex , na.rm = TRUE ) ) )

lodown:::pirls_MIcombine( with( pirls_design ,
	svyby( ~ sex , ~ idcntry , svymean , na.rm = TRUE )
) )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::pirls_MIcombine( with( pirls_design , svytotal( ~ asrrea ) ) )

lodown:::pirls_MIcombine( with( pirls_design ,
	svyby( ~ asrrea , ~ idcntry , svytotal )
) )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::pirls_MIcombine( with( pirls_design , svytotal( ~ sex , na.rm = TRUE ) ) )

lodown:::pirls_MIcombine( with( pirls_design ,
	svyby( ~ sex , ~ idcntry , svytotal , na.rm = TRUE )
) )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::pirls_MIcombine( with( pirls_design , svyquantile( ~ asrrea , 0.5 , se = TRUE ) ) )

lodown:::pirls_MIcombine( with( pirls_design ,
	svyby( 
		~ asrrea , ~ idcntry , svyquantile , 0.5 ,
		se = TRUE , keep.var = TRUE , ci = TRUE 
) ) )
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
lodown:::pirls_MIcombine( with( pirls_design ,
	svyratio( numerator = ~ asrlit , denominator = ~ asrrea )
) )
```

### Subsetting {-}

Restrict the survey design to Australia, Austria, Azerbaijan, Belgium (French):
```{r eval = FALSE , results = "hide" }
sub_pirls_design <- subset( pirls_design , idcntry %in% c( 36 , 40 , 31 , 957 ) )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
lodown:::pirls_MIcombine( with( sub_pirls_design , svymean( ~ asrrea ) ) )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <-
	lodown:::pirls_MIcombine( with( pirls_design ,
		svymean( ~ asrrea )
	) )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	lodown:::pirls_MIcombine( with( pirls_design ,
		svyby( ~ asrrea , ~ idcntry , svymean )
	) )

coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( pirls_design$designs[[1]] )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
lodown:::pirls_MIcombine( with( pirls_design , svyvar( ~ asrrea ) ) )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
lodown:::pirls_MIcombine( with( pirls_design ,
	svymean( ~ asrrea , deff = TRUE )
) )

# SRS with replacement
lodown:::pirls_MIcombine( with( pirls_design ,
	svymean( ~ asrrea , deff = "replace" )
) )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
lodown:::MIsvyciprop( ~ born_2001_or_later , pirls_design ,
	method = "likelihood" , na.rm = TRUE )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
lodown:::MIsvyttest( asrrea ~ born_2001_or_later , pirls_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
lodown:::MIsvychisq( ~ born_2001_or_later + sex , pirls_design )
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	lodown:::pirls_MIcombine( with( pirls_design ,
		svyglm( asrrea ~ born_2001_or_later + sex )
	) )
	
summary( glm_result )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:48-pirls.Rmd-->

# Program for International Student Assessment (PISA) {-}

The authoritative source for evaluating educational achievement across nations, the Program(me) for International Student Assessment ranks the math, science, and reading skills of high school students across the developed world.

* A large table with one row per student, a smaller table with one row per school, then multiple (optional) tables such as one row per parent or per teacher.

* A complex sample survey designed to generalize to 15-year-old schoolchildren in more than sixty countries.

* Released triennially since 2000.

* Administered by the [OECD](www.oecd.org/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available PISA microdata by simply specifying `"pisa"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "pisa" , output_dir = file.path( path.expand( "~" ) , "PISA" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the PISA catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available PISA microdata files
pisa_cat <-
	get_catalog( "pisa" ,
		output_dir = file.path( path.expand( "~" ) , "PISA" ) )

# 2015 only
pisa_cat <- subset( pisa_cat , year == 2015 )
# download the microdata to your local computer
lodown( "pisa" , pisa_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a multiply-imputed, database-backed complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
library(DBI)
library(MonetDBLite)
library(survey)
library(mitools)

pisa_design <- readRDS( file.path( path.expand( "~" ) , "PISA" , "2015 cmb_stu_qqq design.rds" ) )

pisa_design <- lodown:::svyMDBdesign( pisa_design )
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
pisa_design <- 
	update( 
		pisa_design , 
		
		gender = factor( st004d01t , labels = c( "male" , "female" ) ) ,
		
		how_many_computers_at_home = 
			factor( 
				st012q06na , 
				labels = c( "none" , "one" , "two" , "three or more" ) 
			)
 
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( pisa_design , svyby( ~ one , ~ one , unwtd.count ) ) )

MIcombine( with( pisa_design , svyby( ~ one , ~ gender , unwtd.count ) ) )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( pisa_design , svytotal( ~ one ) ) )

MIcombine( with( pisa_design ,
	svyby( ~ one , ~ gender , svytotal )
) )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( pisa_design , svymean( ~ scie ) ) )

MIcombine( with( pisa_design ,
	svyby( ~ scie , ~ gender , svymean )
) )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( pisa_design , svymean( ~ how_many_computers_at_home ) ) )

MIcombine( with( pisa_design ,
	svyby( ~ how_many_computers_at_home , ~ gender , svymean )
) )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( pisa_design , svytotal( ~ scie ) ) )

MIcombine( with( pisa_design ,
	svyby( ~ scie , ~ gender , svytotal )
) )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( pisa_design , svytotal( ~ how_many_computers_at_home ) ) )

MIcombine( with( pisa_design ,
	svyby( ~ how_many_computers_at_home , ~ gender , svytotal )
) )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
MIcombine( with( pisa_design , svyquantile( ~ scie , 0.5 , se = TRUE ) ) )

MIcombine( with( pisa_design ,
	svyby( 
		~ scie , ~ gender , svyquantile , 0.5 ,
		se = TRUE , keep.var = TRUE , ci = TRUE 
) ) )
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
MIcombine( with( pisa_design ,
	svyratio( numerator = ~ math , denominator = ~ reading )
) )
```

### Subsetting {-}

Restrict the survey design to Albania:
```{r eval = FALSE , results = "hide" }
sub_pisa_design <- subset( pisa_design , cnt == "ALB" )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
MIcombine( with( sub_pisa_design , svymean( ~ scie ) ) )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <-
	MIcombine( with( pisa_design ,
		svymean( ~ scie )
	) )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	MIcombine( with( pisa_design ,
		svyby( ~ scie , ~ gender , svymean )
	) )

coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( pisa_design$designs[[1]] )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
MIcombine( with( pisa_design , svyvar( ~ scie ) ) )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
MIcombine( with( pisa_design ,
	svymean( ~ scie , deff = TRUE )
) )

# SRS with replacement
MIcombine( with( pisa_design ,
	svymean( ~ scie , deff = "replace" )
) )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
lodown:::MIsvyciprop( ~ oecd , pisa_design ,
	method = "likelihood" )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
lodown:::MIsvyttest( scie ~ oecd , pisa_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
lodown:::MIsvychisq( ~ oecd + how_many_computers_at_home , pisa_design )
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	MIcombine( with( pisa_design ,
		svyglm( scie ~ oecd + how_many_computers_at_home )
	) )
	
summary( glm_result )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```

## Database Shutdown {-}

```{r eval = FALSE , results = "hide" }
close( pisa_design , shutdown = TRUE )
```

<!--chapter:end:49-pisa.Rmd-->

# Public Libraries Survey (PLS) {-}

[![Build Status](https://travis-ci.org/asdfree/pls.svg?branch=master)](https://travis-ci.org/asdfree/pls) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/pls?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/pls)

An annual census of public libraries in the United States.

* One table with one row per state, a second table with one row per library system, and a third table with one row per library building or bookmobile.

* Released annually since 1992.

* Conducted by the [Institute of Museum and Library Services (IMLS)](https://www.imls.gov/) and collected by the [US Census Bureau](http://www.census.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available PLS microdata by simply specifying `"pls"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "pls" , output_dir = file.path( path.expand( "~" ) , "PLS" ) )
```

## Analysis Examples with base R {-}

Load a data frame:

```{r eval = FALSE }
pls_df <- readRDS( file.path( path.expand( "~" ) , "PLS" , "2014/pls_fy_ae_puplda.rds" ) )
```

```{r eval = FALSE }

```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
pls_df <- 
	transform( 
		pls_df , 
		
		c_relatn = 
			factor( c_relatn , levels = c( "HQ" , "ME" , "NO" ) ,
				c( "HQ-Headquarters of a federation or cooperative" ,
				"ME-Member of a federation or cooperative" ,
				"NO-Not a member of a federation or cooperative" )
			) ,
			
		more_than_one_librarian = as.numeric( libraria > 1 )
				
	)	
```

### Unweighted Counts {-}

Count the unweighted number of records in the table, overall and by groups:
```{r eval = FALSE , results = "hide" }
nrow( pls_df )

table( pls_df[ , "stabr" ] , useNA = "always" )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
mean( pls_df[ , "popu_lsa" ] )

tapply(
	pls_df[ , "popu_lsa" ] ,
	pls_df[ , "stabr" ] ,
	mean 
)
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
prop.table( table( pls_df[ , "c_relatn" ] ) )

prop.table(
	table( pls_df[ , c( "c_relatn" , "stabr" ) ] ) ,
	margin = 2
)
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( pls_df[ , "popu_lsa" ] )

tapply(
	pls_df[ , "popu_lsa" ] ,
	pls_df[ , "stabr" ] ,
	sum 
)
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
quantile( pls_df[ , "popu_lsa" ] , 0.5 )

tapply(
	pls_df[ , "popu_lsa" ] ,
	pls_df[ , "stabr" ] ,
	quantile ,
	0.5 
)
```

### Subsetting {-}

Limit your `data.frame` to more than one million annual visits:
```{r eval = FALSE , results = "hide" }
sub_pls_df <- subset( pls_df , visits > 1000000 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
mean( sub_pls_df[ , "popu_lsa" ] )
```

### Measures of Uncertainty {-}

Calculate the variance, overall and by groups:
```{r eval = FALSE , results = "hide" }
var( pls_df[ , "popu_lsa" ] )

tapply(
	pls_df[ , "popu_lsa" ] ,
	pls_df[ , "stabr" ] ,
	var 
)
```

### Regression Models and Tests of Association {-}

Perform a t-test:
```{r eval = FALSE , results = "hide" }
t.test( popu_lsa ~ more_than_one_librarian , pls_df )
```

Perform a chi-squared test of association:
```{r eval = FALSE , results = "hide" }
this_table <- table( pls_df[ , c( "more_than_one_librarian" , "c_relatn" ) ] )

chisq.test( this_table )
```

Perform a generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	glm( 
		popu_lsa ~ more_than_one_librarian + c_relatn , 
		data = pls_df
	)

summary( glm_result )
```

## Analysis Examples with `dplyr` {-}

The R `dplyr` library offers an alternative grammar of data manipulation to base R and SQL syntax. [dplyr](https://github.com/tidyverse/dplyr/) offers many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, and the `tidyverse` style of non-standard evaluation. [This vignette](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html) details the available features. As a starting point for PLS users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(dplyr)
pls_tbl <- tbl_df( pls_df )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
pls_tbl %>%
	summarize( mean = mean( popu_lsa ) )

pls_tbl %>%
	group_by( stabr ) %>%
	summarize( mean = mean( popu_lsa ) )
```



<!--chapter:end:50-pls.Rmd-->

# Pesquisa Mensal de Emprego (PME) {-}

[![Build Status](https://travis-ci.org/asdfree/pme.svg?branch=master)](https://travis-ci.org/asdfree/pme) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/pme?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/pme)

*Contributed by Dr. Djalma Pessoa <<pessoad@gmail.com>>*

The Pesquisa Mensal de Emprego (PME) is the monthly labor force survey covering the six largest Brazilian cities.

* One table with one row per individual within each sampled household.

* A complex sample survey designed to generalize to the civilian population of Brazil's six largest cities.

* Released monthly since March 2002.

* Administered by the [Instituto Brasileiro de Geografia e Estatistica](http://www.ibge.gov.br/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available PME microdata by simply specifying `"pme"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "pme" , output_dir = file.path( path.expand( "~" ) , "PME" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the PME catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available PME microdata files
pme_cat <-
	get_catalog( "pme" ,
		output_dir = file.path( path.expand( "~" ) , "PME" ) )

# 2016 only
pme_cat <- subset( pme_cat , year == 2016 )
# download the microdata to your local computer
lodown( "pme" , pme_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
options( survey.lonely.psu = "adjust" )

library(survey)

pme_df <- readRDS( file.path( path.expand( "~" ) , "PME" , "pme 2016 01.rds" ) )

# throw out records missing their cluster variable
pme_df <- subset( pme_df , !is.na( v113 ) )

pop_totals <- unique( pme_df[ , c( 'v035' , 'v114' ) ] )

prestratified_design <- 
	svydesign( 
		~ v113 , 
		strata = ~ v112 , 
		data = pme_df ,
		weights = ~ v211 , 
		nest = TRUE
	)

pme_design <- 
	postStratify( prestratified_design , ~ v035 , pop_totals )
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
pme_design <- 
	update( 
		pme_design , 

		one = 1 ,
		
		# calculate whether each person is at least ten years of age
		pia = as.numeric( v234 >= 10 ) ,

		# determine individuals who are employed
		ocup_c = as.numeric( v401 == 1 | v402 == 1 | v403 == 1 ) ,
		
		sexo = factor( v203 , labels = c( "male" , "female" ) ) ,
		
		region = 
			factor( 
				v035 , 
				levels = c( 26 , 29 , 31 , 33 , 35 , 43 ) , 
				labels = c( "Recife" , "Salvador" , "Belo Horizonte" , 
					"Rio de Janeiro" , "Sao Paulo" , "Porto Alegre" )
			)
	)
	
pme_design <-
	update(
		pme_design ,
		
		# determine individuals who are unemployed
		desocup30 = as.numeric( ocup_c == 0 & !is.na( v461 ) & v465 == 1 )
	)
		
pme_design <-
	update(
		pme_design ,
		
		# determine individuals who are either working or not working
		pea_c = as.numeric( ocup_c == 1 | desocup30 == 1 )

	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( pme_design , "sampling" ) != 0 )

svyby( ~ one , ~ region , pme_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , pme_design )

svyby( ~ one , ~ region , pme_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ vd25 , pme_design , na.rm = TRUE )

svyby( ~ vd25 , ~ region , pme_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ sexo , pme_design )

svyby( ~ sexo , ~ region , pme_design , svymean )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ vd25 , pme_design , na.rm = TRUE )

svyby( ~ vd25 , ~ region , pme_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ sexo , pme_design )

svyby( ~ sexo , ~ region , pme_design , svytotal )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ vd25 , pme_design , 0.5 , na.rm = TRUE )

svyby( 
	~ vd25 , 
	~ region , 
	pme_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ desocup30 , 
	denominator = ~ pea_c , 
	pme_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to teenagers:
```{r eval = FALSE , results = "hide" }
sub_pme_design <- subset( pme_design , v234 %in% 13:19 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ vd25 , sub_pme_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ vd25 , pme_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ vd25 , 
		~ region , 
		pme_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( pme_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ vd25 , pme_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ vd25 , pme_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ vd25 , pme_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ ocup_c , pme_design ,
	method = "likelihood" , na.rm = TRUE )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( vd25 ~ ocup_c , pme_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ ocup_c + sexo , 
	pme_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		vd25 ~ ocup_c + sexo , 
		pme_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for PME users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
pme_srvyr_design <- as_survey( pme_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
pme_srvyr_design %>%
	summarize( mean = survey_mean( vd25 , na.rm = TRUE ) )

pme_srvyr_design %>%
	group_by( region ) %>%
	summarize( mean = survey_mean( vd25 , na.rm = TRUE ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:51-pme.Rmd-->

# Pesquisa Nacional por Amostra de Domicilios (PNAD) {-}

[![Build Status](https://travis-ci.org/asdfree/pnad.svg?branch=master)](https://travis-ci.org/asdfree/pnad) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/pnad?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/pnad)

*Contributed by Dr. Djalma Pessoa <<pessoad@gmail.com>>*

Brazil's previous principal household survey, the Pesquisa Nacional por Amostra de Domicilios (PNAD) measures general education, labor, income, and housing characteristics of the population.

* One table with one row per sampled household and a second table with one row per individual within each sampled household.

* A complex sample survey designed to generalize to the civilian non-institutional population of Brazil, although the rural north was not included prior to 2004.

* Released annually since 2001 except for years ending in zero, when the decennial census takes its place.

* Administered by the [Instituto Brasileiro de Geografia e Estatistica](http://www.ibge.gov.br/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available PNAD microdata by simply specifying `"pnad"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "pnad" , output_dir = file.path( path.expand( "~" ) , "PNAD" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the PNAD catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available PNAD microdata files
pnad_cat <-
	get_catalog( "pnad" ,
		output_dir = file.path( path.expand( "~" ) , "PNAD" ) )

# 2011 only
pnad_cat <- subset( pnad_cat , year == 2011 )
# download the microdata to your local computer
lodown( "pnad" , pnad_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
options( survey.lonely.psu = "adjust" )

library(survey)

pnad_df <- readRDS( pnad_cat[ 1 , 'output_filename' ] )

pop_types <- 
	data.frame( 
		v4609 = unique( pnad_df$v4609 ) , 
		Freq = unique( pnad_df$v4609 )
	)

prestratified_design <-
	svydesign(
		id = ~ v4618 ,
		strata = ~ v4617 ,
		data = pnad_df ,
		weights = ~ pre_wgt ,
		nest = TRUE
	)
	
rm( pnad_df ) ; gc()

pnad_design <- 
	postStratify( 
		design = prestratified_design ,
		strata = ~ v4609 ,
		population = pop_types
	)
	
rm( prestratified_design ) ; gc()
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
pnad_design <- 
	update( 
		pnad_design , 
		age_categories = factor( 1 + findInterval( v8005 , seq( 5 , 60 , 5 ) ) ) ,
		male = as.numeric( v0302 == 2 ) ,
		teenagers = as.numeric( v8005 > 12 & v8005 < 20 ) ,
		started_working_before_thirteen = as.numeric( v9892 < 13 )
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( pnad_design , "sampling" ) != 0 )

svyby( ~ one , ~ region , pnad_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , pnad_design )

svyby( ~ one , ~ region , pnad_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ v4720 , pnad_design , na.rm = TRUE )

svyby( ~ v4720 , ~ region , pnad_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ age_categories , pnad_design )

svyby( ~ age_categories , ~ region , pnad_design , svymean )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ v4720 , pnad_design , na.rm = TRUE )

svyby( ~ v4720 , ~ region , pnad_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ age_categories , pnad_design )

svyby( ~ age_categories , ~ region , pnad_design , svytotal )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ v4720 , pnad_design , 0.5 , na.rm = TRUE )

svyby( 
	~ v4720 , 
	~ region , 
	pnad_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ started_working_before_thirteen , 
	denominator = ~ teenagers , 
	pnad_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to married persons:
```{r eval = FALSE , results = "hide" }
sub_pnad_design <- subset( pnad_design , v4011 == 1 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ v4720 , sub_pnad_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ v4720 , pnad_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ v4720 , 
		~ region , 
		pnad_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( pnad_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ v4720 , pnad_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ v4720 , pnad_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ v4720 , pnad_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ male , pnad_design ,
	method = "likelihood" )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( v4720 ~ male , pnad_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ male + age_categories , 
	pnad_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		v4720 ~ male + age_categories , 
		pnad_design 
	)

summary( glm_result )
```

## Poverty and Inequality Estimation with `convey` {-}

The R `convey` library estimates measures of income concentration, poverty, inequality, and wellbeing. [This textbook](https://guilhermejacob.github.io/context/) details the available features. As a starting point for PNAD users, this code calculates the gini coefficient on complex sample survey data:

```{r eval = FALSE , results = "hide" }
library(convey)
pnad_design <- convey_prep( pnad_design )

sub_pnad_design <- 
	subset( 
		pnad_design , 
		!is.na( v4720 ) & v4720 != 0 & v8005 >= 15
	)

svygini( ~ v4720 , sub_pnad_design , na.rm = TRUE )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }
svytotal( ~one , pnad_design )
svytotal( ~factor( v0302 ) , pnad_design )
cv( svytotal( ~factor( v0302 ) , pnad_design ) )
```



<!--chapter:end:52-pnad.Rmd-->

# Pesquisa Nacional por Amostra de Domicilios - Continua (PNADC) {-}

[![Build Status](https://travis-ci.org/asdfree/pnadc.svg?branch=master)](https://travis-ci.org/asdfree/pnadc) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/pnadc?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/pnadc)

*Contributed by Dr. Djalma Pessoa <<pessoad@gmail.com>>*

Brazil's principal household survey, the Pesquisa Nacional por Amostra de Domicilios Continua (PNADC) measures general education, labor, income, and housing characteristics of the population.

* One table with one row per sampled household and a second table with one row per individual within each sampled household.

* A complex sample survey designed to generalize to the civilian non-institutional population of Brazil.

* Released quarterly since 2012.

* Administered by the [Instituto Brasileiro de Geografia e Estatistica](http://www.ibge.gov.br/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available PNADC microdata by simply specifying `"pnadc"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "pnadc" , output_dir = file.path( path.expand( "~" ) , "PNADC" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the PNADC catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available PNADC microdata files
pnadc_cat <-
	get_catalog( "pnadc" ,
		output_dir = file.path( path.expand( "~" ) , "PNADC" ) )

# 2015 3rd quarter only
pnadc_cat <- subset( pnadc_cat , year == 2015 & quarter == '03' )
# download the microdata to your local computer
lodown( "pnadc" , pnadc_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
library(survey)

options( survey.lonely.psu = "adjust" )

pnadc_df <- readRDS( file.path( path.expand( "~" ) , "PNADC" , "pnadc 2015 03.rds" ) )

# add a column of all ones
pnadc_df$one <- 1

# construct a data.frame object with all state names.
uf <-
 structure(list(V1 = c(11L, 12L, 13L, 14L, 15L, 16L, 17L, 21L, 
 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 31L, 32L, 33L, 35L, 41L, 
 42L, 43L, 50L, 51L, 52L, 53L), V2 = structure(c(22L, 1L, 4L, 
 23L, 14L, 3L, 27L, 10L, 18L, 6L, 20L, 15L, 17L, 2L, 26L, 5L, 
 13L, 8L, 19L, 25L, 16L, 24L, 21L, 12L, 11L, 9L, 7L), .Label = c("Acre", 
 "Alagoas", "Amapa", "Amazonas", "Bahia", "Ceara", "Distrito Federal", 
 "Espirito Santo", "Goias", "Maranhao", "Mato Grosso", "Mato Grosso do Sul", 
 "Minas Gerais", "Para", "Paraiba", "Parana", "Pernambuco", "Piaui", 
 "Rio de Janeiro", "Rio Grande do Norte", "Rio Grande do Sul", 
 "Rondonia", "Roraima", "Santa Catarina", "Sao Paulo", "Sergipe", 
 "Tocantins"), class = "factor")), .Names = c("uf", "uf_name"), 
		class = "data.frame", row.names = c(NA, -27L))

# merge this data.frame onto the main `x` data.frame
# using `uf` as the merge field, keeping all non-matches.
pnadc_df <- merge( pnadc_df , uf , all.x = TRUE )

# confirm complete matches
stopifnot( all( !is.na( pnadc_df$uf_name ) ) )

# preliminary survey design
pre_stratified <-
	svydesign(
		ids = ~ upa , 
		strata = ~ estrato , 
		weights = ~ v1027 , 
		data = pnadc_df ,
		nest = TRUE
	)
# warning: do not use `pre_stratified` in your analyses!
# you must use the `pnadc_design` object created below.

# post-stratification targets
df_pos <- 
	data.frame( posest = unique( pnadc_df$posest ) , Freq = unique( pnadc_df$v1029 ) )

# final survey design object
pnadc_design <- postStratify( pre_stratified , ~ posest , df_pos )

# remove the `pnadc_df` data.frame object
# and the `pre_stratified` design before stratification
rm( pnadc_df , pre_stratified )
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
pnadc_design <- 
	update( 
		pnadc_design , 
		age_categories = factor( 1 + findInterval( v2009 , seq( 5 , 60 , 5 ) ) ) ,
		male = as.numeric( v2007 == 1 ) ,
		pia = as.numeric( v2009 >= 14 ) ,
		region = substr( uf , 1 , 1 )
	)
	
pnadc_design <- 
	update( 
		pnadc_design , 
		ocup_c = ifelse( pia == 1 , as.numeric( vd4002 %in% 1 ) , NA ) ,
		desocup30 = ifelse( pia == 1 , as.numeric( vd4002 %in% 2 ) , NA ) ,
		# calculate usual income from main job
		# (rendimento habitual do trabalho principal)
		vd4016n = ifelse( pia %in% 1 & vd4015 %in% 1 , vd4016 , NA ) ,
		# calculate effective income from main job
		# (rendimento efetivo do trabalho principal) 
		vd4017n = ifelse( pia %in% 1 & vd4015 %in% 1 , vd4017 , NA ) ,
		# calculate usual income from all jobs
		# (variavel rendimento habitual de todos os trabalhos)
		vd4019n = ifelse( pia %in% 1 & vd4015 %in% 1 , vd4019 , NA ) ,
		# calculate effective income from all jobs
		# (rendimento efetivo do todos os trabalhos) 
		vd4020n = ifelse( pia %in% 1 & vd4015 %in% 1 , vd4020 , NA ) ,
		# determine individuals who are either working or not working
		# (that is, the potential labor force)
		pea_c = as.numeric( ocup_c == 1 | desocup30 == 1 )
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( pnadc_design , "sampling" ) != 0 )

svyby( ~ one , ~ uf_name , pnadc_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , pnadc_design )

svyby( ~ one , ~ uf_name , pnadc_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ vd4020n , pnadc_design , na.rm = TRUE )

svyby( ~ vd4020n , ~ uf_name , pnadc_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ age_categories , pnadc_design )

svyby( ~ age_categories , ~ uf_name , pnadc_design , svymean )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ vd4020n , pnadc_design , na.rm = TRUE )

svyby( ~ vd4020n , ~ uf_name , pnadc_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ age_categories , pnadc_design )

svyby( ~ age_categories , ~ uf_name , pnadc_design , svytotal )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ vd4020n , pnadc_design , 0.5 , na.rm = TRUE )

svyby( 
	~ vd4020n , 
	~ uf_name , 
	pnadc_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ ocup_c , 
	denominator = ~ pea_c , 
	pnadc_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to unemployed persons in the labor force:
```{r eval = FALSE , results = "hide" }
sub_pnadc_design <- subset( pnadc_design , desocup30 == 1 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ vd4020n , sub_pnadc_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ vd4020n , pnadc_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ vd4020n , 
		~ uf_name , 
		pnadc_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( pnadc_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ vd4020n , pnadc_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ vd4020n , pnadc_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ vd4020n , pnadc_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ male , pnadc_design ,
	method = "likelihood" )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( vd4020n ~ male , pnadc_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ male + age_categories , 
	pnadc_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		vd4020n ~ male + age_categories , 
		pnadc_design 
	)

summary( glm_result )
```

## Poverty and Inequality Estimation with `convey` {-}

The R `convey` library estimates measures of income concentration, poverty, inequality, and wellbeing. [This textbook](https://guilhermejacob.github.io/context/) details the available features. As a starting point for PNADC users, this code calculates the gini coefficient on complex sample survey data:

```{r eval = FALSE , results = "hide" }
library(convey)
pnadc_design <- convey_prep( pnadc_design )

sub_pnadc_design <- 
	subset( pnadc_design , pia == 1 )

svygini( ~ vd4020n , sub_pnadc_design , na.rm = TRUE )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }
nationwide_pop <- 
	svytotal( ~ pia , pnadc_design , na.rm = TRUE )
nationwide_forca <- 
	svytotal( ~ factor( vd4001 ) , pnadc_design , na.rm = TRUE )
nationwide_ocupacao <- 
	svytotal( ~ factor( vd4002 ) , pnadc_design , na.rm = TRUE )
regional_pop <- 
	svyby( ~ pia , ~ region , pnadc_design , svytotal , na.rm = TRUE )
regional_forca <- 
	svyby( ~ factor( vd4001 ) , ~ region , pnadc_design , svytotal , na.rm = TRUE )
regional_ocupacao <- 
	svyby( ~ factor( vd4002 ) , ~ region , pnadc_design , svytotal , na.rm = TRUE )
```



<!--chapter:end:53-pnadc.Rmd-->

# Pesquisa Nacional de Saude (PNS) {-}

[![Build Status](https://travis-ci.org/asdfree/pns.svg?branch=master)](https://travis-ci.org/asdfree/pns) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/pns?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/pns)

*Contributed by Dr. Djalma Pessoa <<pessoad@gmail.com>>*

The Pesquisa Nacional de Saude (PNS) is Brazil's healthcare survey.

* One table with one row per long-questionnaire respondent and a second table with one row for all respondents.

* A complex sample survey designed to generalize to Brazil's civilian population.

* First released 2013.

* Administered by the [Instituto Brasileiro de Geografia e Estatistica](http://www.ibge.gov.br/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available PNS microdata by simply specifying `"pns"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "pns" , output_dir = file.path( path.expand( "~" ) , "PNS" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the PNS catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available PNS microdata files
pns_cat <-
	get_catalog( "pns" ,
		output_dir = file.path( path.expand( "~" ) , "PNS" ) )

# download the microdata to your local computer
lodown( "pns" , pns_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
options( survey.lonely.psu = "adjust" )

library(survey)

pns_design <- 
	readRDS( 
		file.path( 
			path.expand( "~" ) , "PNS" , 
			"2013 long questionnaire survey design.rds" ) 
		)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
pns_design <- 
	update( 
		pns_design , 

		one = 1 ,
		
		health_insurance = as.numeric( i001 == 1 )
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( pns_design , "sampling" ) != 0 )

svyby( ~ one , ~ uf , pns_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , pns_design )

svyby( ~ one , ~ uf , pns_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ w00101 , pns_design , na.rm = TRUE )

svyby( ~ w00101 , ~ uf , pns_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ c006 , pns_design )

svyby( ~ c006 , ~ uf , pns_design , svymean )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ w00101 , pns_design , na.rm = TRUE )

svyby( ~ w00101 , ~ uf , pns_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ c006 , pns_design )

svyby( ~ c006 , ~ uf , pns_design , svytotal )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ w00101 , pns_design , 0.5 , na.rm = TRUE )

svyby( 
	~ w00101 , 
	~ uf , 
	pns_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ w00203 , 
	denominator = ~ w00101 , 
	pns_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to at least 30 minutes of physical activity:
```{r eval = FALSE , results = "hide" }
sub_pns_design <- subset( pns_design , atfi04 == 1 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ w00101 , sub_pns_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ w00101 , pns_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ w00101 , 
		~ uf , 
		pns_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( pns_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ w00101 , pns_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ w00101 , pns_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ w00101 , pns_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ health_insurance , pns_design ,
	method = "likelihood" , na.rm = TRUE )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( w00101 ~ health_insurance , pns_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ health_insurance + c006 , 
	pns_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		w00101 ~ health_insurance + c006 , 
		pns_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for PNS users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
pns_srvyr_design <- as_survey( pns_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
pns_srvyr_design %>%
	summarize( mean = survey_mean( w00101 , na.rm = TRUE ) )

pns_srvyr_design %>%
	group_by( uf ) %>%
	summarize( mean = survey_mean( w00101 , na.rm = TRUE ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:54-pns.Rmd-->

# Pesquisa de Orcamentos Familiares (POF) {-}

[![Build Status](https://travis-ci.org/asdfree/pof.svg?branch=master)](https://travis-ci.org/asdfree/pof) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/pof?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/pof)

*Contributed by Dr. Djalma Pessoa <<pessoad@gmail.com>>*

The Pesquisa de Orcamentos Familiares is Brazil's national survey of household budgets.

* One table of survey responses per sampled household. Additional tables, many containing one record per expenditure.

* A complex sample survey designed to generalize to the civilian population of Brazil.

* Released at irregular intervals, with only 2002-2003 and 2008-2009 microdata available.

* Administered by the [Instituto Brasileiro de Geografia e Estatistica](http://www.ibge.gov.br/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available POF microdata by simply specifying `"pof"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "pof" , output_dir = file.path( path.expand( "~" ) , "POF" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the POF catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available POF microdata files
pof_cat <-
	get_catalog( "pof" ,
		output_dir = file.path( path.expand( "~" ) , "POF" ) )

# 2008-2009 only
pof_cat <- subset( pof_cat , period == "2008_2009" )
# download the microdata to your local computer
lodown( "pof" , pof_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
options( survey.lonely.psu = "adjust" )

library(survey)

poststr <- 
	readRDS( 
		file.path( path.expand( "~" ) , "POF" , 
			"2008_2009/poststr.rds" ) 
		)

		
t_morador_s <- 
	readRDS( 
		file.path( path.expand( "~" ) , "POF" , 
			"2008_2009/t_morador_s.rds" ) 
		)

t_morador_s <-
	transform(
		t_morador_s , 
		control = paste0( cod_uf , num_seq , num_dv ) 
	)
	
pof_df <- merge( t_morador_s , poststr )

stopifnot( nrow( pof_df ) == nrow( t_morador_s ) )

pre_stratified_design <- 
	svydesign(
		id = ~control , 
		strata = ~estrato_unico ,
		weights = ~fator_expansao1 ,
		data = pof_df ,
		nest = TRUE
	)

population_totals <- 
	data.frame(
		pos_estrato = unique( pof_df$pos_estrato ) , 
		Freq = unique( pof_df$tot_pop ) 
	)

pof_design <-
	postStratify(
		pre_stratified_design , 
		~ pos_estrato , 
		population_totals
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
pof_design <- 
	update(
		pof_design , 
		
		one = 1 ,
		
		# centimeters instead of meters
		altura_imputado = altura_imputado / 100 ,
		
		age_categories =
			factor( 
				1 + findInterval( idade_anos , 
					c( 20 , 25 , 30 , 35 , 45 , 55 , 65 , 75 ) ) ,
				levels = 1:9 , labels = c( "under 20" , "20-24" , "25-29" ,
				"30-34" , "35-44" , "45-54" , "55-64" , "65-74" , "75+" )
			) ,
		
		# create a body mass index (bmi) variable, excluding babies (who have altura_imputado==0)			
		body_mass_index = ifelse( altura_imputado == 0 , 0 , peso_imputado / ( altura_imputado ^ 2 ) ) ,
		
		sexo = ifelse( cod_sexo == '01' , "masculino" , ifelse( cod_sexo == '02' , "feminino" , NA ) )
		
		
	)

pof_design <-
	transform(
		pof_design ,
		
		# individuals with a low bmi - underweight
		underweight = ifelse( body_mass_index < 18.5 , 1 , 0 ) ,
		
		# individuals with a high bmi - overweight
		overweight = ifelse( body_mass_index >= 25 , 1 , 0 ) ,
		
		# individuals with a very high bmi - obese
		obese = ifelse( body_mass_index >= 30 , 1 , 0 )
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( pof_design , "sampling" ) != 0 )

svyby( ~ one , ~ sexo , pof_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , pof_design )

svyby( ~ one , ~ sexo , pof_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ body_mass_index , pof_design , na.rm = TRUE )

svyby( ~ body_mass_index , ~ sexo , pof_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ age_categories , pof_design )

svyby( ~ age_categories , ~ sexo , pof_design , svymean )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ body_mass_index , pof_design , na.rm = TRUE )

svyby( ~ body_mass_index , ~ sexo , pof_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ age_categories , pof_design )

svyby( ~ age_categories , ~ sexo , pof_design , svytotal )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ body_mass_index , pof_design , 0.5 , na.rm = TRUE )

svyby( 
	~ body_mass_index , 
	~ sexo , 
	pof_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ peso_imputado , 
	denominator = ~ altura_imputado , 
	pof_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to :
```{r eval = FALSE , results = "hide" }
sub_pof_design <- subset( pof_design , underweight == 1 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ body_mass_index , sub_pof_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ body_mass_index , pof_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ body_mass_index , 
		~ sexo , 
		pof_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( pof_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ body_mass_index , pof_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ body_mass_index , pof_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ body_mass_index , pof_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ obese , pof_design ,
	method = "likelihood" , na.rm = TRUE )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( body_mass_index ~ obese , pof_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ obese + age_categories , 
	pof_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		body_mass_index ~ obese + age_categories , 
		pof_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for POF users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
pof_srvyr_design <- as_survey( pof_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
pof_srvyr_design %>%
	summarize( mean = survey_mean( body_mass_index , na.rm = TRUE ) )

pof_srvyr_design %>%
	group_by( sexo ) %>%
	summarize( mean = survey_mean( body_mass_index , na.rm = TRUE ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:55-pof.Rmd-->

# Panel Study of Income Dynamics (PSID) {-}

[![Build Status](https://travis-ci.org/asdfree/psid.svg?branch=master)](https://travis-ci.org/asdfree/psid) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/psid?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/psid)

The Panel Study of Income Dynamics is the longest running longitudinal household survey in the world.

* One cross-year individual with one record per respondent in participating household, many family data tables with one record per family per timepoint.

* A complex sample survey designed to generalize to residents of the United States.

* Released either annually or biennially since 1968.

* Administered by the [University of Michigan's Institute for Social Research](https://www.isr.umich.edu/home/) and funded by [consortium](https://psidonline.isr.umich.edu/Guide/Sponsorship.aspx).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available PSID microdata by simply specifying `"psid"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "psid" , output_dir = file.path( path.expand( "~" ) , "PSID" ) , 
	your_email = "email@address.com" , 
	your_password = "password" )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the PSID catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available PSID microdata files
psid_cat <-
	get_catalog( "psid" ,
		output_dir = file.path( path.expand( "~" ) , "PSID" ) , 
		your_email = "email@address.com" , 
		your_password = "password" )

# download the microdata to your local computer
lodown( "psid" , psid_cat , 
	your_email = "email@address.com" , 
	your_password = "password" )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
options( survey.lonely.psu = "adjust" )

library(survey)

# identify the cross-year individual filename
cross_year_individual_rds <- 
	grep( 
		"cross-year individual" ,
		list.files( 
			file.path( path.expand( "~" ) , "PSID" ) , 
			recursive = TRUE , 
			full.names = TRUE 
		) ,
		value = TRUE
	)

individual_df <- readRDS( cross_year_individual_rds )

ind_variables_to_keep <-
	c( 
		'one' ,			# column with all ones
		'er30001' , 	# 1968 interview number
		'er30002' , 	# 1968 person number
		'er31997' ,		# primary sampling unit variable
		'er31996' ,		# stratification variable
		'er33802' ,		# sequence number, 2005
		'er34302' , 	# sequence number, 2015
		'er32000' ,		# sex
		'er34305' ,		# age in 2015
		'er33813' ,		# employment status in 2005
		'er34317' ,		# employment status in 2015
		'er33848' ,		# 2005 longitudinal weight
		'er34413'		# 2015 longitudinal weight
	)

	

individual_df <- individual_df[ ind_variables_to_keep ] ; gc()

family_2005_df <- 
	readRDS( file.path( path.expand( "~" ) , "PSID" , "family files/2005.rds" ) )

fam_2005_variables_to_keep <- 
	c( 
		'er25002' ,	# 2005 interview number
		'er28037' 	# 2005 total family income
		
	)

family_2005_df <- family_2005_df[ fam_2005_variables_to_keep ] ; gc()

family_2015_df <- 
	readRDS( file.path( path.expand( "~" ) , "PSID" , "family files/2015.rds" ) )

fam_2015_variables_to_keep <-
	c( 
		'er60002' ,	# 2015 interview number
		'er65349' 	# 2015 total family income
	)

family_2015_df <- family_2015_df[ fam_2015_variables_to_keep ] ; gc()

ind_fam_2005 <- 
	merge( 
		individual_df , 
		family_2005_df , 
		by.x = 'er33802' , 
		by.y = 'er25002' 
	)

ind_fam_2015 <- 
	merge( 
		individual_df , 
		family_2015_df , 
		by.x = 'er34302' , 
		by.y = 'er60002' 
	)

psid_df <- merge( ind_fam_2005 , ind_fam_2015 , all = TRUE )

psid_design <- 
	svydesign( 
		~ er31997 , 
		strata = ~ er31996 , 
		data = psid_df , 
		weights = ~ er33848 , 
		nest = TRUE 
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
psid_design <- 
	update( 
		psid_design , 
		
		employment_2005 =
			factor( er33813 , levels = 1:8 ,
				labels = c( 'working now' , 'only temporarily laid off' ,
				'looking for work, unemployed' , 'retired' , 'permanently disabled' ,
				'housewife; keeping house' , 'student' , 'other' )
			) ,
			
		employed_in_2015 = 
			factor( er34317 , levels = 1:8 ,
				labels = c( 'working now' , 'only temporarily laid off' ,
				'looking for work, unemployed' , 'retired' , 'permanently disabled' ,
				'housewife; keeping house' , 'student' , 'other' )
			) ,
			
		female = as.numeric( er32000 == 2 )

	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( psid_design , "sampling" ) != 0 )

svyby( ~ one , ~ employment_2005 , psid_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , psid_design )

svyby( ~ one , ~ employment_2005 , psid_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ er28037 , psid_design , na.rm = TRUE )

svyby( ~ er28037 , ~ employment_2005 , psid_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ employed_in_2015 , psid_design , na.rm = TRUE )

svyby( ~ employed_in_2015 , ~ employment_2005 , psid_design , svymean , na.rm = TRUE )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ er28037 , psid_design , na.rm = TRUE )

svyby( ~ er28037 , ~ employment_2005 , psid_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ employed_in_2015 , psid_design , na.rm = TRUE )

svyby( ~ employed_in_2015 , ~ employment_2005 , psid_design , svytotal , na.rm = TRUE )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ er28037 , psid_design , 0.5 , na.rm = TRUE )

svyby( 
	~ er28037 , 
	~ employment_2005 , 
	psid_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ er28037 , 
	denominator = ~ er65349 , 
	psid_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to senior in 2015:
```{r eval = FALSE , results = "hide" }
sub_psid_design <- subset( psid_design , er34305 >= 65 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ er28037 , sub_psid_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ er28037 , psid_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ er28037 , 
		~ employment_2005 , 
		psid_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( psid_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ er28037 , psid_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ er28037 , psid_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ er28037 , psid_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ female , psid_design ,
	method = "likelihood" )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( er28037 ~ female , psid_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ female + employed_in_2015 , 
	psid_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		er28037 ~ female + employed_in_2015 , 
		psid_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for PSID users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
psid_srvyr_design <- as_survey( psid_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
psid_srvyr_design %>%
	summarize( mean = survey_mean( er28037 , na.rm = TRUE ) )

psid_srvyr_design %>%
	group_by( employment_2005 ) %>%
	summarize( mean = survey_mean( er28037 , na.rm = TRUE ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:56-psid.Rmd-->

# National Plan and Provider Enumeration System (SAEB) {-}

The National Plan and Provider Enumeration System (NPPES) contains information about every medical provider, insurance plan, and clearinghouse actively operating in the United States healthcare industry.

* A single large table with one row per enumerated health care provider.

* A census of individuals and organizations who bill for medical services in the United States.

* Updated monthly with new providers.

* Maintained by the United States [Centers for Medicare & Medicaid Services (CMS)](http://www.cms.gov/)

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available SAEB microdata by simply specifying `"saeb"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "saeb" , output_dir = file.path( path.expand( "~" ) , "SAEB" ) )
```

## Analysis Examples with SQL and `MonetDBLite` {-}

Connect to a database:

```{r eval = FALSE }
library(DBI)
dbdir <- file.path( path.expand( "~" ) , "SAEB" , "MonetDB" )
db <- dbConnect( MonetDBLite::MonetDBLite() , dbdir )
```

```{r eval = FALSE }

```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
dbSendQuery( db , "ALTER TABLE npi ADD COLUMN individual INTEGER" )

dbSendQuery( db , 
	"UPDATE npi 
	SET individual = 
		CASE WHEN entity_type_code = 1 THEN 1 ELSE 0 END" 
)

dbSendQuery( db , "ALTER TABLE npi ADD COLUMN provider_enumeration_year INTEGER" )

dbSendQuery( db , 
	"UPDATE npi 
	SET provider_enumeration_year = 
		CAST( SUBSTRING( provider_enumeration_date , 7 , 10 ) AS INTEGER )" 
)
```

### Unweighted Counts {-}

Count the unweighted number of records in the SQL table, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT COUNT(*) FROM npi" )

dbGetQuery( db ,
	"SELECT
		provider_gender_code ,
		COUNT(*) 
	FROM npi
	GROUP BY provider_gender_code"
)
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT AVG( provider_enumeration_year ) FROM npi" )

dbGetQuery( db , 
	"SELECT 
		provider_gender_code , 
		AVG( provider_enumeration_year ) AS mean_provider_enumeration_year
	FROM npi 
	GROUP BY provider_gender_code" 
)
```

Initiate a function that allows division by zero:
```{r eval = FALSE , results = "hide" }
dbSendQuery( db , 
	"CREATE FUNCTION 
		div_noerror(l DOUBLE, r DOUBLE) 
	RETURNS DOUBLE 
	EXTERNAL NAME calc.div_noerror" 
)
```

Calculate the distribution of a categorical variable:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		is_sole_proprietor , 
		div_noerror( 
			COUNT(*) , 
			( SELECT COUNT(*) FROM npi ) 
		) AS share_is_sole_proprietor
	FROM npi 
	GROUP BY is_sole_proprietor" 
)
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT SUM( provider_enumeration_year ) FROM npi" )

dbGetQuery( db , 
	"SELECT 
		provider_gender_code , 
		SUM( provider_enumeration_year ) AS sum_provider_enumeration_year 
	FROM npi 
	GROUP BY provider_gender_code" 
)
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT QUANTILE( provider_enumeration_year , 0.5 ) FROM npi" )

dbGetQuery( db , 
	"SELECT 
		provider_gender_code , 
		QUANTILE( provider_enumeration_year , 0.5 ) AS median_provider_enumeration_year
	FROM npi 
	GROUP BY provider_gender_code" 
)
```

### Subsetting {-}

Limit your SQL analysis to California with `WHERE`:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db ,
	"SELECT
		AVG( provider_enumeration_year )
	FROM npi
	WHERE provider_business_practice_location_address_state_name = 'CA'"
)
```

### Measures of Uncertainty {-}

Calculate the variance and standard deviation, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		VAR_SAMP( provider_enumeration_year ) , 
		STDDEV_SAMP( provider_enumeration_year ) 
	FROM npi" 
)

dbGetQuery( db , 
	"SELECT 
		provider_gender_code , 
		VAR_SAMP( provider_enumeration_year ) AS var_provider_enumeration_year ,
		STDDEV_SAMP( provider_enumeration_year ) AS stddev_provider_enumeration_year
	FROM npi 
	GROUP BY provider_gender_code" 
)
```

### Regression Models and Tests of Association {-}

Calculate the correlation between two variables, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		CORR( CAST( individual AS DOUBLE ) , CAST( provider_enumeration_year AS DOUBLE ) )
	FROM npi" 
)

dbGetQuery( db , 
	"SELECT 
		provider_gender_code , 
		CORR( CAST( individual AS DOUBLE ) , CAST( provider_enumeration_year AS DOUBLE ) )
	FROM npi 
	GROUP BY provider_gender_code" 
)
```

## Analysis Examples with `dplyr` {-}

The R `dplyr` library offers an alternative grammar of data manipulation to base R and SQL syntax. [dplyr](https://github.com/tidyverse/dplyr/) offers many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, and the `tidyverse` style of non-standard evaluation. [This vignette](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html) details the available features. As a starting point for SAEB users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(dplyr)
dplyr_db <- MonetDBLite::src_monetdblite( dbdir )
saeb_tbl <- tbl( dplyr_db , 'npi' )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
saeb_tbl %>%
	summarize( mean = mean( provider_enumeration_year ) )

saeb_tbl %>%
	group_by( provider_gender_code ) %>%
	summarize( mean = mean( provider_enumeration_year ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT COUNT(*) FROM npi" )
```

## Database Shutdown {-}

```{r eval = FALSE , results = "hide" }
dbDisconnect( db , shutdown = TRUE )
```

<!--chapter:end:57-saeb.Rmd-->

# Survey of Business Owners (SBO) {-}

[![Build Status](https://travis-ci.org/asdfree/sbo.svg?branch=master)](https://travis-ci.org/asdfree/sbo) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/sbo?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/sbo)

The Survey of Business Owners tracks nearly every tax-filing sole proprietorship, partnership, and corporation in the nation.

* One table with one row per firm per state per industry.

* A complex sample survey designed to generalize to all firms in the United States, however the public use microdata only includes [classifiable (non-identifiable) firms](https://www2.census.gov/econ/sbo/07/pums/2007_sbo_pums_users_guide.pdf#page=17) which comprise nearly all businesses but only about half of workers.

* Released as part of the U.S. Census Bureau's Economic Census, every year ending in 2 or 7.

* Administered by the [U.S. Census Bureau](http://www.census.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available SBO microdata by simply specifying `"sbo"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "sbo" , output_dir = file.path( path.expand( "~" ) , "SBO" ) )
```

## Analysis Examples with the `survey` library {-}

Construct a multiply-imputed, complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
gc()

options( survey.lonely.psu = "adjust" )

library(survey)
library(mitools)

sbo_design <- 
	readRDS( file.path( path.expand( "~" ) , "SBO" , "2007 main.rds" ) )
	
# keep only the variables you need
variables_to_keep <- 
	c( 
		"one" , 
		"newwgt" , 
		"tabwgt" , 
		"receipts_noisy" ,
		"employment_noisy" ,
		"n07_employer" ,
		"established" ,
		"healthins" ,
		"husbwife"
	)

# keep only columns used in this analysis
sbo_design$coef$variables <-
	sbo_design$coef$variables[ variables_to_keep ]
	
sbo_design$var <-
	lapply( 
		sbo_design$var , 
		function( w ){
			w$variables <- w$variables[ variables_to_keep ]
			w
		}
	)
	
gc()
# this step conserves RAM
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
sbo_design <- 
	lodown:::sbo_update( 
		sbo_design , 
		established_before_2000 =
			ifelse( established %in% c( '0' , 'A' ) , NA , as.numeric( established < 4 ) ) ,
			
		healthins =
			factor( healthins , levels = 1:2 ,
				labels = c( "offered health insurance" , "did not offer health insurance" )
			)
	)

gc()
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::sbo_MIcombine( lodown:::sbo_with( sbo_design , svyby( ~ one , ~ one , unwtd.count ) ) )

lodown:::sbo_MIcombine( lodown:::sbo_with( sbo_design , svyby( ~ one , ~ healthins , unwtd.count ) ) )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::sbo_MIcombine( lodown:::sbo_with( sbo_design , svytotal( ~ one ) ) )

lodown:::sbo_MIcombine( lodown:::sbo_with( sbo_design ,
	svyby( ~ one , ~ healthins , svytotal )
) )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::sbo_MIcombine( lodown:::sbo_with( sbo_design , svymean( ~ receipts_noisy ) ) )

lodown:::sbo_MIcombine( lodown:::sbo_with( sbo_design ,
	svyby( ~ receipts_noisy , ~ healthins , svymean )
) )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::sbo_MIcombine( lodown:::sbo_with( sbo_design , svymean( ~ n07_employer , na.rm = TRUE ) ) )

lodown:::sbo_MIcombine( lodown:::sbo_with( sbo_design ,
	svyby( ~ n07_employer , ~ healthins , svymean , na.rm = TRUE )
) )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::sbo_MIcombine( lodown:::sbo_with( sbo_design , svytotal( ~ receipts_noisy ) ) )

lodown:::sbo_MIcombine( lodown:::sbo_with( sbo_design ,
	svyby( ~ receipts_noisy , ~ healthins , svytotal )
) )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::sbo_MIcombine( lodown:::sbo_with( sbo_design , svytotal( ~ n07_employer , na.rm = TRUE ) ) )

lodown:::sbo_MIcombine( lodown:::sbo_with( sbo_design ,
	svyby( ~ n07_employer , ~ healthins , svytotal , na.rm = TRUE )
) )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::sbo_MIcombine( lodown:::sbo_with( sbo_design , svyquantile( ~ receipts_noisy , 0.5 , se = TRUE ) ) )

lodown:::sbo_MIcombine( lodown:::sbo_with( sbo_design ,
	svyby( 
		~ receipts_noisy , ~ healthins , svyquantile , 0.5 ,
		se = TRUE , keep.var = TRUE , ci = TRUE 
) ) )
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
lodown:::sbo_MIcombine( lodown:::sbo_with( sbo_design ,
	svyratio( numerator = ~ receipts_noisy , denominator = ~ employment_noisy )
) )
```

### Subsetting {-}

Restrict the survey design to jointly owned by husband and wife:
```{r eval = FALSE , results = "hide" }
sub_sbo_design <- lodown:::sbo_subset( sbo_design , husbwife %in% 1:3 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
lodown:::sbo_MIcombine( lodown:::sbo_with( sub_sbo_design , svymean( ~ receipts_noisy ) ) ) ; rm( sub_sbo_design ) ; gc()
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <-
	lodown:::sbo_MIcombine( lodown:::sbo_with( sbo_design ,
		svymean( ~ receipts_noisy )
	) )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	lodown:::sbo_MIcombine( lodown:::sbo_with( sbo_design ,
		svyby( ~ receipts_noisy , ~ healthins , svymean )
	) )

coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
lodown:::sbo_degf( sbo_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
lodown:::sbo_MIcombine( lodown:::sbo_with( sbo_design , svyvar( ~ receipts_noisy ) ) )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
lodown:::sbo_MIcombine( lodown:::sbo_with( sbo_design ,
	svymean( ~ receipts_noisy , deff = TRUE )
) )

# SRS with replacement
lodown:::sbo_MIcombine( lodown:::sbo_with( sbo_design ,
	svymean( ~ receipts_noisy , deff = "replace" )
) )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
lodown:::sbo_MIsvyciprop( ~ established_before_2000 , sbo_design ,
	method = "likelihood" , na.rm = TRUE ) ; gc()
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
# not implemented lodown:::MIsvyttest( receipts_noisy ~ established_before_2000 , sbo_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
# not implemented lodown:::MIsvychisq( ~ established_before_2000 + n07_employer , sbo_design )
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	lodown:::sbo_MIcombine( lodown:::sbo_with( sbo_design ,
		svyglm( receipts_noisy ~ established_before_2000 + n07_employer )
	) )
	
glm_result
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:58-sbo.Rmd-->

# Survey of Consumer Finances (SCF) {-}

[![Build Status](https://travis-ci.org/asdfree/scf.svg?branch=master)](https://travis-ci.org/asdfree/scf) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/scf?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/scf)

The Survey of Consumer Finances (SCF) tracks the wealth of American families. Five thousand households answer a battery of questions about income, net worth, credit card debt, pensions, mortgages, even the lease on their cars. Plenty of surveys collect annual income, only the Survey of Consumer Finances captures such detailed asset data.

* One table of survey responses and a second table with replicate weights, both with one row per sampled household.

* A complex sample survey designed to generalize to the civilian non-institutional population of the United States.

* Released triennially since 1983.

* Administered by the [Board of Governors of the Federal Reserve System](https://www.federalreserve.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available SCF microdata by simply specifying `"scf"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "scf" , output_dir = file.path( path.expand( "~" ) , "SCF" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the SCF catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available SCF microdata files
scf_cat <-
	get_catalog( "scf" ,
		output_dir = file.path( path.expand( "~" ) , "SCF" ) )

# 2016 only
scf_cat <- subset( scf_cat , year == 2016 )
# download the microdata to your local computer
lodown( "scf" , scf_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a multiply-imputed, complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
library(survey)
library(mitools)

scf_imp <- readRDS( file.path( path.expand( "~" ) , "SCF" , "scf 2016.rds" ) )

scf_rw <- readRDS( file.path( path.expand( "~" ) , "SCF" , "scf 2016 rw.rds" ) )

scf_design <- 
	svrepdesign( 
		weights = ~wgt , 
		repweights = scf_rw[ , -1 ] , 
		data = imputationList( scf_imp ) , 
		scale = 1 ,
		rscales = rep( 1 / 998 , 999 ) ,
		mse = TRUE ,
		type = "other" ,
		combined.weights = TRUE
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
scf_design <- 
	update( 
		scf_design , 
		
		hhsex = factor( hhsex , labels = c( "male" , "female" ) ) ,
		
		married = as.numeric( married == 1 ) ,
		
		edcl = 
			factor( 
				edcl , 
				labels = 
					c( 
						"less than high school" , 
						"high school or GED" , 
						"some college" , 
						"college degree" 
					) 
			)

	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::scf_MIcombine( with( scf_design , svyby( ~ one , ~ one , unwtd.count ) ) )

lodown:::scf_MIcombine( with( scf_design , svyby( ~ one , ~ hhsex , unwtd.count ) ) )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::scf_MIcombine( with( scf_design , svytotal( ~ one ) ) )

lodown:::scf_MIcombine( with( scf_design ,
	svyby( ~ one , ~ hhsex , svytotal )
) )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::scf_MIcombine( with( scf_design , svymean( ~ networth ) ) )

lodown:::scf_MIcombine( with( scf_design ,
	svyby( ~ networth , ~ hhsex , svymean )
) )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::scf_MIcombine( with( scf_design , svymean( ~ edcl ) ) )

lodown:::scf_MIcombine( with( scf_design ,
	svyby( ~ edcl , ~ hhsex , svymean )
) )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::scf_MIcombine( with( scf_design , svytotal( ~ networth ) ) )

lodown:::scf_MIcombine( with( scf_design ,
	svyby( ~ networth , ~ hhsex , svytotal )
) )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::scf_MIcombine( with( scf_design , svytotal( ~ edcl ) ) )

lodown:::scf_MIcombine( with( scf_design ,
	svyby( ~ edcl , ~ hhsex , svytotal )
) )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::scf_MIcombine( with( scf_design , svyquantile( ~ networth , 0.5 , se = TRUE ) ) )

lodown:::scf_MIcombine( with( scf_design ,
	svyby( 
		~ networth , ~ hhsex , svyquantile , 0.5 ,
		se = TRUE , keep.var = TRUE , ci = TRUE 
) ) )
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
lodown:::scf_MIcombine( with( scf_design ,
	svyratio( numerator = ~ income , denominator = ~ networth )
) )
```

### Subsetting {-}

Restrict the survey design to labor force participants:
```{r eval = FALSE , results = "hide" }
sub_scf_design <- subset( scf_design , lf == 1 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
lodown:::scf_MIcombine( with( sub_scf_design , svymean( ~ networth ) ) )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <-
	lodown:::scf_MIcombine( with( scf_design ,
		svymean( ~ networth )
	) )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	lodown:::scf_MIcombine( with( scf_design ,
		svyby( ~ networth , ~ hhsex , svymean )
	) )

coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( scf_design$designs[[1]] )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
lodown:::scf_MIcombine( with( scf_design , svyvar( ~ networth ) ) )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
lodown:::scf_MIcombine( with( scf_design ,
	svymean( ~ networth , deff = TRUE )
) )

# SRS with replacement
lodown:::scf_MIcombine( with( scf_design ,
	svymean( ~ networth , deff = "replace" )
) )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
lodown:::MIsvyciprop( ~ married , scf_design ,
	method = "likelihood" )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
lodown:::MIsvyttest( networth ~ married , scf_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
lodown:::MIsvychisq( ~ married + edcl , scf_design )
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	lodown:::scf_MIcombine( with( scf_design ,
		svyglm( networth ~ married + edcl )
	) )
	
summary( glm_result )
```

## Poverty and Inequality Estimation with `convey` {-}

The R `convey` library estimates measures of income concentration, poverty, inequality, and wellbeing. [This textbook](https://guilhermejacob.github.io/context/) details the available features. As a starting point for SCF users, this code calculates the gini coefficient on complex sample survey data:

```{r eval = FALSE , results = "hide" }
library(convey)
scf_design$designs <- lapply( scf_design$designs , convey_prep )

lodown:::scf_MIcombine( with( scf_design , svygini( ~ networth ) ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:59-scf.Rmd-->

# National Plan and Provider Enumeration System (SEER) {-}

The National Plan and Provider Enumeration System (NPPES) contains information about every medical provider, insurance plan, and clearinghouse actively operating in the United States healthcare industry.

* A single large table with one row per enumerated health care provider.

* A census of individuals and organizations who bill for medical services in the United States.

* Updated monthly with new providers.

* Maintained by the United States [Centers for Medicare & Medicaid Services (CMS)](http://www.cms.gov/)

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available SEER microdata by simply specifying `"seer"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "seer" , output_dir = file.path( path.expand( "~" ) , "SEER" ) , 
	your_username = "username" , 
	your_password = "password" )
```

## Analysis Examples with SQL and `MonetDBLite` {-}

Connect to a database:

```{r eval = FALSE }
library(DBI)
dbdir <- file.path( path.expand( "~" ) , "SEER" , "MonetDB" )
db <- dbConnect( MonetDBLite::MonetDBLite() , dbdir )
```

```{r eval = FALSE }

```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
dbSendQuery( db , "ALTER TABLE npi ADD COLUMN individual INTEGER" )

dbSendQuery( db , 
	"UPDATE npi 
	SET individual = 
		CASE WHEN entity_type_code = 1 THEN 1 ELSE 0 END" 
)

dbSendQuery( db , "ALTER TABLE npi ADD COLUMN provider_enumeration_year INTEGER" )

dbSendQuery( db , 
	"UPDATE npi 
	SET provider_enumeration_year = 
		CAST( SUBSTRING( provider_enumeration_date , 7 , 10 ) AS INTEGER )" 
)
```

### Unweighted Counts {-}

Count the unweighted number of records in the SQL table, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT COUNT(*) FROM npi" )

dbGetQuery( db ,
	"SELECT
		provider_gender_code ,
		COUNT(*) 
	FROM npi
	GROUP BY provider_gender_code"
)
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT AVG( provider_enumeration_year ) FROM npi" )

dbGetQuery( db , 
	"SELECT 
		provider_gender_code , 
		AVG( provider_enumeration_year ) AS mean_provider_enumeration_year
	FROM npi 
	GROUP BY provider_gender_code" 
)
```

Initiate a function that allows division by zero:
```{r eval = FALSE , results = "hide" }
dbSendQuery( db , 
	"CREATE FUNCTION 
		div_noerror(l DOUBLE, r DOUBLE) 
	RETURNS DOUBLE 
	EXTERNAL NAME calc.div_noerror" 
)
```

Calculate the distribution of a categorical variable:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		is_sole_proprietor , 
		div_noerror( 
			COUNT(*) , 
			( SELECT COUNT(*) FROM npi ) 
		) AS share_is_sole_proprietor
	FROM npi 
	GROUP BY is_sole_proprietor" 
)
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT SUM( provider_enumeration_year ) FROM npi" )

dbGetQuery( db , 
	"SELECT 
		provider_gender_code , 
		SUM( provider_enumeration_year ) AS sum_provider_enumeration_year 
	FROM npi 
	GROUP BY provider_gender_code" 
)
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT QUANTILE( provider_enumeration_year , 0.5 ) FROM npi" )

dbGetQuery( db , 
	"SELECT 
		provider_gender_code , 
		QUANTILE( provider_enumeration_year , 0.5 ) AS median_provider_enumeration_year
	FROM npi 
	GROUP BY provider_gender_code" 
)
```

### Subsetting {-}

Limit your SQL analysis to California with `WHERE`:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db ,
	"SELECT
		AVG( provider_enumeration_year )
	FROM npi
	WHERE provider_business_practice_location_address_state_name = 'CA'"
)
```

### Measures of Uncertainty {-}

Calculate the variance and standard deviation, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		VAR_SAMP( provider_enumeration_year ) , 
		STDDEV_SAMP( provider_enumeration_year ) 
	FROM npi" 
)

dbGetQuery( db , 
	"SELECT 
		provider_gender_code , 
		VAR_SAMP( provider_enumeration_year ) AS var_provider_enumeration_year ,
		STDDEV_SAMP( provider_enumeration_year ) AS stddev_provider_enumeration_year
	FROM npi 
	GROUP BY provider_gender_code" 
)
```

### Regression Models and Tests of Association {-}

Calculate the correlation between two variables, overall and by groups:
```{r eval = FALSE , results = "hide" }
dbGetQuery( db , 
	"SELECT 
		CORR( CAST( individual AS DOUBLE ) , CAST( provider_enumeration_year AS DOUBLE ) )
	FROM npi" 
)

dbGetQuery( db , 
	"SELECT 
		provider_gender_code , 
		CORR( CAST( individual AS DOUBLE ) , CAST( provider_enumeration_year AS DOUBLE ) )
	FROM npi 
	GROUP BY provider_gender_code" 
)
```

## Analysis Examples with `dplyr` {-}

The R `dplyr` library offers an alternative grammar of data manipulation to base R and SQL syntax. [dplyr](https://github.com/tidyverse/dplyr/) offers many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, and the `tidyverse` style of non-standard evaluation. [This vignette](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html) details the available features. As a starting point for SEER users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(dplyr)
dplyr_db <- MonetDBLite::src_monetdblite( dbdir )
seer_tbl <- tbl( dplyr_db , 'npi' )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
seer_tbl %>%
	summarize( mean = mean( provider_enumeration_year ) )

seer_tbl %>%
	group_by( provider_gender_code ) %>%
	summarize( mean = mean( provider_enumeration_year ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }
dbGetQuery( db , "SELECT COUNT(*) FROM npi" )
```

## Database Shutdown {-}

```{r eval = FALSE , results = "hide" }
dbDisconnect( db , shutdown = TRUE )
```

<!--chapter:end:60-seer.Rmd-->

# Survey of Health, Ageing and Retirement in Europe (SHARE) {-}

[![Build Status](https://travis-ci.org/asdfree/share.svg?branch=master)](https://travis-ci.org/asdfree/share) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/share?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/share)

The Survey of Health, Ageing and Retirement in Europe interviews senior citizens across the continent for their entire life. Allows for findings like, "Among Belgians who were 50-74 years old in 2004, X% lived in nursing homes by 2010."

* Many tables, most with one row per sampled respondent for the period.

* A complex sample longitudinal survey designed to generalize to the civilian, non-institutionalized population of participating European countries aged 50 or older.

* Released every two or three years since 2004.

* Coordinated at the [Max Planck Institute](http://www.share-project.org/organisation/coordination.html) and [funded by consortium](http://www.share-project.org/organisation/funding.html).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available SHARE microdata by simply specifying `"share"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "share" , output_dir = file.path( path.expand( "~" ) , "SHARE" ) , 
	your_username = "username" , 
	your_password = "password" )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the SHARE catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available SHARE microdata files
share_cat <-
	get_catalog( "share" ,
		output_dir = file.path( path.expand( "~" ) , "SHARE" ) , 
		your_username = "username" , 
		your_password = "password" )

# wave 1, wave 6, and longitudinal weights only
share_cat <- subset( share_cat , grepl( "ave 1|ave 6|ongitudinal" , output_folder ) )
# download the microdata to your local computer
lodown( "share" , share_cat , 
	your_username = "username" , 
	your_password = "password" )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
options( survey.lonely.psu = "adjust" )

library(survey)

available_files <-
	list.files( 
		file.path( path.expand( "~" ) , "SHARE" ) , 
		recursive = TRUE , 
		full.names = TRUE 
	)

# wave six demographics file
share_dn6_df <-
	readRDS( grep( "6\\.0\\.0(.*)sharew6(.*)dn\\.rds" , available_files , value = TRUE ) )

share_dn6_df <-
	share_dn6_df[ c( "mergeid" , "country" , "dn042_" , "dn004_" ) ]
	
# wave six physical health file
share_ph1_df <-
	readRDS( grep( "sharew1(.*)ph\\.rds" , available_files , value = TRUE ) )

share_ph1_df$weight_in_2004 <-
		ifelse( share_ph1_df$ph012_ < 0 , NA , share_ph1_df$ph012_ )
		
share_ph1_df <-
	share_ph1_df[ c( "mergeid" , "weight_in_2004" , "ph005_" ) ]
	
# wave six physical health file
share_ph6_df <-
	readRDS( grep( "6\\.0\\.0(.*)sharew6(.*)ph\\.rds" , available_files , value = TRUE ) )

share_ph6_df$weight_in_2015 <-
		ifelse( share_ph6_df$ph012_ < 0 , NA , share_ph6_df$ph012_ )
		
share_ph6_df <-
	share_ph6_df[ c( "mergeid" , "weight_in_2015" , "ph003_" ) ]
	

# longitudinal weights file
share_longwt_df <-
	readRDS( grep( "longitudinal_weights_w1\\-(.*)\\.rds" , available_files , value = TRUE ) )

# france only longitudinal weights
france_df <- subset( share_longwt_df , country == 17 & ( cliw_a > 0 ) )

nrow_check <- nrow( france_df )

# merge on each of the tables
france_df <- merge( france_df , share_dn6_df )
france_df <- merge( france_df , share_ph1_df )
france_df <- merge( france_df , share_ph6_df )

# confirm no change in records
stopifnot( nrow( france_df ) == nrow_check )

share_design <- 
	svydesign( 
		~ psu + ssu , 
		strata = ~ stratum1 + stratum2 , 
		data = france_df , 
		weights = ~ cliw_a , 
		nest = TRUE 
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
share_design <- 
	update( 
		share_design , 
		
		one = 1 ,
		
		sexe = factor( dn042_ , levels = 1:2 , labels = c( 'masculin' , 'feminin' ) ) ,
		
		health_in_general_2015 =
			factor( ph003_ , levels = 1:5 , labels =
				c( "excellente" , "tres bonne" , "bonne" , "acceptable" , "mediocre" )
			) ,
			
		fortemente_limite_2004 = ifelse( ph005_ %in% 1:3 , as.numeric( ph005_ == 1 ) , NA )

	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( share_design , "sampling" ) != 0 )

svyby( ~ one , ~ sexe , share_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , share_design )

svyby( ~ one , ~ sexe , share_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ weight_in_2015 , share_design , na.rm = TRUE )

svyby( ~ weight_in_2015 , ~ sexe , share_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ health_in_general_2015 , share_design , na.rm = TRUE )

svyby( ~ health_in_general_2015 , ~ sexe , share_design , svymean , na.rm = TRUE )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ weight_in_2015 , share_design , na.rm = TRUE )

svyby( ~ weight_in_2015 , ~ sexe , share_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ health_in_general_2015 , share_design , na.rm = TRUE )

svyby( ~ health_in_general_2015 , ~ sexe , share_design , svytotal , na.rm = TRUE )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ weight_in_2015 , share_design , 0.5 , na.rm = TRUE )

svyby( 
	~ weight_in_2015 , 
	~ sexe , 
	share_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ weight_in_2015 , 
	denominator = ~ weight_in_2004 , 
	share_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to persons born in france:
```{r eval = FALSE , results = "hide" }
sub_share_design <- subset( share_design , dn004_ == 1 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ weight_in_2015 , sub_share_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ weight_in_2015 , share_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ weight_in_2015 , 
		~ sexe , 
		share_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( share_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ weight_in_2015 , share_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ weight_in_2015 , share_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ weight_in_2015 , share_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ fortemente_limite_2004 , share_design ,
	method = "likelihood" , na.rm = TRUE )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( weight_in_2015 ~ fortemente_limite_2004 , share_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ fortemente_limite_2004 + health_in_general_2015 , 
	share_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		weight_in_2015 ~ fortemente_limite_2004 + health_in_general_2015 , 
		share_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for SHARE users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
share_srvyr_design <- as_survey( share_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
share_srvyr_design %>%
	summarize( mean = survey_mean( weight_in_2015 , na.rm = TRUE ) )

share_srvyr_design %>%
	group_by( sexe ) %>%
	summarize( mean = survey_mean( weight_in_2015 , na.rm = TRUE ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:61-share.Rmd-->

# Pesquisa Nacional por Amostra de Domicilios (SIPP) {-}

*Contributed by Dr. Djalma Pessoa <<pessoad@gmail.com>>*

Brazil's previous principal household survey, the Pesquisa Nacional por Amostra de Domicilios (PNAD) measures general education, labor, income, and housing characteristics of the population.

* One table with one row per sampled household and a second table with one row per individual within each sampled household.

* A complex sample survey designed to generalize to the civilian non-institutional population of Brazil, although the rural north was not included prior to 2004.

* Released annually since 2001 except for years ending in zero, when the decennial census takes its place.

* Administered by the [Instituto Brasileiro de Geografia e Estatistica](http://www.ibge.gov.br/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available SIPP microdata by simply specifying `"sipp"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "sipp" , output_dir = file.path( path.expand( "~" ) , "SIPP" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the SIPP catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available SIPP microdata files
sipp_cat <-
	get_catalog( "sipp" ,
		output_dir = file.path( path.expand( "~" ) , "SIPP" ) )

# 2011 only
sipp_cat <- subset( sipp_cat , year == 2011 )
# download the microdata to your local computer
lodown( "sipp" , sipp_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a database-backed complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
library(DBI)
library(MonetDBLite)
library(survey)

options( survey.lonely.psu = "adjust" )

prestratified_design <-
	svydesign(
		id = ~v4618 ,
		strata = ~v4617 ,
		data = sipp_cat[ 1 , "db_tablename" ] ,
		weights = ~pre_wgt ,
		nest = TRUE ,
		dbtype = "MonetDBLite" ,
		dbname = sipp_cat[ 1 , "dbfolder" ]
	)
	
sipp_design <- 
	lodown:::pnad_postStratify( 
		design = prestratified_design ,
		strata.col = 'v4609' ,
		oldwgt = 'pre_wgt'
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
sipp_design <- 
	update( 
		sipp_design , 
		age_categories = factor( 1 + findInterval( v8005 , seq( 5 , 60 , 5 ) ) ) ,
		male = as.numeric( v0302 == 2 ) ,
		teenagers = as.numeric( v8005 > 12 & v8005 < 20 ) ,
		started_working_before_thirteen = as.numeric( v9892 < 13 )
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( sipp_design , "sampling" ) != 0 )

svyby( ~ one , ~ region , sipp_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , sipp_design )

svyby( ~ one , ~ region , sipp_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ v4720 , sipp_design , na.rm = TRUE )

svyby( ~ v4720 , ~ region , sipp_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ age_categories , sipp_design )

svyby( ~ age_categories , ~ region , sipp_design , svymean )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ v4720 , sipp_design , na.rm = TRUE )

svyby( ~ v4720 , ~ region , sipp_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ age_categories , sipp_design )

svyby( ~ age_categories , ~ region , sipp_design , svytotal )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ v4720 , sipp_design , 0.5 , na.rm = TRUE )

svyby( 
	~ v4720 , 
	~ region , 
	sipp_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ started_working_before_thirteen , 
	denominator = ~ teenagers , 
	sipp_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to married persons:
```{r eval = FALSE , results = "hide" }
sub_sipp_design <- subset( sipp_design , v4011 == 1 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ v4720 , sub_sipp_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ v4720 , sipp_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ v4720 , 
		~ region , 
		sipp_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( sipp_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ v4720 , sipp_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ v4720 , sipp_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ v4720 , sipp_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ male , sipp_design ,
	method = "likelihood" )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( v4720 ~ male , sipp_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ male + age_categories , 
	sipp_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		v4720 ~ male + age_categories , 
		sipp_design 
	)

summary( glm_result )
```

## Poverty and Inequality Estimation with `convey` {-}

The R `convey` library estimates measures of income concentration, poverty, inequality, and wellbeing. [This textbook](https://guilhermejacob.github.io/context/) details the available features. As a starting point for SIPP users, this code calculates the gini coefficient on complex sample survey data:

```{r eval = FALSE , results = "hide" }
library(convey)
sipp_design <- convey_prep( sipp_design )

sub_sipp_design <- 
	subset( 
		sipp_design , 
		!is.na( v4720 ) & v4720 != 0 & v8005 >= 15
	)

svygini( ~ v4720 , sub_sipp_design , na.rm = TRUE )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }
svytotal( ~one , sipp_design )
svytotal( ~factor( v0302 ) , sipp_design )
cv( svytotal( ~factor( v0302 ) , sipp_design ) )
```

## Database Shutdown {-}

```{r eval = FALSE , results = "hide" }
close( sipp_design , shutdown = TRUE )
```

<!--chapter:end:62-sipp.Rmd-->

# Social Security Administration Public Use Microdata (SSA) {-}

[![Build Status](https://travis-ci.org/asdfree/ssa.svg?branch=master)](https://travis-ci.org/asdfree/ssa) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/ssa?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/ssa)

Research extracts provided by the Social Security Administration.

* Tables contain either one record per person or one record per person per year.

* The entire population of either social security number holders (most of the country) or social security recipients (just beneficiaries). One-percent samples should be multiplied by 100 to get accurate nationwide count statistics, five-percent samples by 20.

* No expected release timeline.

* Released by the United States [Social Security Administration (SSA)](http://www.ssa.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available SSA microdata by simply specifying `"ssa"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "ssa" , output_dir = file.path( path.expand( "~" ) , "SSA" ) )
```

## Analysis Examples with base R {-}

Load a data frame:

```{r eval = FALSE }
ssa_df <- readRDS( file.path( path.expand( "~" ) , "SSA" , "ssr_data/SSIPUF.rds" ) )
```

```{r eval = FALSE }

```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
ssa_df <- 
	transform( 
		ssa_df , 
		
		mental_disorder = as.numeric( diag %in% 1:2 ) ,
		
		program_eligibility =
			factor( 
				prel , 
				
				levels = 0:5 , 
				
				labels =
					c( "Unspecified" ,
					"Aged individual" ,
					"Aged spouse" ,
					"Disabled or blind individual" ,
					"Disabled or blind spouse" ,
					"Disabled or blind child" )
			)
			
	)
	
```

### Unweighted Counts {-}

Count the unweighted number of records in the table, overall and by groups:
```{r eval = FALSE , results = "hide" }
nrow( ssa_df )

table( ssa_df[ , "stat" ] , useNA = "always" )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
mean( ssa_df[ , "fpmt" ] )

tapply(
	ssa_df[ , "fpmt" ] ,
	ssa_df[ , "stat" ] ,
	mean 
)
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
prop.table( table( ssa_df[ , "program_eligibility" ] ) )

prop.table(
	table( ssa_df[ , c( "program_eligibility" , "stat" ) ] ) ,
	margin = 2
)
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( ssa_df[ , "fpmt" ] )

tapply(
	ssa_df[ , "fpmt" ] ,
	ssa_df[ , "stat" ] ,
	sum 
)
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
quantile( ssa_df[ , "fpmt" ] , 0.5 )

tapply(
	ssa_df[ , "fpmt" ] ,
	ssa_df[ , "stat" ] ,
	quantile ,
	0.5 
)
```

### Subsetting {-}

Limit your `data.frame` to females:
```{r eval = FALSE , results = "hide" }
sub_ssa_df <- subset( ssa_df , sex == "F" )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
mean( sub_ssa_df[ , "fpmt" ] )
```

### Measures of Uncertainty {-}

Calculate the variance, overall and by groups:
```{r eval = FALSE , results = "hide" }
var( ssa_df[ , "fpmt" ] )

tapply(
	ssa_df[ , "fpmt" ] ,
	ssa_df[ , "stat" ] ,
	var 
)
```

### Regression Models and Tests of Association {-}

Perform a t-test:
```{r eval = FALSE , results = "hide" }
t.test( fpmt ~ mental_disorder , ssa_df )
```

Perform a chi-squared test of association:
```{r eval = FALSE , results = "hide" }
this_table <- table( ssa_df[ , c( "mental_disorder" , "program_eligibility" ) ] )

chisq.test( this_table )
```

Perform a generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	glm( 
		fpmt ~ mental_disorder + program_eligibility , 
		data = ssa_df
	)

summary( glm_result )
```

## Analysis Examples with `dplyr` {-}

The R `dplyr` library offers an alternative grammar of data manipulation to base R and SQL syntax. [dplyr](https://github.com/tidyverse/dplyr/) offers many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, and the `tidyverse` style of non-standard evaluation. [This vignette](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html) details the available features. As a starting point for SSA users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(dplyr)
ssa_tbl <- tbl_df( ssa_df )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
ssa_tbl %>%
	summarize( mean = mean( fpmt ) )

ssa_tbl %>%
	group_by( stat ) %>%
	summarize( mean = mean( fpmt ) )
```



<!--chapter:end:63-ssa.Rmd-->

# Trends in International Mathematics and Science Study (TIMSS) {-}

The Trends in International Mathematics and Science Study (TIMSS) tracks the math and science competency of fourth graders across about fifty nations.

* A series of tables with one record per school (ACG), per student (ASG), per teacher (ATG), as well as files containing student achievement (ASA), home background (ASH), student-teacher linkage (AST), and within-country scoring reliability (ASR).

* A complex sample survey designed to generalize to the fourth-grade student population of participating countries.

* Released quadrennially since 1995.

* Funded by the [International Association for the Evaluation of Educational Achievement](https://www.iea.nl/) and compiled by the [Lynch School of Education at Boston College](http://www.bc.edu/bc-web/schools/lsoe.html).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available TIMSS microdata by simply specifying `"timss"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "timss" , output_dir = file.path( path.expand( "~" ) , "TIMSS" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the TIMSS catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available TIMSS microdata files
timss_cat <-
	get_catalog( "timss" ,
		output_dir = file.path( path.expand( "~" ) , "TIMSS" ) )

# 2015 only
timss_cat <- subset( timss_cat , year == 2015 )
# download the microdata to your local computer
lodown( "timss" , timss_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a multiply-imputed, complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
library(survey)
library(mitools)
library(MonetDBLite)

# load the ASG (student background) + ASH (home background) merged design
timss_design <- readRDS( file.path( path.expand( "~" ) , "TIMSS" , "2015/asg_design.rds" ) )

design_weights <- readRDS( file.path( path.expand( "~" ) , "TIMSS" , "2015/asg_weights.rds" ) )

five_tablenames <- paste0( "asg_2015_" , 1:5 )

timss_design <- lodown:::svyMDBdesign( timss_design )
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
timss_design <- 
	update( 
		timss_design , 
		
		one = 1 ,
		
		idcntry = factor( idcntry ) ,
		
		sex = factor( itsex , labels = c( "male" , "female" ) ) ,
		
		born_2005_or_later = as.numeric( itbirthy >= 2005 )

	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::timss_MIcombine( with( timss_design , svyby( ~ one , ~ one , unwtd.count ) ) )

lodown:::timss_MIcombine( with( timss_design , svyby( ~ one , ~ sex , unwtd.count ) ) )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::timss_MIcombine( with( timss_design , svytotal( ~ one ) ) )

lodown:::timss_MIcombine( with( timss_design ,
	svyby( ~ one , ~ sex , svytotal )
) )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::timss_MIcombine( with( timss_design , svymean( ~ asmmat ) ) )

lodown:::timss_MIcombine( with( timss_design ,
	svyby( ~ asmmat , ~ sex , svymean )
) )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::timss_MIcombine( with( timss_design , svymean( ~ idcntry ) ) )

lodown:::timss_MIcombine( with( timss_design ,
	svyby( ~ idcntry , ~ sex , svymean )
) )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::timss_MIcombine( with( timss_design , svytotal( ~ asmmat ) ) )

lodown:::timss_MIcombine( with( timss_design ,
	svyby( ~ asmmat , ~ sex , svytotal )
) )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::timss_MIcombine( with( timss_design , svytotal( ~ idcntry ) ) )

lodown:::timss_MIcombine( with( timss_design ,
	svyby( ~ idcntry , ~ sex , svytotal )
) )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
lodown:::timss_MIcombine( with( timss_design , svyquantile( ~ asmmat , 0.5 , se = TRUE ) ) )

lodown:::timss_MIcombine( with( timss_design ,
	svyby( 
		~ asmmat , ~ sex , svyquantile , 0.5 ,
		se = TRUE , keep.var = TRUE , ci = TRUE 
) ) )
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
lodown:::timss_MIcombine( with( timss_design ,
	svyratio( numerator = ~ asssci , denominator = ~ asmmat )
) )
```

### Subsetting {-}

Restrict the survey design to Australia, Austria, Azerbaijan, Belgium (French):
```{r eval = FALSE , results = "hide" }
sub_timss_design <- subset( timss_design , idcntry %in% c( 36 , 40 , 31 , 957 ) )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
lodown:::timss_MIcombine( with( sub_timss_design , svymean( ~ asmmat ) ) )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <-
	lodown:::timss_MIcombine( with( timss_design ,
		svymean( ~ asmmat )
	) )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	lodown:::timss_MIcombine( with( timss_design ,
		svyby( ~ asmmat , ~ sex , svymean )
	) )

coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( timss_design$designs[[1]] )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
lodown:::timss_MIcombine( with( timss_design , svyvar( ~ asmmat ) ) )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
lodown:::timss_MIcombine( with( timss_design ,
	svymean( ~ asmmat , deff = TRUE )
) )

# SRS with replacement
lodown:::timss_MIcombine( with( timss_design ,
	svymean( ~ asmmat , deff = "replace" )
) )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
lodown:::MIsvyciprop( ~ born_2001_or_later , timss_design ,
	method = "likelihood" , na.rm = TRUE )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
lodown:::MIsvyttest( asmmat ~ born_2001_or_later , timss_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
lodown:::MIsvychisq( ~ born_2001_or_later + idcntry , timss_design )
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	lodown:::timss_MIcombine( with( timss_design ,
		svyglm( asmmat ~ born_2001_or_later + idcntry )
	) )
	
summary( glm_result )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:64-timss.Rmd-->

# United States Decennial Census Public Use Microdata Sample (USPUMS) {-}

The Long-Form Decennial Census of the United States.

* One table with one row per household and a second table with one row per individual within each household. 1990 and 2000 include both 1% and 5% samples. 2010 contains only a 10% sample.

* An enumeration of the civilian population of the United States.

* Released decennially by the United States Census Bureau since 1990, however earlier extracts are available from IPUMS International.

* Administered by the [US Census Bureau](http://www.census.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available USPUMS microdata by simply specifying `"uspums"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "uspums" , output_dir = file.path( path.expand( "~" ) , "USPUMS" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the USPUMS catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available USPUMS microdata files
uspums_cat <-
	get_catalog( "uspums" ,
		output_dir = file.path( path.expand( "~" ) , "USPUMS" ) )

# 2000 1% sample only
uspums_cat <- subset( uspums_cat , year == 2000 & percent == 1 )
# download the microdata to your local computer
lodown( "uspums" , uspums_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a database-backed complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
library(DBI)
library(MonetDBLite)
library(survey)

options( survey.lonely.psu = "adjust" )

uspums_design <- readRDS( file.path( path.expand( "~" ) , "USPUMS" , "pums_2000_1_m.rds" ) )

uspums_design <- open( uspums_design , driver = MonetDBLite() )
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
uspums_design <-
	update(
		
		uspums_design ,
		
		age_categories = factor( 1 + findInterval( age , c( 18 , 35 , 65 ) ) , labels = c( "under 18" , "18-34" , "35-64" , "65+" ) ) ,
		
		married = as.numeric( marstat == 1 ) ,
		
		poverty_status = ifelse( poverty == 0 , NA , poverty ) ,
		
		unemployed = as.numeric( esr %in% 3 ) ,
		
		labor_force = as.numeric( esr %in% 1:5 ) ,
		
		employment_status = 
			factor( 
				esr , 
				levels = 0:6 , 
				labels = 
					c( 
						"NIU" ,
						"Employed, at work" , 
						"Employed, with a job but not at work" ,
						"Unemployed" ,
						"Armed Forces, at work" ,
						"Armed Forces, with a job but not at work" ,
						"Not in labor force"
					)
			) ,
			
		
		state_name =
		
			factor(
			
				state ,
				
				levels = 
					c(1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 
					21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 
					37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 
					55, 56, 66, 72, 78) ,
					
				labels = 
					c("ALABAMA", "ALASKA", "ARIZONA", "ARKANSAS", "CALIFORNIA", 
					"COLORADO", "CONNECTICUT", "DELAWARE", "DISTRICT OF COLUMBIA", 
					"FLORIDA", "GEORGIA", "HAWAII", "IDAHO", "ILLINOIS", "INDIANA",
					"IOWA", "KANSAS", "KENTUCKY", "LOUISIANA", "MAINE", "MARYLAND",
					"MASSACHUSETTS", "MICHIGAN", "MINNESOTA", "MISSISSIPPI", 
					"MISSOURI", "MONTANA", "NEBRASKA", "NEVADA", "NEW HAMPSHIRE",
					"NEW JERSEY", "NEW MEXICO", "NEW YORK", "NORTH CAROLINA", 
					"NORTH DAKOTA", "OHIO", "OKLAHOMA", "OREGON", "PENNSYLVANIA",
					"RHODE ISLAND", "SOUTH CAROLINA", "SOUTH DAKOTA", "TENNESSEE",
					"TEXAS", "UTAH", "VERMONT", "VIRGINIA", "WASHINGTON",
					"WEST VIRGINIA", "WISCONSIN", "WYOMING", "GUAM", "PUERTO RICO",
					"U.S. VIRGIN ISLANDS")
					
			) 
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( uspums_design , "sampling" ) != 0 )

svyby( ~ one , ~ state_name , uspums_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , uspums_design )

svyby( ~ one , ~ state_name , uspums_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ poverty_status , uspums_design , na.rm = TRUE )

svyby( ~ poverty_status , ~ state_name , uspums_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ employment_status , uspums_design )

svyby( ~ employment_status , ~ state_name , uspums_design , svymean )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ poverty_status , uspums_design , na.rm = TRUE )

svyby( ~ poverty_status , ~ state_name , uspums_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ employment_status , uspums_design )

svyby( ~ employment_status , ~ state_name , uspums_design , svytotal )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ poverty_status , uspums_design , 0.5 , na.rm = TRUE )

svyby( 
	~ poverty_status , 
	~ state_name , 
	uspums_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ unemployed , 
	denominator = ~ labor_force , 
	uspums_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to females:
```{r eval = FALSE , results = "hide" }
sub_uspums_design <- subset( uspums_design , sex == 2 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ poverty_status , sub_uspums_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ poverty_status , uspums_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ poverty_status , 
		~ state_name , 
		uspums_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( uspums_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ poverty_status , uspums_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ poverty_status , uspums_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ poverty_status , uspums_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ married , uspums_design ,
	method = "likelihood" , na.rm = TRUE )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( poverty_status ~ married , uspums_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ married + employment_status , 
	uspums_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		poverty_status ~ married + employment_status , 
		uspums_design 
	)

summary( glm_result )
```

## Poverty and Inequality Estimation with `convey` {-}

The R `convey` library estimates measures of income concentration, poverty, inequality, and wellbeing. [This textbook](https://guilhermejacob.github.io/context/) details the available features. As a starting point for USPUMS users, this code calculates the gini coefficient on complex sample survey data:

```{r eval = FALSE , results = "hide" }
library(convey)
uspums_design <- convey_prep( uspums_design )

svygini( ~ hinc , uspums_design , na.rm = TRUE )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```

## Database Shutdown {-}

```{r eval = FALSE , results = "hide" }
close( uspums_design , shutdown = TRUE )
```

<!--chapter:end:65-uspums.Rmd-->

# World Values Survey (WVS) {-}

[![Build Status](https://travis-ci.org/asdfree/wvs.svg?branch=master)](https://travis-ci.org/asdfree/wvs) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/wvs?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/wvs)

The World Values Survey studies changing values and their impact on social and political life in almost one hundred nations.

* One table per country per wave, with one row per sampled respondent.

* A complex sample survey designed to generalize the population aged eighteen and older in participating countries.

* Released about twice per decade since 1981.

* Administered as a confederacy, guided by a [scientific advisory committee](http://www.worldvaluessurvey.org/WVSContents.jsp?CMSID=SAC) and funded by [consortium](http://www.worldvaluessurvey.org/WVSContents.jsp).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available WVS microdata by simply specifying `"wvs"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "wvs" , output_dir = file.path( path.expand( "~" ) , "WVS" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the WVS catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available WVS microdata files
wvs_cat <-
	get_catalog( "wvs" ,
		output_dir = file.path( path.expand( "~" ) , "WVS" ) )

# wave six only
wvs_cat <- subset( wvs_cat , grepl( "United(.*)States" , full_url ) & wave == 6 )
# download the microdata to your local computer
lodown( "wvs" , wvs_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
library(survey)

wvs_df <-
	readRDS( 
		file.path( path.expand( "~" ) , "WVS" , 
			"wave 6/F00003106-WV6_Data_United_States_2011_spss_v_2016-01-01.rds" ) 
	)

# construct a fake survey design
warning( "this survey design produces correct point estimates
but incorrect standard errors." )
wvs_design <- 
	svydesign( 
		~ 1 , 
		data = wvs_df , 
		weights = ~ v258
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
wvs_design <- 
	update( 
		wvs_design , 
		
		one = 1 ,
		
		language_spoken_at_home =
			factor( v247 , 
				levels = c( 101 , 128 , 144 , 208 , 426 , 800 ) , 
				labels = c( 'chinese' , 'english' , 'french' , 
					'japanese' , 'spanish; castilian' , 'other' )
			) ,

		citizen = as.numeric( v246 == 1 ) ,
		
		task_creativity_1_10 = as.numeric( v232 ) ,
		
		work_independence_1_10 = as.numeric( v233 ) ,
		
		family_importance =
			factor( v4 , 
				labels = c( 'very' , 'rather' , 'not very' , 'not at all' ) 
			)
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( wvs_design , "sampling" ) != 0 )

svyby( ~ one , ~ language_spoken_at_home , wvs_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , wvs_design )

svyby( ~ one , ~ language_spoken_at_home , wvs_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ task_creativity_1_10 , wvs_design , na.rm = TRUE )

svyby( ~ task_creativity_1_10 , ~ language_spoken_at_home , wvs_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ family_importance , wvs_design , na.rm = TRUE )

svyby( ~ family_importance , ~ language_spoken_at_home , wvs_design , svymean , na.rm = TRUE )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ task_creativity_1_10 , wvs_design , na.rm = TRUE )

svyby( ~ task_creativity_1_10 , ~ language_spoken_at_home , wvs_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ family_importance , wvs_design , na.rm = TRUE )

svyby( ~ family_importance , ~ language_spoken_at_home , wvs_design , svytotal , na.rm = TRUE )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ task_creativity_1_10 , wvs_design , 0.5 , na.rm = TRUE )

svyby( 
	~ task_creativity_1_10 , 
	~ language_spoken_at_home , 
	wvs_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ task_creativity_1_10 , 
	denominator = ~ work_independence_1_10 , 
	wvs_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to seniors:
```{r eval = FALSE , results = "hide" }
sub_wvs_design <- subset( wvs_design , v242 >= 65 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ task_creativity_1_10 , sub_wvs_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ task_creativity_1_10 , wvs_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ task_creativity_1_10 , 
		~ language_spoken_at_home , 
		wvs_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( wvs_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ task_creativity_1_10 , wvs_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ task_creativity_1_10 , wvs_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ task_creativity_1_10 , wvs_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ citizen , wvs_design ,
	method = "likelihood" , na.rm = TRUE )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( task_creativity_1_10 ~ citizen , wvs_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ citizen + family_importance , 
	wvs_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		task_creativity_1_10 ~ citizen + family_importance , 
		wvs_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for WVS users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
wvs_srvyr_design <- as_survey( wvs_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
wvs_srvyr_design %>%
	summarize( mean = survey_mean( task_creativity_1_10 , na.rm = TRUE ) )

wvs_srvyr_design %>%
	group_by( language_spoken_at_home ) %>%
	summarize( mean = survey_mean( task_creativity_1_10 , na.rm = TRUE ) )
```

---

## Replication Example {-}

```{r eval = FALSE , results = "hide" }

```



<!--chapter:end:66-wvs.Rmd-->

# Youth Risk Behavior Surveillance System (YRBSS) {-}

[![Build Status](https://travis-ci.org/asdfree/yrbss.svg?branch=master)](https://travis-ci.org/asdfree/yrbss) [![Build status](https://ci.appveyor.com/api/projects/status/github/asdfree/yrbss?svg=TRUE)](https://ci.appveyor.com/project/ajdamico/yrbss)

The Youth Risk Behavior Surveillance System is the high school edition of the Behavioral Risk Factor Surveillance System (BRFSS), a scientific study of good kids who do bad things.

* One table with one row per sampled youth respondent.

* A complex sample survey designed to generalize to all public and private school students in grades 9-12 in the United States.

* Released biennially since 1993.

* Administered by the [Centers for Disease Control and Prevention](http://www.cdc.gov/).

## Simplified Download and Importation {-}

The R `lodown` package easily downloads and imports all available YRBSS microdata by simply specifying `"yrbss"` with an `output_dir =` parameter in the `lodown()` function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight.

```{r eval = FALSE }
library(lodown)
lodown( "yrbss" , output_dir = file.path( path.expand( "~" ) , "YRBSS" ) )
```

`lodown` also provides a catalog of available microdata extracts with the `get_catalog()` function. After requesting the YRBSS catalog, you could pass a subsetted catalog through the `lodown()` function in order to download and import specific extracts (rather than all available extracts).

```{r eval = FALSE , results = "hide" }
library(lodown)
# examine all available YRBSS microdata files
yrbss_cat <-
	get_catalog( "yrbss" ,
		output_dir = file.path( path.expand( "~" ) , "YRBSS" ) )

# 2015 only
yrbss_cat <- subset( yrbss_cat , year == 2015 )
# download the microdata to your local computer
lodown( "yrbss" , yrbss_cat )
```

## Analysis Examples with the `survey` library {-}

Construct a complex sample survey design:

```{r eval = FALSE }

```

```{r eval = FALSE }
library(survey)

yrbss_df <- readRDS( file.path( path.expand( "~" ) , "YRBSS" , "2015 main.rds" ) )

yrbss_design <- 
	svydesign( 
		~ psu , 
		strata = ~ stratum , 
		data = yrbss_df , 
		weights = ~ weight , 
		nest = TRUE 
	)
```

### Variable Recoding {-}

Add new columns to the data set:
```{r eval = FALSE }
yrbss_design <- 
	update( 
		yrbss_design , 
		q2 = q2 ,
		never_rarely_wore_bike_helmet = as.numeric( qn8 == 1 ) ,
		ever_smoked_marijuana = as.numeric( qn47 == 1 ) ,
		ever_tried_to_quit_cigarettes = as.numeric( q36 > 2 ) ,
		smoked_cigarettes_past_year = as.numeric( q36 > 1 )
	)
```

### Unweighted Counts {-}

Count the unweighted number of records in the survey sample, overall and by groups:
```{r eval = FALSE , results = "hide" }
sum( weights( yrbss_design , "sampling" ) != 0 )

svyby( ~ one , ~ ever_smoked_marijuana , yrbss_design , unwtd.count )
```

### Weighted Counts {-}
Count the weighted size of the generalizable population, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ one , yrbss_design )

svyby( ~ one , ~ ever_smoked_marijuana , yrbss_design , svytotal )
```

### Descriptive Statistics {-}

Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ bmipct , yrbss_design , na.rm = TRUE )

svyby( ~ bmipct , ~ ever_smoked_marijuana , yrbss_design , svymean , na.rm = TRUE )
```

Calculate the distribution of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svymean( ~ q2 , yrbss_design , na.rm = TRUE )

svyby( ~ q2 , ~ ever_smoked_marijuana , yrbss_design , svymean , na.rm = TRUE )
```

Calculate the sum of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ bmipct , yrbss_design , na.rm = TRUE )

svyby( ~ bmipct , ~ ever_smoked_marijuana , yrbss_design , svytotal , na.rm = TRUE )
```

Calculate the weighted sum of a categorical variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svytotal( ~ q2 , yrbss_design , na.rm = TRUE )

svyby( ~ q2 , ~ ever_smoked_marijuana , yrbss_design , svytotal , na.rm = TRUE )
```

Calculate the median (50th percentile) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
svyquantile( ~ bmipct , yrbss_design , 0.5 , na.rm = TRUE )

svyby( 
	~ bmipct , 
	~ ever_smoked_marijuana , 
	yrbss_design , 
	svyquantile , 
	0.5 ,
	ci = TRUE ,
	keep.var = TRUE ,
	na.rm = TRUE
)
```

Estimate a ratio:
```{r eval = FALSE , results = "hide" }
svyratio( 
	numerator = ~ ever_tried_to_quit_cigarettes , 
	denominator = ~ smoked_cigarettes_past_year , 
	yrbss_design ,
	na.rm = TRUE
)
```

### Subsetting {-}

Restrict the survey design to youths who ever drank alcohol:
```{r eval = FALSE , results = "hide" }
sub_yrbss_design <- subset( yrbss_design , qn41 == 1 )
```
Calculate the mean (average) of this subset:
```{r eval = FALSE , results = "hide" }
svymean( ~ bmipct , sub_yrbss_design , na.rm = TRUE )
```

### Measures of Uncertainty {-}

Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups:
```{r eval = FALSE , results = "hide" }
this_result <- svymean( ~ bmipct , yrbss_design , na.rm = TRUE )

coef( this_result )
SE( this_result )
confint( this_result )
cv( this_result )

grouped_result <-
	svyby( 
		~ bmipct , 
		~ ever_smoked_marijuana , 
		yrbss_design , 
		svymean ,
		na.rm = TRUE 
	)
	
coef( grouped_result )
SE( grouped_result )
confint( grouped_result )
cv( grouped_result )
```

Calculate the degrees of freedom of any survey design object:
```{r eval = FALSE , results = "hide" }
degf( yrbss_design )
```

Calculate the complex sample survey-adjusted variance of any statistic:
```{r eval = FALSE , results = "hide" }
svyvar( ~ bmipct , yrbss_design , na.rm = TRUE )
```

Include the complex sample design effect in the result for a specific statistic:
```{r eval = FALSE , results = "hide" }
# SRS without replacement
svymean( ~ bmipct , yrbss_design , na.rm = TRUE , deff = TRUE )

# SRS with replacement
svymean( ~ bmipct , yrbss_design , na.rm = TRUE , deff = "replace" )
```

Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See `?svyciprop` for alternatives:
```{r eval = FALSE , results = "hide" }
svyciprop( ~ never_rarely_wore_bike_helmet , yrbss_design ,
	method = "likelihood" , na.rm = TRUE )
```

### Regression Models and Tests of Association {-}

Perform a design-based t-test:
```{r eval = FALSE , results = "hide" }
svyttest( bmipct ~ never_rarely_wore_bike_helmet , yrbss_design )
```

Perform a chi-squared test of association for survey data:
```{r eval = FALSE , results = "hide" }
svychisq( 
	~ never_rarely_wore_bike_helmet + q2 , 
	yrbss_design 
)
```

Perform a survey-weighted generalized linear model:
```{r eval = FALSE , results = "hide" }
glm_result <- 
	svyglm( 
		bmipct ~ never_rarely_wore_bike_helmet + q2 , 
		yrbss_design 
	)

summary( glm_result )
```

## Analysis Examples with `srvyr` {-}

The R `srvyr` library calculates summary statistics from survey data, such as the mean, total or quantile using [dplyr](https://github.com/tidyverse/dplyr/)-like syntax. [srvyr](https://github.com/gergness/srvyr) allows for the use of many verbs, such as `summarize`, `group_by`, and `mutate`, the convenience of pipe-able functions, the `tidyverse` style of non-standard evaluation and more consistent return types than the `survey` package. [This vignette](https://cran.r-project.org/web/packages/srvyr/vignettes/srvyr-vs-survey.html) details the available features. As a starting point for YRBSS users, this code replicates previously-presented examples:

```{r eval = FALSE , results = "hide" }
library(srvyr)
yrbss_srvyr_design <- as_survey( yrbss_design )
```
Calculate the mean (average) of a linear variable, overall and by groups:
```{r eval = FALSE , results = "hide" }
yrbss_srvyr_design %>%
	summarize( mean = survey_mean( bmipct , na.rm = TRUE ) )

yrbss_srvyr_design %>%
	group_by( ever_smoked_marijuana ) %>%
	summarize( mean = survey_mean( bmipct , na.rm = TRUE ) )
```

---

## Replication Example {-}

This snippet replicates the "never/rarely wore bicycle helmet" row of [PDF page 29 of this CDC analysis software document](https://www.cdc.gov/healthyyouth/data/yrbs/pdf/2015/2015_yrbs_analysis_software.pdf#page=29).

```{r eval = FALSE , results = "hide" }

unwtd.count( ~ never_rarely_wore_bike_helmet , yrbss_design )

svytotal( ~ one , subset( yrbss_design , !is.na( never_rarely_wore_bike_helmet ) ) )
 
svymean( ~ never_rarely_wore_bike_helmet , yrbss_design , na.rm = TRUE )

svyciprop( ~ never_rarely_wore_bike_helmet , yrbss_design , na.rm = TRUE , method = "beta" )

```



<!--chapter:end:67-yrbss.Rmd-->

